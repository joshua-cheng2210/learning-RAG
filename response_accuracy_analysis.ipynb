{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cf596ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# pd.set_option('display.max_rows', None)        # Show all rows\n",
    "# pd.set_option('display.max_columns', None)     # Show all columns\n",
    "# pd.set_option('display.width', None)           # No width limit\n",
    "pd.set_option('display.max_colwidth', 50)      # Limit column width\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dc80d7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_temp_file_to_numpy():\n",
    "    \"\"\"Parse temp.txt and organize data into numpy arrays with embedding models, chat models, and scores.\"\"\"\n",
    "    \n",
    "    # Read and parse the data\n",
    "    embedding_models = []\n",
    "    chat_models = []\n",
    "    scores = []\n",
    "    raw_scores = []  # New: store raw x/90 scores\n",
    "    \n",
    "    with open('temp.txt', 'r') as file:\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            if line.startswith('Model:'):\n",
    "                # Extract model names and score using regex\n",
    "                match = re.search(r'Model: (.+?)--(.+?)_quiz_results\\.json, Correct: (\\d+)/(\\d+)', line)\n",
    "                if match:\n",
    "                    embedding_model = match.group(1)\n",
    "                    chat_model = match.group(2)\n",
    "                    correct = int(match.group(3))\n",
    "                    total = int(match.group(4))\n",
    "                    accuracy = (correct / total) * 100\n",
    "                    raw_score = f\"{correct}/{total}\"  # New: raw score format\n",
    "                    \n",
    "                    embedding_models.append(embedding_model)\n",
    "                    chat_models.append(chat_model)\n",
    "                    scores.append(accuracy)\n",
    "                    raw_scores.append(raw_score)  # New: add raw score\n",
    "    \n",
    "    # Get unique models for matrix creation\n",
    "    unique_embedding_models = sorted(list(set(embedding_models)))\n",
    "    unique_chat_models = sorted(list(set(chat_models)))\n",
    "    \n",
    "    print(f\"📊 Data Summary:\")\n",
    "    print(f\"Total entries: {len(scores)}\")\n",
    "    print(f\"Unique embedding models: {len(unique_embedding_models)}\")\n",
    "    print(f\"Unique chat models: {len(unique_chat_models)}\")\n",
    "    \n",
    "    # Create a matrix with embedding models as rows and chat models as columns\n",
    "    score_matrix = np.full((len(unique_embedding_models), len(unique_chat_models)), np.nan)\n",
    "    \n",
    "    # Fill the matrix with scores\n",
    "    for i, (emb, chat, score) in enumerate(zip(embedding_models, chat_models, scores)):\n",
    "        emb_idx = unique_embedding_models.index(emb)\n",
    "        chat_idx = unique_chat_models.index(chat)\n",
    "        score_matrix[emb_idx, chat_idx] = score\n",
    "    \n",
    "    # Create structured arrays for easier access - Updated with raw_score\n",
    "    data_array = np.array(list(zip(embedding_models, chat_models, scores, raw_scores)), \n",
    "                         dtype=[('embedding_model', 'U50'), ('chat_model', 'U50'), ('accuracy', 'f4'), ('raw_score', 'U10')])\n",
    "    \n",
    "    print(f\"\\n🏆 Top 10 Performing Combinations:\")\n",
    "    sorted_indices = np.argsort(data_array['accuracy'])[::-1]\n",
    "    for i, idx in enumerate(sorted_indices[:10]):\n",
    "        entry = data_array[idx]\n",
    "        print(f\"{i+1:2d}. {entry['embedding_model']} + {entry['chat_model']}: {entry['accuracy']:.1f}% ({entry['raw_score']})\")\n",
    "    \n",
    "    print(f\"\\n📉 Bottom 5 Performing Combinations:\")\n",
    "    for i, idx in enumerate(sorted_indices[-5:]):\n",
    "        entry = data_array[idx]\n",
    "        print(f\"{i+1:2d}. {entry['embedding_model']} + {entry['chat_model']}: {entry['accuracy']:.1f}% ({entry['raw_score']})\")\n",
    "    \n",
    "    return {\n",
    "        'data_array': data_array,\n",
    "        'score_matrix': score_matrix,\n",
    "        'embedding_models': unique_embedding_models,\n",
    "        'chat_models': unique_chat_models,\n",
    "        'raw_data': {\n",
    "            'embedding_models': embedding_models,\n",
    "            'chat_models': chat_models,\n",
    "            'scores': scores,\n",
    "            'raw_scores': raw_scores  # New: include raw scores\n",
    "        }\n",
    "    }\n",
    "\n",
    "def analyze_performance(data_dict):\n",
    "    \"\"\"Analyze performance statistics from the numpy data.\"\"\"\n",
    "    data_array = data_dict['data_array']\n",
    "    score_matrix = data_dict['score_matrix']\n",
    "    embedding_models = data_dict['embedding_models']\n",
    "    chat_models = data_dict['chat_models']\n",
    "    \n",
    "    print(f\"\\n📈 Performance Analysis:\")\n",
    "    print(f\"Overall average accuracy: {np.mean(data_array['accuracy']):.2f}%\")\n",
    "    print(f\"Standard deviation: {np.std(data_array['accuracy']):.2f}%\")\n",
    "    print(f\"Best performance: {np.max(data_array['accuracy']):.1f}%\")\n",
    "    print(f\"Worst performance: {np.min(data_array['accuracy']):.1f}%\")\n",
    "    \n",
    "    # Average performance by embedding model\n",
    "    print(f\"\\n🎯 Average Performance by Embedding Model:\")\n",
    "    for i, emb_model in enumerate(embedding_models):\n",
    "        row_scores = score_matrix[i, :]\n",
    "        valid_scores = row_scores[~np.isnan(row_scores)]\n",
    "        if len(valid_scores) > 0:\n",
    "            avg_score = np.mean(valid_scores)\n",
    "            # Calculate average raw score\n",
    "            emb_data = data_array[data_array['embedding_model'] == emb_model]\n",
    "            avg_correct = np.mean([int(score.split('/')[0]) for score in emb_data['raw_score']])\n",
    "            print(f\"{emb_model:30s}: {avg_score:5.1f}% ({avg_correct:.1f}/90 avg) from {len(valid_scores)} models\")\n",
    "    \n",
    "    # Average performance by chat model\n",
    "    print(f\"\\n💬 Average Performance by Chat Model:\")\n",
    "    for j, chat_model in enumerate(chat_models):\n",
    "        col_scores = score_matrix[:, j]\n",
    "        valid_scores = col_scores[~np.isnan(col_scores)]\n",
    "        if len(valid_scores) > 0:\n",
    "            avg_score = np.mean(valid_scores)\n",
    "            # Calculate average raw score\n",
    "            chat_data = data_array[data_array['chat_model'] == chat_model]\n",
    "            avg_correct = np.mean([int(score.split('/')[0]) for score in chat_data['raw_score']])\n",
    "            print(f\"{chat_model:30s}: {avg_score:5.1f}% ({avg_correct:.1f}/90 avg) from {len(valid_scores)} embeddings\")\n",
    "    \n",
    "    return score_matrix\n",
    "\n",
    "def create_pandas_dataframe(data_dict):\n",
    "    \"\"\"Convert numpy data to pandas DataFrame for easier manipulation.\"\"\"\n",
    "    data_array = data_dict['data_array']\n",
    "    \n",
    "    # Create DataFrame with both percentage and raw scores\n",
    "    df = pd.DataFrame({\n",
    "        'embedding_model': data_array['embedding_model'],\n",
    "        'chat_model': data_array['chat_model'],\n",
    "        'accuracy_percent': data_array['accuracy'],\n",
    "        'raw_score': data_array['raw_score']  # New: raw score column\n",
    "    })\n",
    "    \n",
    "    # Create pivot tables for both metrics\n",
    "    pivot_df_percent = df.pivot(index='embedding_model', columns='chat_model', values='accuracy_percent')\n",
    "    pivot_df_raw = df.pivot(index='embedding_model', columns='chat_model', values='raw_score')\n",
    "    \n",
    "    print(f\"\\n📋 Data as Pandas DataFrame:\")\n",
    "    print(df.head(10))\n",
    "    \n",
    "    print(f\"\\n📊 Pivot Table - Percentages (first 5x5):\")\n",
    "    print(pivot_df_percent.iloc[:5, :5])\n",
    "    \n",
    "    print(f\"\\n📊 Pivot Table - Raw Scores (first 5x5):\")\n",
    "    print(pivot_df_raw.iloc[:5, :5])\n",
    "    \n",
    "    return df, pivot_df_percent, pivot_df_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1b1128f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Parsing temp.txt file...\n",
      "📊 Data Summary:\n",
      "Total entries: 183\n",
      "Unique embedding models: 13\n",
      "Unique chat models: 15\n",
      "\n",
      "🏆 Top 10 Performing Combinations:\n",
      " 1. instructor_xl + gemma_3_1b_it: 78.9% (71/90)\n",
      " 2. instructor_xl + Llama_3.2_1B: 78.9% (71/90)\n",
      " 3. instructor_xl + flan_t5_large: 78.9% (71/90)\n",
      " 4. instructor_xl + falcon_7b: 78.9% (71/90)\n",
      " 5. instructor_xl + gemma_2_2b: 78.9% (71/90)\n",
      " 6. instructor_xl + Falcon_H1_0.5B_Instruct: 78.9% (71/90)\n",
      " 7. instructor_xl + falcon_7b_instruct: 78.9% (71/90)\n",
      " 8. instructor_xl + Llama_3.1_8B_Instruct: 78.9% (71/90)\n",
      " 9. instructor_xl + Llama_3.2_3B_Instruct: 78.9% (71/90)\n",
      "10. instructor_xl + zephyr_7b_beta: 78.9% (71/90)\n",
      "\n",
      "📉 Bottom 5 Performing Combinations:\n",
      " 1. static_retrieval_mrl_en_v1 + flan_t5_small: 58.9% (53/90)\n",
      " 2. static_retrieval_mrl_en_v1 + falcon_7b: 58.9% (53/90)\n",
      " 3. static_retrieval_mrl_en_v1 + zephyr_7b_beta: 58.9% (53/90)\n",
      " 4. static_retrieval_mrl_en_v1 + Llama_3.2_3B_Instruct: 58.9% (53/90)\n",
      " 5. static_retrieval_mrl_en_v1 + flan_t5_base: 50.0% (45/90)\n",
      "\n",
      "======================================================================\n",
      "\n",
      "📈 Performance Analysis:\n",
      "Overall average accuracy: 69.79%\n",
      "Standard deviation: 5.23%\n",
      "Best performance: 78.9%\n",
      "Worst performance: 50.0%\n",
      "\n",
      "🎯 Average Performance by Embedding Model:\n",
      "all_MiniLM_L12_v2             :  61.1% (55.0/90 avg) from 4 models\n",
      "all_MiniLM_L6_v2              :  72.8% (65.5/90 avg) from 15 models\n",
      "all_mpnet_base_v2             :  67.6% (60.9/90 avg) from 15 models\n",
      "bge_base_en_v1.5              :  73.2% (65.9/90 avg) from 15 models\n",
      "bge_large_en                  :  71.5% (64.3/90 avg) from 15 models\n",
      "bge_large_en_v1.5             :  71.3% (64.1/90 avg) from 15 models\n",
      "bge_m3                        :  70.0% (63.0/90 avg) from 15 models\n",
      "e5_base_v2                    :  72.5% (65.3/90 avg) from 15 models\n",
      "instructor_large              :  68.3% (61.5/90 avg) from 15 models\n",
      "instructor_xl                 :  77.6% (69.9/90 avg) from 14 models\n",
      "multi_qa_mpnet_base_cos_v1    :  71.3% (64.1/90 avg) from 15 models\n",
      "multi_qa_mpnet_base_dot_v1    :  65.7% (59.1/90 avg) from 15 models\n",
      "static_retrieval_mrl_en_v1    :  58.5% (52.7/90 avg) from 15 models\n",
      "\n",
      "💬 Average Performance by Chat Model:\n",
      "Falcon_H1_0.5B_Instruct       :  70.8% (63.8/90 avg) from 12 embeddings\n",
      "Llama_3.1_8B_Instruct         :  70.1% (63.1/90 avg) from 13 embeddings\n",
      "Llama_3.2_1B                  :  70.8% (63.8/90 avg) from 12 embeddings\n",
      "Llama_3.2_3B_Instruct         :  70.8% (63.8/90 avg) from 12 embeddings\n",
      "Meta_Llama_3_8B_Instruct      :  70.8% (63.8/90 avg) from 12 embeddings\n",
      "falcon_7b                     :  70.1% (63.1/90 avg) from 13 embeddings\n",
      "falcon_7b_instruct            :  70.8% (63.8/90 avg) from 12 embeddings\n",
      "flan_t5_base                  :  63.1% (56.8/90 avg) from 12 embeddings\n",
      "flan_t5_large                 :  70.8% (63.8/90 avg) from 12 embeddings\n",
      "flan_t5_small                 :  67.5% (60.8/90 avg) from 12 embeddings\n",
      "flan_t5_xl                    :  69.1% (62.2/90 avg) from 11 embeddings\n",
      "gemma_2_2b                    :  70.8% (63.8/90 avg) from 12 embeddings\n",
      "gemma_2_2b_it                 :  70.8% (63.8/90 avg) from 12 embeddings\n",
      "gemma_3_1b_it                 :  70.1% (63.1/90 avg) from 13 embeddings\n",
      "zephyr_7b_beta                :  70.1% (63.1/90 avg) from 13 embeddings\n",
      "\n",
      "======================================================================\n",
      "\n",
      "📋 Data as Pandas DataFrame:\n",
      "              embedding_model             chat_model  accuracy_percent  \\\n",
      "0  multi_qa_mpnet_base_dot_v1             gemma_2_2b         66.666664   \n",
      "1                bge_large_en          gemma_2_2b_it         73.333336   \n",
      "2            bge_base_en_v1.5  Llama_3.1_8B_Instruct         74.444443   \n",
      "3                bge_large_en           flan_t5_base         62.222221   \n",
      "4           all_mpnet_base_v2  Llama_3.1_8B_Instruct         67.777779   \n",
      "5  multi_qa_mpnet_base_cos_v1             flan_t5_xl         68.888885   \n",
      "6  static_retrieval_mrl_en_v1          flan_t5_large         58.888889   \n",
      "7                bge_large_en          gemma_3_1b_it         73.333336   \n",
      "8           all_mpnet_base_v2          gemma_3_1b_it         67.777779   \n",
      "9            instructor_large          gemma_3_1b_it         68.888885   \n",
      "\n",
      "  raw_score  \n",
      "0     60/90  \n",
      "1     66/90  \n",
      "2     67/90  \n",
      "3     56/90  \n",
      "4     61/90  \n",
      "5     62/90  \n",
      "6     53/90  \n",
      "7     66/90  \n",
      "8     61/90  \n",
      "9     62/90  \n",
      "\n",
      "📊 Pivot Table - Percentages (first 5x5):\n",
      "chat_model         Falcon_H1_0.5B_Instruct  Llama_3.1_8B_Instruct  \\\n",
      "embedding_model                                                     \n",
      "all_MiniLM_L12_v2                      NaN              61.111111   \n",
      "all_MiniLM_L6_v2                 73.333336              73.333336   \n",
      "all_mpnet_base_v2                67.777779              67.777779   \n",
      "bge_base_en_v1.5                 74.444443              74.444443   \n",
      "bge_large_en                     73.333336              73.333336   \n",
      "\n",
      "chat_model         Llama_3.2_1B  Llama_3.2_3B_Instruct  \\\n",
      "embedding_model                                          \n",
      "all_MiniLM_L12_v2           NaN                    NaN   \n",
      "all_MiniLM_L6_v2      73.333336              73.333336   \n",
      "all_mpnet_base_v2     67.777779              67.777779   \n",
      "bge_base_en_v1.5      74.444443              74.444443   \n",
      "bge_large_en          73.333336              73.333336   \n",
      "\n",
      "chat_model         Meta_Llama_3_8B_Instruct  \n",
      "embedding_model                              \n",
      "all_MiniLM_L12_v2                       NaN  \n",
      "all_MiniLM_L6_v2                  73.333336  \n",
      "all_mpnet_base_v2                 67.777779  \n",
      "bge_base_en_v1.5                  74.444443  \n",
      "bge_large_en                      73.333336  \n",
      "\n",
      "📊 Pivot Table - Raw Scores (first 5x5):\n",
      "chat_model        Falcon_H1_0.5B_Instruct Llama_3.1_8B_Instruct Llama_3.2_1B  \\\n",
      "embedding_model                                                                \n",
      "all_MiniLM_L12_v2                     NaN                 55/90          NaN   \n",
      "all_MiniLM_L6_v2                    66/90                 66/90        66/90   \n",
      "all_mpnet_base_v2                   61/90                 61/90        61/90   \n",
      "bge_base_en_v1.5                    67/90                 67/90        67/90   \n",
      "bge_large_en                        66/90                 66/90        66/90   \n",
      "\n",
      "chat_model        Llama_3.2_3B_Instruct Meta_Llama_3_8B_Instruct  \n",
      "embedding_model                                                   \n",
      "all_MiniLM_L12_v2                   NaN                      NaN  \n",
      "all_MiniLM_L6_v2                  66/90                    66/90  \n",
      "all_mpnet_base_v2                 61/90                    61/90  \n",
      "bge_base_en_v1.5                  67/90                    67/90  \n",
      "bge_large_en                      66/90                    66/90  \n",
      "\n",
      "✅ Data successfully loaded into numpy arrays!\n",
      "📐 Score matrix shape: (13, 15)\n",
      "📏 Data array length: 183\n",
      "\n",
      "🔍 DataFrame columns: ['embedding_model', 'chat_model', 'accuracy_percent', 'raw_score']\n",
      "📊 Sample data:\n",
      "              embedding_model             chat_model  accuracy_percent  \\\n",
      "0  multi_qa_mpnet_base_dot_v1             gemma_2_2b         66.666664   \n",
      "1                bge_large_en          gemma_2_2b_it         73.333336   \n",
      "2            bge_base_en_v1.5  Llama_3.1_8B_Instruct         74.444443   \n",
      "3                bge_large_en           flan_t5_base         62.222221   \n",
      "4           all_mpnet_base_v2  Llama_3.1_8B_Instruct         67.777779   \n",
      "\n",
      "  raw_score  \n",
      "0     60/90  \n",
      "1     66/90  \n",
      "2     67/90  \n",
      "3     56/90  \n",
      "4     61/90  \n"
     ]
    }
   ],
   "source": [
    "# Run the analysis\n",
    "print(\"🔍 Parsing temp.txt file...\")\n",
    "data_dict = parse_temp_file_to_numpy()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "analyze_performance(data_dict)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "df, pivot_df_percent, pivot_df_raw = create_pandas_dataframe(data_dict)\n",
    "\n",
    "# Access the numpy arrays\n",
    "data_array = data_dict['data_array']\n",
    "score_matrix = data_dict['score_matrix']\n",
    "embedding_models = data_dict['embedding_models']\n",
    "chat_models = data_dict['chat_models']\n",
    "\n",
    "print(f\"\\n✅ Data successfully loaded into numpy arrays!\")\n",
    "print(f\"📐 Score matrix shape: {score_matrix.shape}\")\n",
    "print(f\"📏 Data array length: {len(data_array)}\")\n",
    "\n",
    "# Show the updated DataFrame structure\n",
    "print(f\"\\n🔍 DataFrame columns: {list(df.columns)}\")\n",
    "print(f\"📊 Sample data:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b7230cde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>embedding_model</th>\n",
       "      <th>chat_model</th>\n",
       "      <th>accuracy_percent</th>\n",
       "      <th>raw_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>multi_qa_mpnet_base_dot_v1</td>\n",
       "      <td>gemma_2_2b</td>\n",
       "      <td>66.666664</td>\n",
       "      <td>60/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bge_large_en</td>\n",
       "      <td>gemma_2_2b_it</td>\n",
       "      <td>73.333336</td>\n",
       "      <td>66/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bge_base_en_v1.5</td>\n",
       "      <td>Llama_3.1_8B_Instruct</td>\n",
       "      <td>74.444443</td>\n",
       "      <td>67/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bge_large_en</td>\n",
       "      <td>flan_t5_base</td>\n",
       "      <td>62.222221</td>\n",
       "      <td>56/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>all_mpnet_base_v2</td>\n",
       "      <td>Llama_3.1_8B_Instruct</td>\n",
       "      <td>67.777779</td>\n",
       "      <td>61/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>multi_qa_mpnet_base_cos_v1</td>\n",
       "      <td>flan_t5_xl</td>\n",
       "      <td>68.888885</td>\n",
       "      <td>62/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>static_retrieval_mrl_en_v1</td>\n",
       "      <td>flan_t5_large</td>\n",
       "      <td>58.888889</td>\n",
       "      <td>53/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>bge_large_en</td>\n",
       "      <td>gemma_3_1b_it</td>\n",
       "      <td>73.333336</td>\n",
       "      <td>66/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>all_mpnet_base_v2</td>\n",
       "      <td>gemma_3_1b_it</td>\n",
       "      <td>67.777779</td>\n",
       "      <td>61/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>instructor_large</td>\n",
       "      <td>gemma_3_1b_it</td>\n",
       "      <td>68.888885</td>\n",
       "      <td>62/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>multi_qa_mpnet_base_dot_v1</td>\n",
       "      <td>Llama_3.2_1B</td>\n",
       "      <td>66.666664</td>\n",
       "      <td>60/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>bge_m3</td>\n",
       "      <td>Llama_3.1_8B_Instruct</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>63/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>instructor_large</td>\n",
       "      <td>falcon_7b</td>\n",
       "      <td>68.888885</td>\n",
       "      <td>62/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>bge_large_en</td>\n",
       "      <td>flan_t5_large</td>\n",
       "      <td>73.333336</td>\n",
       "      <td>66/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>multi_qa_mpnet_base_cos_v1</td>\n",
       "      <td>zephyr_7b_beta</td>\n",
       "      <td>72.222221</td>\n",
       "      <td>65/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>bge_large_en_v1.5</td>\n",
       "      <td>zephyr_7b_beta</td>\n",
       "      <td>72.222221</td>\n",
       "      <td>65/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>bge_base_en_v1.5</td>\n",
       "      <td>Falcon_H1_0.5B_Instruct</td>\n",
       "      <td>74.444443</td>\n",
       "      <td>67/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>all_mpnet_base_v2</td>\n",
       "      <td>Llama_3.2_1B</td>\n",
       "      <td>67.777779</td>\n",
       "      <td>61/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>bge_large_en_v1.5</td>\n",
       "      <td>flan_t5_large</td>\n",
       "      <td>72.222221</td>\n",
       "      <td>65/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>bge_large_en_v1.5</td>\n",
       "      <td>flan_t5_small</td>\n",
       "      <td>68.888885</td>\n",
       "      <td>62/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>all_MiniLM_L6_v2</td>\n",
       "      <td>Meta_Llama_3_8B_Instruct</td>\n",
       "      <td>73.333336</td>\n",
       "      <td>66/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>multi_qa_mpnet_base_dot_v1</td>\n",
       "      <td>Llama_3.2_3B_Instruct</td>\n",
       "      <td>66.666664</td>\n",
       "      <td>60/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>e5_base_v2</td>\n",
       "      <td>gemma_2_2b_it</td>\n",
       "      <td>73.333336</td>\n",
       "      <td>66/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>bge_large_en_v1.5</td>\n",
       "      <td>gemma_2_2b_it</td>\n",
       "      <td>72.222221</td>\n",
       "      <td>65/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>bge_base_en_v1.5</td>\n",
       "      <td>falcon_7b</td>\n",
       "      <td>74.444443</td>\n",
       "      <td>67/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>static_retrieval_mrl_en_v1</td>\n",
       "      <td>falcon_7b</td>\n",
       "      <td>58.888889</td>\n",
       "      <td>53/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>bge_base_en_v1.5</td>\n",
       "      <td>Llama_3.2_1B</td>\n",
       "      <td>74.444443</td>\n",
       "      <td>67/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>e5_base_v2</td>\n",
       "      <td>flan_t5_small</td>\n",
       "      <td>68.888885</td>\n",
       "      <td>62/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>all_mpnet_base_v2</td>\n",
       "      <td>flan_t5_base</td>\n",
       "      <td>63.333332</td>\n",
       "      <td>57/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>static_retrieval_mrl_en_v1</td>\n",
       "      <td>falcon_7b_instruct</td>\n",
       "      <td>58.888889</td>\n",
       "      <td>53/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>all_MiniLM_L6_v2</td>\n",
       "      <td>falcon_7b_instruct</td>\n",
       "      <td>73.333336</td>\n",
       "      <td>66/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>bge_large_en_v1.5</td>\n",
       "      <td>Llama_3.2_1B</td>\n",
       "      <td>72.222221</td>\n",
       "      <td>65/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>bge_m3</td>\n",
       "      <td>flan_t5_large</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>63/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>e5_base_v2</td>\n",
       "      <td>Llama_3.2_3B_Instruct</td>\n",
       "      <td>73.333336</td>\n",
       "      <td>66/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>all_mpnet_base_v2</td>\n",
       "      <td>flan_t5_large</td>\n",
       "      <td>67.777779</td>\n",
       "      <td>61/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>all_MiniLM_L6_v2</td>\n",
       "      <td>flan_t5_large</td>\n",
       "      <td>73.333336</td>\n",
       "      <td>66/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>bge_m3</td>\n",
       "      <td>falcon_7b_instruct</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>63/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>static_retrieval_mrl_en_v1</td>\n",
       "      <td>Llama_3.2_3B_Instruct</td>\n",
       "      <td>58.888889</td>\n",
       "      <td>53/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>bge_base_en_v1.5</td>\n",
       "      <td>Meta_Llama_3_8B_Instruct</td>\n",
       "      <td>74.444443</td>\n",
       "      <td>67/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>e5_base_v2</td>\n",
       "      <td>Meta_Llama_3_8B_Instruct</td>\n",
       "      <td>73.333336</td>\n",
       "      <td>66/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>bge_base_en_v1.5</td>\n",
       "      <td>flan_t5_small</td>\n",
       "      <td>71.111115</td>\n",
       "      <td>64/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>bge_large_en_v1.5</td>\n",
       "      <td>falcon_7b</td>\n",
       "      <td>72.222221</td>\n",
       "      <td>65/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>all_MiniLM_L6_v2</td>\n",
       "      <td>falcon_7b</td>\n",
       "      <td>73.333336</td>\n",
       "      <td>66/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>bge_base_en_v1.5</td>\n",
       "      <td>gemma_3_1b_it</td>\n",
       "      <td>74.444443</td>\n",
       "      <td>67/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>instructor_large</td>\n",
       "      <td>Llama_3.1_8B_Instruct</td>\n",
       "      <td>68.888885</td>\n",
       "      <td>62/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>static_retrieval_mrl_en_v1</td>\n",
       "      <td>gemma_3_1b_it</td>\n",
       "      <td>58.888889</td>\n",
       "      <td>53/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>all_mpnet_base_v2</td>\n",
       "      <td>gemma_2_2b</td>\n",
       "      <td>67.777779</td>\n",
       "      <td>61/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>all_MiniLM_L6_v2</td>\n",
       "      <td>flan_t5_xl</td>\n",
       "      <td>72.222221</td>\n",
       "      <td>65/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>bge_m3</td>\n",
       "      <td>Llama_3.2_1B</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>63/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>instructor_large</td>\n",
       "      <td>gemma_2_2b_it</td>\n",
       "      <td>68.888885</td>\n",
       "      <td>62/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>all_MiniLM_L6_v2</td>\n",
       "      <td>Falcon_H1_0.5B_Instruct</td>\n",
       "      <td>73.333336</td>\n",
       "      <td>66/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>instructor_large</td>\n",
       "      <td>flan_t5_base</td>\n",
       "      <td>58.888889</td>\n",
       "      <td>53/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>bge_m3</td>\n",
       "      <td>falcon_7b</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>63/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>bge_large_en_v1.5</td>\n",
       "      <td>flan_t5_xl</td>\n",
       "      <td>71.111115</td>\n",
       "      <td>64/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>multi_qa_mpnet_base_dot_v1</td>\n",
       "      <td>falcon_7b</td>\n",
       "      <td>66.666664</td>\n",
       "      <td>60/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>instructor_large</td>\n",
       "      <td>flan_t5_large</td>\n",
       "      <td>68.888885</td>\n",
       "      <td>62/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>static_retrieval_mrl_en_v1</td>\n",
       "      <td>Llama_3.1_8B_Instruct</td>\n",
       "      <td>58.888889</td>\n",
       "      <td>53/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>multi_qa_mpnet_base_dot_v1</td>\n",
       "      <td>zephyr_7b_beta</td>\n",
       "      <td>66.666664</td>\n",
       "      <td>60/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>all_MiniLM_L12_v2</td>\n",
       "      <td>Llama_3.1_8B_Instruct</td>\n",
       "      <td>61.111111</td>\n",
       "      <td>55/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>multi_qa_mpnet_base_cos_v1</td>\n",
       "      <td>Falcon_H1_0.5B_Instruct</td>\n",
       "      <td>72.222221</td>\n",
       "      <td>65/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>all_MiniLM_L6_v2</td>\n",
       "      <td>gemma_2_2b_it</td>\n",
       "      <td>73.333336</td>\n",
       "      <td>66/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>all_MiniLM_L6_v2</td>\n",
       "      <td>flan_t5_small</td>\n",
       "      <td>71.111115</td>\n",
       "      <td>64/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>e5_base_v2</td>\n",
       "      <td>flan_t5_base</td>\n",
       "      <td>68.888885</td>\n",
       "      <td>62/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>instructor_large</td>\n",
       "      <td>zephyr_7b_beta</td>\n",
       "      <td>68.888885</td>\n",
       "      <td>62/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>instructor_large</td>\n",
       "      <td>Llama_3.2_3B_Instruct</td>\n",
       "      <td>68.888885</td>\n",
       "      <td>62/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>bge_large_en</td>\n",
       "      <td>falcon_7b_instruct</td>\n",
       "      <td>73.333336</td>\n",
       "      <td>66/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>all_MiniLM_L12_v2</td>\n",
       "      <td>gemma_3_1b_it</td>\n",
       "      <td>61.111111</td>\n",
       "      <td>55/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>e5_base_v2</td>\n",
       "      <td>Llama_3.2_1B</td>\n",
       "      <td>73.333336</td>\n",
       "      <td>66/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>all_mpnet_base_v2</td>\n",
       "      <td>Llama_3.2_3B_Instruct</td>\n",
       "      <td>67.777779</td>\n",
       "      <td>61/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>e5_base_v2</td>\n",
       "      <td>zephyr_7b_beta</td>\n",
       "      <td>73.333336</td>\n",
       "      <td>66/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>all_MiniLM_L12_v2</td>\n",
       "      <td>falcon_7b</td>\n",
       "      <td>61.111111</td>\n",
       "      <td>55/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>bge_large_en</td>\n",
       "      <td>Llama_3.1_8B_Instruct</td>\n",
       "      <td>73.333336</td>\n",
       "      <td>66/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>bge_large_en_v1.5</td>\n",
       "      <td>falcon_7b_instruct</td>\n",
       "      <td>72.222221</td>\n",
       "      <td>65/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>bge_m3</td>\n",
       "      <td>gemma_2_2b_it</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>63/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>bge_m3</td>\n",
       "      <td>gemma_2_2b</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>63/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>all_mpnet_base_v2</td>\n",
       "      <td>Meta_Llama_3_8B_Instruct</td>\n",
       "      <td>67.777779</td>\n",
       "      <td>61/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>bge_base_en_v1.5</td>\n",
       "      <td>flan_t5_xl</td>\n",
       "      <td>68.888885</td>\n",
       "      <td>62/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>multi_qa_mpnet_base_cos_v1</td>\n",
       "      <td>gemma_2_2b</td>\n",
       "      <td>72.222221</td>\n",
       "      <td>65/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>multi_qa_mpnet_base_dot_v1</td>\n",
       "      <td>gemma_2_2b_it</td>\n",
       "      <td>66.666664</td>\n",
       "      <td>60/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>bge_base_en_v1.5</td>\n",
       "      <td>flan_t5_base</td>\n",
       "      <td>64.444443</td>\n",
       "      <td>58/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>bge_base_en_v1.5</td>\n",
       "      <td>falcon_7b_instruct</td>\n",
       "      <td>74.444443</td>\n",
       "      <td>67/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>e5_base_v2</td>\n",
       "      <td>flan_t5_large</td>\n",
       "      <td>73.333336</td>\n",
       "      <td>66/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>instructor_large</td>\n",
       "      <td>gemma_2_2b</td>\n",
       "      <td>68.888885</td>\n",
       "      <td>62/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>multi_qa_mpnet_base_dot_v1</td>\n",
       "      <td>Llama_3.1_8B_Instruct</td>\n",
       "      <td>66.666664</td>\n",
       "      <td>60/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>bge_base_en_v1.5</td>\n",
       "      <td>gemma_2_2b</td>\n",
       "      <td>74.444443</td>\n",
       "      <td>67/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>bge_large_en_v1.5</td>\n",
       "      <td>flan_t5_base</td>\n",
       "      <td>62.222221</td>\n",
       "      <td>56/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>bge_large_en</td>\n",
       "      <td>zephyr_7b_beta</td>\n",
       "      <td>73.333336</td>\n",
       "      <td>66/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>multi_qa_mpnet_base_cos_v1</td>\n",
       "      <td>Meta_Llama_3_8B_Instruct</td>\n",
       "      <td>72.222221</td>\n",
       "      <td>65/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>static_retrieval_mrl_en_v1</td>\n",
       "      <td>Llama_3.2_1B</td>\n",
       "      <td>58.888889</td>\n",
       "      <td>53/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>all_MiniLM_L6_v2</td>\n",
       "      <td>flan_t5_base</td>\n",
       "      <td>68.888885</td>\n",
       "      <td>62/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>multi_qa_mpnet_base_dot_v1</td>\n",
       "      <td>flan_t5_xl</td>\n",
       "      <td>64.444443</td>\n",
       "      <td>58/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>multi_qa_mpnet_base_cos_v1</td>\n",
       "      <td>Llama_3.2_1B</td>\n",
       "      <td>72.222221</td>\n",
       "      <td>65/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>bge_m3</td>\n",
       "      <td>flan_t5_base</td>\n",
       "      <td>63.333332</td>\n",
       "      <td>57/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>multi_qa_mpnet_base_cos_v1</td>\n",
       "      <td>flan_t5_small</td>\n",
       "      <td>66.666664</td>\n",
       "      <td>60/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>multi_qa_mpnet_base_dot_v1</td>\n",
       "      <td>flan_t5_large</td>\n",
       "      <td>66.666664</td>\n",
       "      <td>60/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>static_retrieval_mrl_en_v1</td>\n",
       "      <td>gemma_2_2b_it</td>\n",
       "      <td>58.888889</td>\n",
       "      <td>53/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>multi_qa_mpnet_base_dot_v1</td>\n",
       "      <td>flan_t5_small</td>\n",
       "      <td>62.222221</td>\n",
       "      <td>56/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>bge_base_en_v1.5</td>\n",
       "      <td>gemma_2_2b_it</td>\n",
       "      <td>74.444443</td>\n",
       "      <td>67/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>bge_large_en</td>\n",
       "      <td>Llama_3.2_3B_Instruct</td>\n",
       "      <td>73.333336</td>\n",
       "      <td>66/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>instructor_large</td>\n",
       "      <td>Meta_Llama_3_8B_Instruct</td>\n",
       "      <td>68.888885</td>\n",
       "      <td>62/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>bge_m3</td>\n",
       "      <td>Falcon_H1_0.5B_Instruct</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>63/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>bge_base_en_v1.5</td>\n",
       "      <td>Llama_3.2_3B_Instruct</td>\n",
       "      <td>74.444443</td>\n",
       "      <td>67/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>e5_base_v2</td>\n",
       "      <td>gemma_3_1b_it</td>\n",
       "      <td>73.333336</td>\n",
       "      <td>66/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>bge_large_en</td>\n",
       "      <td>gemma_2_2b</td>\n",
       "      <td>73.333336</td>\n",
       "      <td>66/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>static_retrieval_mrl_en_v1</td>\n",
       "      <td>Falcon_H1_0.5B_Instruct</td>\n",
       "      <td>58.888889</td>\n",
       "      <td>53/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>multi_qa_mpnet_base_cos_v1</td>\n",
       "      <td>gemma_3_1b_it</td>\n",
       "      <td>72.222221</td>\n",
       "      <td>65/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>all_MiniLM_L6_v2</td>\n",
       "      <td>gemma_2_2b</td>\n",
       "      <td>73.333336</td>\n",
       "      <td>66/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>instructor_large</td>\n",
       "      <td>flan_t5_small</td>\n",
       "      <td>67.777779</td>\n",
       "      <td>61/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>all_mpnet_base_v2</td>\n",
       "      <td>falcon_7b_instruct</td>\n",
       "      <td>67.777779</td>\n",
       "      <td>61/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>static_retrieval_mrl_en_v1</td>\n",
       "      <td>flan_t5_xl</td>\n",
       "      <td>62.222221</td>\n",
       "      <td>56/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>all_MiniLM_L6_v2</td>\n",
       "      <td>zephyr_7b_beta</td>\n",
       "      <td>73.333336</td>\n",
       "      <td>66/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>bge_m3</td>\n",
       "      <td>zephyr_7b_beta</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>63/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>bge_large_en_v1.5</td>\n",
       "      <td>Llama_3.2_3B_Instruct</td>\n",
       "      <td>72.222221</td>\n",
       "      <td>65/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>multi_qa_mpnet_base_cos_v1</td>\n",
       "      <td>gemma_2_2b_it</td>\n",
       "      <td>72.222221</td>\n",
       "      <td>65/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>instructor_large</td>\n",
       "      <td>Falcon_H1_0.5B_Instruct</td>\n",
       "      <td>68.888885</td>\n",
       "      <td>62/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>bge_large_en</td>\n",
       "      <td>Falcon_H1_0.5B_Instruct</td>\n",
       "      <td>73.333336</td>\n",
       "      <td>66/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>multi_qa_mpnet_base_cos_v1</td>\n",
       "      <td>falcon_7b</td>\n",
       "      <td>72.222221</td>\n",
       "      <td>65/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>multi_qa_mpnet_base_dot_v1</td>\n",
       "      <td>Falcon_H1_0.5B_Instruct</td>\n",
       "      <td>66.666664</td>\n",
       "      <td>60/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>all_mpnet_base_v2</td>\n",
       "      <td>zephyr_7b_beta</td>\n",
       "      <td>67.777779</td>\n",
       "      <td>61/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>all_mpnet_base_v2</td>\n",
       "      <td>flan_t5_xl</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>63/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>multi_qa_mpnet_base_cos_v1</td>\n",
       "      <td>Llama_3.1_8B_Instruct</td>\n",
       "      <td>72.222221</td>\n",
       "      <td>65/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>bge_large_en_v1.5</td>\n",
       "      <td>Falcon_H1_0.5B_Instruct</td>\n",
       "      <td>72.222221</td>\n",
       "      <td>65/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>bge_large_en</td>\n",
       "      <td>Llama_3.2_1B</td>\n",
       "      <td>73.333336</td>\n",
       "      <td>66/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>multi_qa_mpnet_base_dot_v1</td>\n",
       "      <td>flan_t5_base</td>\n",
       "      <td>58.888889</td>\n",
       "      <td>53/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>all_MiniLM_L6_v2</td>\n",
       "      <td>Llama_3.2_3B_Instruct</td>\n",
       "      <td>73.333336</td>\n",
       "      <td>66/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>bge_m3</td>\n",
       "      <td>flan_t5_small</td>\n",
       "      <td>71.111115</td>\n",
       "      <td>64/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>e5_base_v2</td>\n",
       "      <td>Llama_3.1_8B_Instruct</td>\n",
       "      <td>73.333336</td>\n",
       "      <td>66/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>bge_base_en_v1.5</td>\n",
       "      <td>flan_t5_large</td>\n",
       "      <td>74.444443</td>\n",
       "      <td>67/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>bge_large_en_v1.5</td>\n",
       "      <td>gemma_3_1b_it</td>\n",
       "      <td>72.222221</td>\n",
       "      <td>65/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>all_MiniLM_L6_v2</td>\n",
       "      <td>Llama_3.2_1B</td>\n",
       "      <td>73.333336</td>\n",
       "      <td>66/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>instructor_large</td>\n",
       "      <td>flan_t5_xl</td>\n",
       "      <td>71.111115</td>\n",
       "      <td>64/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>e5_base_v2</td>\n",
       "      <td>flan_t5_xl</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>63/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>bge_m3</td>\n",
       "      <td>Llama_3.2_3B_Instruct</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>63/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>static_retrieval_mrl_en_v1</td>\n",
       "      <td>flan_t5_small</td>\n",
       "      <td>58.888889</td>\n",
       "      <td>53/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>bge_large_en</td>\n",
       "      <td>flan_t5_xl</td>\n",
       "      <td>65.555557</td>\n",
       "      <td>59/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>multi_qa_mpnet_base_dot_v1</td>\n",
       "      <td>Meta_Llama_3_8B_Instruct</td>\n",
       "      <td>66.666664</td>\n",
       "      <td>60/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>e5_base_v2</td>\n",
       "      <td>gemma_2_2b</td>\n",
       "      <td>73.333336</td>\n",
       "      <td>66/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>bge_large_en</td>\n",
       "      <td>falcon_7b</td>\n",
       "      <td>73.333336</td>\n",
       "      <td>66/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>all_MiniLM_L6_v2</td>\n",
       "      <td>gemma_3_1b_it</td>\n",
       "      <td>73.333336</td>\n",
       "      <td>66/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>multi_qa_mpnet_base_cos_v1</td>\n",
       "      <td>falcon_7b_instruct</td>\n",
       "      <td>72.222221</td>\n",
       "      <td>65/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>instructor_large</td>\n",
       "      <td>falcon_7b_instruct</td>\n",
       "      <td>68.888885</td>\n",
       "      <td>62/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>static_retrieval_mrl_en_v1</td>\n",
       "      <td>Meta_Llama_3_8B_Instruct</td>\n",
       "      <td>58.888889</td>\n",
       "      <td>53/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>bge_large_en_v1.5</td>\n",
       "      <td>Meta_Llama_3_8B_Instruct</td>\n",
       "      <td>72.222221</td>\n",
       "      <td>65/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>bge_m3</td>\n",
       "      <td>flan_t5_xl</td>\n",
       "      <td>75.555557</td>\n",
       "      <td>68/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>e5_base_v2</td>\n",
       "      <td>falcon_7b</td>\n",
       "      <td>73.333336</td>\n",
       "      <td>66/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>bge_large_en</td>\n",
       "      <td>flan_t5_small</td>\n",
       "      <td>64.444443</td>\n",
       "      <td>58/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>bge_large_en_v1.5</td>\n",
       "      <td>Llama_3.1_8B_Instruct</td>\n",
       "      <td>72.222221</td>\n",
       "      <td>65/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>bge_base_en_v1.5</td>\n",
       "      <td>zephyr_7b_beta</td>\n",
       "      <td>74.444443</td>\n",
       "      <td>67/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>instructor_large</td>\n",
       "      <td>Llama_3.2_1B</td>\n",
       "      <td>68.888885</td>\n",
       "      <td>62/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>bge_m3</td>\n",
       "      <td>gemma_3_1b_it</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>63/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>all_mpnet_base_v2</td>\n",
       "      <td>falcon_7b</td>\n",
       "      <td>67.777779</td>\n",
       "      <td>61/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>all_MiniLM_L6_v2</td>\n",
       "      <td>Llama_3.1_8B_Instruct</td>\n",
       "      <td>73.333336</td>\n",
       "      <td>66/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>multi_qa_mpnet_base_cos_v1</td>\n",
       "      <td>flan_t5_base</td>\n",
       "      <td>66.666664</td>\n",
       "      <td>60/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>static_retrieval_mrl_en_v1</td>\n",
       "      <td>flan_t5_base</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>45/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>static_retrieval_mrl_en_v1</td>\n",
       "      <td>zephyr_7b_beta</td>\n",
       "      <td>58.888889</td>\n",
       "      <td>53/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>all_mpnet_base_v2</td>\n",
       "      <td>Falcon_H1_0.5B_Instruct</td>\n",
       "      <td>67.777779</td>\n",
       "      <td>61/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>e5_base_v2</td>\n",
       "      <td>Falcon_H1_0.5B_Instruct</td>\n",
       "      <td>73.333336</td>\n",
       "      <td>66/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>bge_large_en_v1.5</td>\n",
       "      <td>gemma_2_2b</td>\n",
       "      <td>72.222221</td>\n",
       "      <td>65/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>all_mpnet_base_v2</td>\n",
       "      <td>flan_t5_small</td>\n",
       "      <td>67.777779</td>\n",
       "      <td>61/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>bge_large_en</td>\n",
       "      <td>Meta_Llama_3_8B_Instruct</td>\n",
       "      <td>73.333336</td>\n",
       "      <td>66/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>bge_m3</td>\n",
       "      <td>Meta_Llama_3_8B_Instruct</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>63/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>e5_base_v2</td>\n",
       "      <td>falcon_7b_instruct</td>\n",
       "      <td>73.333336</td>\n",
       "      <td>66/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>all_MiniLM_L12_v2</td>\n",
       "      <td>zephyr_7b_beta</td>\n",
       "      <td>61.111111</td>\n",
       "      <td>55/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>multi_qa_mpnet_base_dot_v1</td>\n",
       "      <td>falcon_7b_instruct</td>\n",
       "      <td>66.666664</td>\n",
       "      <td>60/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>multi_qa_mpnet_base_dot_v1</td>\n",
       "      <td>gemma_3_1b_it</td>\n",
       "      <td>66.666664</td>\n",
       "      <td>60/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>static_retrieval_mrl_en_v1</td>\n",
       "      <td>gemma_2_2b</td>\n",
       "      <td>58.888889</td>\n",
       "      <td>53/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>multi_qa_mpnet_base_cos_v1</td>\n",
       "      <td>flan_t5_large</td>\n",
       "      <td>72.222221</td>\n",
       "      <td>65/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>multi_qa_mpnet_base_cos_v1</td>\n",
       "      <td>Llama_3.2_3B_Instruct</td>\n",
       "      <td>72.222221</td>\n",
       "      <td>65/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>all_mpnet_base_v2</td>\n",
       "      <td>gemma_2_2b_it</td>\n",
       "      <td>67.777779</td>\n",
       "      <td>61/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>instructor_xl</td>\n",
       "      <td>flan_t5_small</td>\n",
       "      <td>71.111115</td>\n",
       "      <td>64/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>instructor_xl</td>\n",
       "      <td>flan_t5_base</td>\n",
       "      <td>68.888885</td>\n",
       "      <td>62/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>instructor_xl</td>\n",
       "      <td>flan_t5_large</td>\n",
       "      <td>78.888885</td>\n",
       "      <td>71/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>instructor_xl</td>\n",
       "      <td>falcon_7b</td>\n",
       "      <td>78.888885</td>\n",
       "      <td>71/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>instructor_xl</td>\n",
       "      <td>falcon_7b_instruct</td>\n",
       "      <td>78.888885</td>\n",
       "      <td>71/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>instructor_xl</td>\n",
       "      <td>Falcon_H1_0.5B_Instruct</td>\n",
       "      <td>78.888885</td>\n",
       "      <td>71/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>instructor_xl</td>\n",
       "      <td>Llama_3.2_1B</td>\n",
       "      <td>78.888885</td>\n",
       "      <td>71/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>instructor_xl</td>\n",
       "      <td>Llama_3.1_8B_Instruct</td>\n",
       "      <td>78.888885</td>\n",
       "      <td>71/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>instructor_xl</td>\n",
       "      <td>Llama_3.2_3B_Instruct</td>\n",
       "      <td>78.888885</td>\n",
       "      <td>71/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>instructor_xl</td>\n",
       "      <td>zephyr_7b_beta</td>\n",
       "      <td>78.888885</td>\n",
       "      <td>71/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>instructor_xl</td>\n",
       "      <td>Meta_Llama_3_8B_Instruct</td>\n",
       "      <td>78.888885</td>\n",
       "      <td>71/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>instructor_xl</td>\n",
       "      <td>gemma_2_2b_it</td>\n",
       "      <td>78.888885</td>\n",
       "      <td>71/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>instructor_xl</td>\n",
       "      <td>gemma_2_2b</td>\n",
       "      <td>78.888885</td>\n",
       "      <td>71/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>instructor_xl</td>\n",
       "      <td>gemma_3_1b_it</td>\n",
       "      <td>78.888885</td>\n",
       "      <td>71/90</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                embedding_model                chat_model  accuracy_percent  \\\n",
       "0    multi_qa_mpnet_base_dot_v1                gemma_2_2b         66.666664   \n",
       "1                  bge_large_en             gemma_2_2b_it         73.333336   \n",
       "2              bge_base_en_v1.5     Llama_3.1_8B_Instruct         74.444443   \n",
       "3                  bge_large_en              flan_t5_base         62.222221   \n",
       "4             all_mpnet_base_v2     Llama_3.1_8B_Instruct         67.777779   \n",
       "5    multi_qa_mpnet_base_cos_v1                flan_t5_xl         68.888885   \n",
       "6    static_retrieval_mrl_en_v1             flan_t5_large         58.888889   \n",
       "7                  bge_large_en             gemma_3_1b_it         73.333336   \n",
       "8             all_mpnet_base_v2             gemma_3_1b_it         67.777779   \n",
       "9              instructor_large             gemma_3_1b_it         68.888885   \n",
       "10   multi_qa_mpnet_base_dot_v1              Llama_3.2_1B         66.666664   \n",
       "11                       bge_m3     Llama_3.1_8B_Instruct         70.000000   \n",
       "12             instructor_large                 falcon_7b         68.888885   \n",
       "13                 bge_large_en             flan_t5_large         73.333336   \n",
       "14   multi_qa_mpnet_base_cos_v1            zephyr_7b_beta         72.222221   \n",
       "15            bge_large_en_v1.5            zephyr_7b_beta         72.222221   \n",
       "16             bge_base_en_v1.5   Falcon_H1_0.5B_Instruct         74.444443   \n",
       "17            all_mpnet_base_v2              Llama_3.2_1B         67.777779   \n",
       "18            bge_large_en_v1.5             flan_t5_large         72.222221   \n",
       "19            bge_large_en_v1.5             flan_t5_small         68.888885   \n",
       "20             all_MiniLM_L6_v2  Meta_Llama_3_8B_Instruct         73.333336   \n",
       "21   multi_qa_mpnet_base_dot_v1     Llama_3.2_3B_Instruct         66.666664   \n",
       "22                   e5_base_v2             gemma_2_2b_it         73.333336   \n",
       "23            bge_large_en_v1.5             gemma_2_2b_it         72.222221   \n",
       "24             bge_base_en_v1.5                 falcon_7b         74.444443   \n",
       "25   static_retrieval_mrl_en_v1                 falcon_7b         58.888889   \n",
       "26             bge_base_en_v1.5              Llama_3.2_1B         74.444443   \n",
       "27                   e5_base_v2             flan_t5_small         68.888885   \n",
       "28            all_mpnet_base_v2              flan_t5_base         63.333332   \n",
       "29   static_retrieval_mrl_en_v1        falcon_7b_instruct         58.888889   \n",
       "30             all_MiniLM_L6_v2        falcon_7b_instruct         73.333336   \n",
       "31            bge_large_en_v1.5              Llama_3.2_1B         72.222221   \n",
       "32                       bge_m3             flan_t5_large         70.000000   \n",
       "33                   e5_base_v2     Llama_3.2_3B_Instruct         73.333336   \n",
       "34            all_mpnet_base_v2             flan_t5_large         67.777779   \n",
       "35             all_MiniLM_L6_v2             flan_t5_large         73.333336   \n",
       "36                       bge_m3        falcon_7b_instruct         70.000000   \n",
       "37   static_retrieval_mrl_en_v1     Llama_3.2_3B_Instruct         58.888889   \n",
       "38             bge_base_en_v1.5  Meta_Llama_3_8B_Instruct         74.444443   \n",
       "39                   e5_base_v2  Meta_Llama_3_8B_Instruct         73.333336   \n",
       "40             bge_base_en_v1.5             flan_t5_small         71.111115   \n",
       "41            bge_large_en_v1.5                 falcon_7b         72.222221   \n",
       "42             all_MiniLM_L6_v2                 falcon_7b         73.333336   \n",
       "43             bge_base_en_v1.5             gemma_3_1b_it         74.444443   \n",
       "44             instructor_large     Llama_3.1_8B_Instruct         68.888885   \n",
       "45   static_retrieval_mrl_en_v1             gemma_3_1b_it         58.888889   \n",
       "46            all_mpnet_base_v2                gemma_2_2b         67.777779   \n",
       "47             all_MiniLM_L6_v2                flan_t5_xl         72.222221   \n",
       "48                       bge_m3              Llama_3.2_1B         70.000000   \n",
       "49             instructor_large             gemma_2_2b_it         68.888885   \n",
       "50             all_MiniLM_L6_v2   Falcon_H1_0.5B_Instruct         73.333336   \n",
       "51             instructor_large              flan_t5_base         58.888889   \n",
       "52                       bge_m3                 falcon_7b         70.000000   \n",
       "53            bge_large_en_v1.5                flan_t5_xl         71.111115   \n",
       "54   multi_qa_mpnet_base_dot_v1                 falcon_7b         66.666664   \n",
       "55             instructor_large             flan_t5_large         68.888885   \n",
       "56   static_retrieval_mrl_en_v1     Llama_3.1_8B_Instruct         58.888889   \n",
       "57   multi_qa_mpnet_base_dot_v1            zephyr_7b_beta         66.666664   \n",
       "58            all_MiniLM_L12_v2     Llama_3.1_8B_Instruct         61.111111   \n",
       "59   multi_qa_mpnet_base_cos_v1   Falcon_H1_0.5B_Instruct         72.222221   \n",
       "60             all_MiniLM_L6_v2             gemma_2_2b_it         73.333336   \n",
       "61             all_MiniLM_L6_v2             flan_t5_small         71.111115   \n",
       "62                   e5_base_v2              flan_t5_base         68.888885   \n",
       "63             instructor_large            zephyr_7b_beta         68.888885   \n",
       "64             instructor_large     Llama_3.2_3B_Instruct         68.888885   \n",
       "65                 bge_large_en        falcon_7b_instruct         73.333336   \n",
       "66            all_MiniLM_L12_v2             gemma_3_1b_it         61.111111   \n",
       "67                   e5_base_v2              Llama_3.2_1B         73.333336   \n",
       "68            all_mpnet_base_v2     Llama_3.2_3B_Instruct         67.777779   \n",
       "69                   e5_base_v2            zephyr_7b_beta         73.333336   \n",
       "70            all_MiniLM_L12_v2                 falcon_7b         61.111111   \n",
       "71                 bge_large_en     Llama_3.1_8B_Instruct         73.333336   \n",
       "72            bge_large_en_v1.5        falcon_7b_instruct         72.222221   \n",
       "73                       bge_m3             gemma_2_2b_it         70.000000   \n",
       "74                       bge_m3                gemma_2_2b         70.000000   \n",
       "75            all_mpnet_base_v2  Meta_Llama_3_8B_Instruct         67.777779   \n",
       "76             bge_base_en_v1.5                flan_t5_xl         68.888885   \n",
       "77   multi_qa_mpnet_base_cos_v1                gemma_2_2b         72.222221   \n",
       "78   multi_qa_mpnet_base_dot_v1             gemma_2_2b_it         66.666664   \n",
       "79             bge_base_en_v1.5              flan_t5_base         64.444443   \n",
       "80             bge_base_en_v1.5        falcon_7b_instruct         74.444443   \n",
       "81                   e5_base_v2             flan_t5_large         73.333336   \n",
       "82             instructor_large                gemma_2_2b         68.888885   \n",
       "83   multi_qa_mpnet_base_dot_v1     Llama_3.1_8B_Instruct         66.666664   \n",
       "84             bge_base_en_v1.5                gemma_2_2b         74.444443   \n",
       "85            bge_large_en_v1.5              flan_t5_base         62.222221   \n",
       "86                 bge_large_en            zephyr_7b_beta         73.333336   \n",
       "87   multi_qa_mpnet_base_cos_v1  Meta_Llama_3_8B_Instruct         72.222221   \n",
       "88   static_retrieval_mrl_en_v1              Llama_3.2_1B         58.888889   \n",
       "89             all_MiniLM_L6_v2              flan_t5_base         68.888885   \n",
       "90   multi_qa_mpnet_base_dot_v1                flan_t5_xl         64.444443   \n",
       "91   multi_qa_mpnet_base_cos_v1              Llama_3.2_1B         72.222221   \n",
       "92                       bge_m3              flan_t5_base         63.333332   \n",
       "93   multi_qa_mpnet_base_cos_v1             flan_t5_small         66.666664   \n",
       "94   multi_qa_mpnet_base_dot_v1             flan_t5_large         66.666664   \n",
       "95   static_retrieval_mrl_en_v1             gemma_2_2b_it         58.888889   \n",
       "96   multi_qa_mpnet_base_dot_v1             flan_t5_small         62.222221   \n",
       "97             bge_base_en_v1.5             gemma_2_2b_it         74.444443   \n",
       "98                 bge_large_en     Llama_3.2_3B_Instruct         73.333336   \n",
       "99             instructor_large  Meta_Llama_3_8B_Instruct         68.888885   \n",
       "100                      bge_m3   Falcon_H1_0.5B_Instruct         70.000000   \n",
       "101            bge_base_en_v1.5     Llama_3.2_3B_Instruct         74.444443   \n",
       "102                  e5_base_v2             gemma_3_1b_it         73.333336   \n",
       "103                bge_large_en                gemma_2_2b         73.333336   \n",
       "104  static_retrieval_mrl_en_v1   Falcon_H1_0.5B_Instruct         58.888889   \n",
       "105  multi_qa_mpnet_base_cos_v1             gemma_3_1b_it         72.222221   \n",
       "106            all_MiniLM_L6_v2                gemma_2_2b         73.333336   \n",
       "107            instructor_large             flan_t5_small         67.777779   \n",
       "108           all_mpnet_base_v2        falcon_7b_instruct         67.777779   \n",
       "109  static_retrieval_mrl_en_v1                flan_t5_xl         62.222221   \n",
       "110            all_MiniLM_L6_v2            zephyr_7b_beta         73.333336   \n",
       "111                      bge_m3            zephyr_7b_beta         70.000000   \n",
       "112           bge_large_en_v1.5     Llama_3.2_3B_Instruct         72.222221   \n",
       "113  multi_qa_mpnet_base_cos_v1             gemma_2_2b_it         72.222221   \n",
       "114            instructor_large   Falcon_H1_0.5B_Instruct         68.888885   \n",
       "115                bge_large_en   Falcon_H1_0.5B_Instruct         73.333336   \n",
       "116  multi_qa_mpnet_base_cos_v1                 falcon_7b         72.222221   \n",
       "117  multi_qa_mpnet_base_dot_v1   Falcon_H1_0.5B_Instruct         66.666664   \n",
       "118           all_mpnet_base_v2            zephyr_7b_beta         67.777779   \n",
       "119           all_mpnet_base_v2                flan_t5_xl         70.000000   \n",
       "120  multi_qa_mpnet_base_cos_v1     Llama_3.1_8B_Instruct         72.222221   \n",
       "121           bge_large_en_v1.5   Falcon_H1_0.5B_Instruct         72.222221   \n",
       "122                bge_large_en              Llama_3.2_1B         73.333336   \n",
       "123  multi_qa_mpnet_base_dot_v1              flan_t5_base         58.888889   \n",
       "124            all_MiniLM_L6_v2     Llama_3.2_3B_Instruct         73.333336   \n",
       "125                      bge_m3             flan_t5_small         71.111115   \n",
       "126                  e5_base_v2     Llama_3.1_8B_Instruct         73.333336   \n",
       "127            bge_base_en_v1.5             flan_t5_large         74.444443   \n",
       "128           bge_large_en_v1.5             gemma_3_1b_it         72.222221   \n",
       "129            all_MiniLM_L6_v2              Llama_3.2_1B         73.333336   \n",
       "130            instructor_large                flan_t5_xl         71.111115   \n",
       "131                  e5_base_v2                flan_t5_xl         70.000000   \n",
       "132                      bge_m3     Llama_3.2_3B_Instruct         70.000000   \n",
       "133  static_retrieval_mrl_en_v1             flan_t5_small         58.888889   \n",
       "134                bge_large_en                flan_t5_xl         65.555557   \n",
       "135  multi_qa_mpnet_base_dot_v1  Meta_Llama_3_8B_Instruct         66.666664   \n",
       "136                  e5_base_v2                gemma_2_2b         73.333336   \n",
       "137                bge_large_en                 falcon_7b         73.333336   \n",
       "138            all_MiniLM_L6_v2             gemma_3_1b_it         73.333336   \n",
       "139  multi_qa_mpnet_base_cos_v1        falcon_7b_instruct         72.222221   \n",
       "140            instructor_large        falcon_7b_instruct         68.888885   \n",
       "141  static_retrieval_mrl_en_v1  Meta_Llama_3_8B_Instruct         58.888889   \n",
       "142           bge_large_en_v1.5  Meta_Llama_3_8B_Instruct         72.222221   \n",
       "143                      bge_m3                flan_t5_xl         75.555557   \n",
       "144                  e5_base_v2                 falcon_7b         73.333336   \n",
       "145                bge_large_en             flan_t5_small         64.444443   \n",
       "146           bge_large_en_v1.5     Llama_3.1_8B_Instruct         72.222221   \n",
       "147            bge_base_en_v1.5            zephyr_7b_beta         74.444443   \n",
       "148            instructor_large              Llama_3.2_1B         68.888885   \n",
       "149                      bge_m3             gemma_3_1b_it         70.000000   \n",
       "150           all_mpnet_base_v2                 falcon_7b         67.777779   \n",
       "151            all_MiniLM_L6_v2     Llama_3.1_8B_Instruct         73.333336   \n",
       "152  multi_qa_mpnet_base_cos_v1              flan_t5_base         66.666664   \n",
       "153  static_retrieval_mrl_en_v1              flan_t5_base         50.000000   \n",
       "154  static_retrieval_mrl_en_v1            zephyr_7b_beta         58.888889   \n",
       "155           all_mpnet_base_v2   Falcon_H1_0.5B_Instruct         67.777779   \n",
       "156                  e5_base_v2   Falcon_H1_0.5B_Instruct         73.333336   \n",
       "157           bge_large_en_v1.5                gemma_2_2b         72.222221   \n",
       "158           all_mpnet_base_v2             flan_t5_small         67.777779   \n",
       "159                bge_large_en  Meta_Llama_3_8B_Instruct         73.333336   \n",
       "160                      bge_m3  Meta_Llama_3_8B_Instruct         70.000000   \n",
       "161                  e5_base_v2        falcon_7b_instruct         73.333336   \n",
       "162           all_MiniLM_L12_v2            zephyr_7b_beta         61.111111   \n",
       "163  multi_qa_mpnet_base_dot_v1        falcon_7b_instruct         66.666664   \n",
       "164  multi_qa_mpnet_base_dot_v1             gemma_3_1b_it         66.666664   \n",
       "165  static_retrieval_mrl_en_v1                gemma_2_2b         58.888889   \n",
       "166  multi_qa_mpnet_base_cos_v1             flan_t5_large         72.222221   \n",
       "167  multi_qa_mpnet_base_cos_v1     Llama_3.2_3B_Instruct         72.222221   \n",
       "168           all_mpnet_base_v2             gemma_2_2b_it         67.777779   \n",
       "169               instructor_xl             flan_t5_small         71.111115   \n",
       "170               instructor_xl              flan_t5_base         68.888885   \n",
       "171               instructor_xl             flan_t5_large         78.888885   \n",
       "172               instructor_xl                 falcon_7b         78.888885   \n",
       "173               instructor_xl        falcon_7b_instruct         78.888885   \n",
       "174               instructor_xl   Falcon_H1_0.5B_Instruct         78.888885   \n",
       "175               instructor_xl              Llama_3.2_1B         78.888885   \n",
       "176               instructor_xl     Llama_3.1_8B_Instruct         78.888885   \n",
       "177               instructor_xl     Llama_3.2_3B_Instruct         78.888885   \n",
       "178               instructor_xl            zephyr_7b_beta         78.888885   \n",
       "179               instructor_xl  Meta_Llama_3_8B_Instruct         78.888885   \n",
       "180               instructor_xl             gemma_2_2b_it         78.888885   \n",
       "181               instructor_xl                gemma_2_2b         78.888885   \n",
       "182               instructor_xl             gemma_3_1b_it         78.888885   \n",
       "\n",
       "    raw_score  \n",
       "0       60/90  \n",
       "1       66/90  \n",
       "2       67/90  \n",
       "3       56/90  \n",
       "4       61/90  \n",
       "5       62/90  \n",
       "6       53/90  \n",
       "7       66/90  \n",
       "8       61/90  \n",
       "9       62/90  \n",
       "10      60/90  \n",
       "11      63/90  \n",
       "12      62/90  \n",
       "13      66/90  \n",
       "14      65/90  \n",
       "15      65/90  \n",
       "16      67/90  \n",
       "17      61/90  \n",
       "18      65/90  \n",
       "19      62/90  \n",
       "20      66/90  \n",
       "21      60/90  \n",
       "22      66/90  \n",
       "23      65/90  \n",
       "24      67/90  \n",
       "25      53/90  \n",
       "26      67/90  \n",
       "27      62/90  \n",
       "28      57/90  \n",
       "29      53/90  \n",
       "30      66/90  \n",
       "31      65/90  \n",
       "32      63/90  \n",
       "33      66/90  \n",
       "34      61/90  \n",
       "35      66/90  \n",
       "36      63/90  \n",
       "37      53/90  \n",
       "38      67/90  \n",
       "39      66/90  \n",
       "40      64/90  \n",
       "41      65/90  \n",
       "42      66/90  \n",
       "43      67/90  \n",
       "44      62/90  \n",
       "45      53/90  \n",
       "46      61/90  \n",
       "47      65/90  \n",
       "48      63/90  \n",
       "49      62/90  \n",
       "50      66/90  \n",
       "51      53/90  \n",
       "52      63/90  \n",
       "53      64/90  \n",
       "54      60/90  \n",
       "55      62/90  \n",
       "56      53/90  \n",
       "57      60/90  \n",
       "58      55/90  \n",
       "59      65/90  \n",
       "60      66/90  \n",
       "61      64/90  \n",
       "62      62/90  \n",
       "63      62/90  \n",
       "64      62/90  \n",
       "65      66/90  \n",
       "66      55/90  \n",
       "67      66/90  \n",
       "68      61/90  \n",
       "69      66/90  \n",
       "70      55/90  \n",
       "71      66/90  \n",
       "72      65/90  \n",
       "73      63/90  \n",
       "74      63/90  \n",
       "75      61/90  \n",
       "76      62/90  \n",
       "77      65/90  \n",
       "78      60/90  \n",
       "79      58/90  \n",
       "80      67/90  \n",
       "81      66/90  \n",
       "82      62/90  \n",
       "83      60/90  \n",
       "84      67/90  \n",
       "85      56/90  \n",
       "86      66/90  \n",
       "87      65/90  \n",
       "88      53/90  \n",
       "89      62/90  \n",
       "90      58/90  \n",
       "91      65/90  \n",
       "92      57/90  \n",
       "93      60/90  \n",
       "94      60/90  \n",
       "95      53/90  \n",
       "96      56/90  \n",
       "97      67/90  \n",
       "98      66/90  \n",
       "99      62/90  \n",
       "100     63/90  \n",
       "101     67/90  \n",
       "102     66/90  \n",
       "103     66/90  \n",
       "104     53/90  \n",
       "105     65/90  \n",
       "106     66/90  \n",
       "107     61/90  \n",
       "108     61/90  \n",
       "109     56/90  \n",
       "110     66/90  \n",
       "111     63/90  \n",
       "112     65/90  \n",
       "113     65/90  \n",
       "114     62/90  \n",
       "115     66/90  \n",
       "116     65/90  \n",
       "117     60/90  \n",
       "118     61/90  \n",
       "119     63/90  \n",
       "120     65/90  \n",
       "121     65/90  \n",
       "122     66/90  \n",
       "123     53/90  \n",
       "124     66/90  \n",
       "125     64/90  \n",
       "126     66/90  \n",
       "127     67/90  \n",
       "128     65/90  \n",
       "129     66/90  \n",
       "130     64/90  \n",
       "131     63/90  \n",
       "132     63/90  \n",
       "133     53/90  \n",
       "134     59/90  \n",
       "135     60/90  \n",
       "136     66/90  \n",
       "137     66/90  \n",
       "138     66/90  \n",
       "139     65/90  \n",
       "140     62/90  \n",
       "141     53/90  \n",
       "142     65/90  \n",
       "143     68/90  \n",
       "144     66/90  \n",
       "145     58/90  \n",
       "146     65/90  \n",
       "147     67/90  \n",
       "148     62/90  \n",
       "149     63/90  \n",
       "150     61/90  \n",
       "151     66/90  \n",
       "152     60/90  \n",
       "153     45/90  \n",
       "154     53/90  \n",
       "155     61/90  \n",
       "156     66/90  \n",
       "157     65/90  \n",
       "158     61/90  \n",
       "159     66/90  \n",
       "160     63/90  \n",
       "161     66/90  \n",
       "162     55/90  \n",
       "163     60/90  \n",
       "164     60/90  \n",
       "165     53/90  \n",
       "166     65/90  \n",
       "167     65/90  \n",
       "168     61/90  \n",
       "169     64/90  \n",
       "170     62/90  \n",
       "171     71/90  \n",
       "172     71/90  \n",
       "173     71/90  \n",
       "174     71/90  \n",
       "175     71/90  \n",
       "176     71/90  \n",
       "177     71/90  \n",
       "178     71/90  \n",
       "179     71/90  \n",
       "180     71/90  \n",
       "181     71/90  \n",
       "182     71/90  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f0747056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv(\"temp2.txt\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3966c1d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 Embedding Model Performance Rankings:\n",
      "================================================================================\n",
      "Rank Embedding Model                     Avg %    Std    Count  Avg Raw   \n",
      "--------------------------------------------------------------------------------\n",
      "1    instructor_xl                       77.62000274658203 3.26   14     69.9/90   \n",
      "2    bge_base_en_v1.5                    73.19000244140625 2.91   15     65.9/90   \n",
      "3    all_MiniLM_L6_v2                    72.80999755859375 1.25   15     65.5/90   \n",
      "4    e5_base_v2                          72.5199966430664 1.7    15     65.3/90   \n",
      "5    bge_large_en                        71.4800033569336 3.89   15     64.3/90   \n",
      "6    bge_large_en_v1.5                   71.26000213623047 2.65   15     64.1/90   \n",
      "7    multi_qa_mpnet_base_cos_v1          71.26000213623047 2.05   15     64.1/90   \n",
      "8    bge_m3                              70.0     2.34   15     63.0/90   \n",
      "9    instructor_large                    68.30000305175781 2.68   15     61.5/90   \n",
      "10   all_mpnet_base_v2                   67.62999725341797 1.32   15     60.9/90   \n",
      "11   multi_qa_mpnet_base_dot_v1          65.69999694824219 2.26   15     59.1/90   \n",
      "12   all_MiniLM_L12_v2                   61.11000061035156 0.0    4      55.0/90   \n",
      "13   static_retrieval_mrl_en_v1          58.52000045776367 2.51   15     52.7/90   \n",
      "\n",
      "📊 Detailed Embedding Model Statistics:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>avg_accuracy</th>\n",
       "      <th>std_accuracy</th>\n",
       "      <th>count_models</th>\n",
       "      <th>avg_raw_score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>embedding_model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>instructor_xl</th>\n",
       "      <td>77.620003</td>\n",
       "      <td>3.26</td>\n",
       "      <td>14</td>\n",
       "      <td>69.9/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bge_base_en_v1.5</th>\n",
       "      <td>73.190002</td>\n",
       "      <td>2.91</td>\n",
       "      <td>15</td>\n",
       "      <td>65.9/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all_MiniLM_L6_v2</th>\n",
       "      <td>72.809998</td>\n",
       "      <td>1.25</td>\n",
       "      <td>15</td>\n",
       "      <td>65.5/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e5_base_v2</th>\n",
       "      <td>72.519997</td>\n",
       "      <td>1.70</td>\n",
       "      <td>15</td>\n",
       "      <td>65.3/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bge_large_en</th>\n",
       "      <td>71.480003</td>\n",
       "      <td>3.89</td>\n",
       "      <td>15</td>\n",
       "      <td>64.3/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bge_large_en_v1.5</th>\n",
       "      <td>71.260002</td>\n",
       "      <td>2.65</td>\n",
       "      <td>15</td>\n",
       "      <td>64.1/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>multi_qa_mpnet_base_cos_v1</th>\n",
       "      <td>71.260002</td>\n",
       "      <td>2.05</td>\n",
       "      <td>15</td>\n",
       "      <td>64.1/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bge_m3</th>\n",
       "      <td>70.000000</td>\n",
       "      <td>2.34</td>\n",
       "      <td>15</td>\n",
       "      <td>63.0/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>instructor_large</th>\n",
       "      <td>68.300003</td>\n",
       "      <td>2.68</td>\n",
       "      <td>15</td>\n",
       "      <td>61.5/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all_mpnet_base_v2</th>\n",
       "      <td>67.629997</td>\n",
       "      <td>1.32</td>\n",
       "      <td>15</td>\n",
       "      <td>60.9/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>multi_qa_mpnet_base_dot_v1</th>\n",
       "      <td>65.699997</td>\n",
       "      <td>2.26</td>\n",
       "      <td>15</td>\n",
       "      <td>59.1/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all_MiniLM_L12_v2</th>\n",
       "      <td>61.110001</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4</td>\n",
       "      <td>55.0/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>static_retrieval_mrl_en_v1</th>\n",
       "      <td>58.520000</td>\n",
       "      <td>2.51</td>\n",
       "      <td>15</td>\n",
       "      <td>52.7/90</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            avg_accuracy  std_accuracy  count_models  \\\n",
       "embedding_model                                                        \n",
       "instructor_xl                  77.620003          3.26            14   \n",
       "bge_base_en_v1.5               73.190002          2.91            15   \n",
       "all_MiniLM_L6_v2               72.809998          1.25            15   \n",
       "e5_base_v2                     72.519997          1.70            15   \n",
       "bge_large_en                   71.480003          3.89            15   \n",
       "bge_large_en_v1.5              71.260002          2.65            15   \n",
       "multi_qa_mpnet_base_cos_v1     71.260002          2.05            15   \n",
       "bge_m3                         70.000000          2.34            15   \n",
       "instructor_large               68.300003          2.68            15   \n",
       "all_mpnet_base_v2              67.629997          1.32            15   \n",
       "multi_qa_mpnet_base_dot_v1     65.699997          2.26            15   \n",
       "all_MiniLM_L12_v2              61.110001          0.00             4   \n",
       "static_retrieval_mrl_en_v1     58.520000          2.51            15   \n",
       "\n",
       "                           avg_raw_score  \n",
       "embedding_model                           \n",
       "instructor_xl                    69.9/90  \n",
       "bge_base_en_v1.5                 65.9/90  \n",
       "all_MiniLM_L6_v2                 65.5/90  \n",
       "e5_base_v2                       65.3/90  \n",
       "bge_large_en                     64.3/90  \n",
       "bge_large_en_v1.5                64.1/90  \n",
       "multi_qa_mpnet_base_cos_v1       64.1/90  \n",
       "bge_m3                           63.0/90  \n",
       "instructor_large                 61.5/90  \n",
       "all_mpnet_base_v2                60.9/90  \n",
       "multi_qa_mpnet_base_dot_v1       59.1/90  \n",
       "all_MiniLM_L12_v2                55.0/90  \n",
       "static_retrieval_mrl_en_v1       52.7/90  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Group by embedding model and calculate average scores\n",
    "embedding_stats = df.groupby('embedding_model').agg({\n",
    "    'accuracy_percent': ['mean', 'std', 'count'],\n",
    "    'raw_score': lambda x: f\"{x.apply(lambda s: int(s.split('/')[0])).mean():.1f}/90\"\n",
    "}).round(2)\n",
    "\n",
    "# Flatten column names\n",
    "embedding_stats.columns = ['avg_accuracy', 'std_accuracy', 'count_models', 'avg_raw_score']\n",
    "\n",
    "# Sort by average accuracy (descending)\n",
    "embedding_stats_sorted = embedding_stats.sort_values('avg_accuracy', ascending=False)\n",
    "\n",
    "print(\"🎯 Embedding Model Performance Rankings:\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"{'Rank':<4} {'Embedding Model':<35} {'Avg %':<8} {'Std':<6} {'Count':<6} {'Avg Raw':<10}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for rank, (model, row) in enumerate(embedding_stats_sorted.iterrows(), 1):\n",
    "    print(f\"{rank:<4} {model:<35} {row['avg_accuracy']:<8} {row['std_accuracy']:<6} {row['count_models']:<6} {row['avg_raw_score']:<10}\")\n",
    "\n",
    "# Display as DataFrame\n",
    "print(f\"\\n📊 Detailed Embedding Model Statistics:\")\n",
    "embedding_stats_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bf6fb48f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💬 Chat Model Performance Rankings:\n",
      "================================================================================\n",
      "Rank Chat Model                          Avg %    Std    Count  Avg Raw   \n",
      "--------------------------------------------------------------------------------\n",
      "1    Falcon_H1_0.5B_Instruct             70.83000183105469 4.99   12     63.8/90   \n",
      "2    Llama_3.2_1B                        70.83000183105469 4.99   12     63.8/90   \n",
      "3    Llama_3.2_3B_Instruct               70.83000183105469 4.99   12     63.8/90   \n",
      "4    Meta_Llama_3_8B_Instruct            70.83000183105469 4.99   12     63.8/90   \n",
      "5    falcon_7b_instruct                  70.83000183105469 4.99   12     63.8/90   \n",
      "6    flan_t5_large                       70.83000183105469 4.99   12     63.8/90   \n",
      "7    gemma_2_2b                          70.83000183105469 4.99   12     63.8/90   \n",
      "8    gemma_2_2b_it                       70.83000183105469 4.99   12     63.8/90   \n",
      "9    Llama_3.1_8B_Instruct               70.08999633789062 5.49   13     63.1/90   \n",
      "10   falcon_7b                           70.08999633789062 5.49   13     63.1/90   \n",
      "11   gemma_3_1b_it                       70.08999633789062 5.49   13     63.1/90   \n",
      "12   zephyr_7b_beta                      70.08999633789062 5.49   13     63.1/90   \n",
      "13   flan_t5_xl                          69.08999633789062 3.78   11     62.2/90   \n",
      "14   flan_t5_small                       67.5     3.91   12     60.8/90   \n",
      "15   flan_t5_base                        63.060001373291016 5.43   12     56.8/90   \n",
      "\n",
      "📊 Detailed Chat Model Statistics:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>avg_accuracy</th>\n",
       "      <th>std_accuracy</th>\n",
       "      <th>count_embeddings</th>\n",
       "      <th>avg_raw_score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chat_model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Falcon_H1_0.5B_Instruct</th>\n",
       "      <td>70.830002</td>\n",
       "      <td>4.99</td>\n",
       "      <td>12</td>\n",
       "      <td>63.8/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Llama_3.2_1B</th>\n",
       "      <td>70.830002</td>\n",
       "      <td>4.99</td>\n",
       "      <td>12</td>\n",
       "      <td>63.8/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Llama_3.2_3B_Instruct</th>\n",
       "      <td>70.830002</td>\n",
       "      <td>4.99</td>\n",
       "      <td>12</td>\n",
       "      <td>63.8/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Meta_Llama_3_8B_Instruct</th>\n",
       "      <td>70.830002</td>\n",
       "      <td>4.99</td>\n",
       "      <td>12</td>\n",
       "      <td>63.8/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>falcon_7b_instruct</th>\n",
       "      <td>70.830002</td>\n",
       "      <td>4.99</td>\n",
       "      <td>12</td>\n",
       "      <td>63.8/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>flan_t5_large</th>\n",
       "      <td>70.830002</td>\n",
       "      <td>4.99</td>\n",
       "      <td>12</td>\n",
       "      <td>63.8/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemma_2_2b</th>\n",
       "      <td>70.830002</td>\n",
       "      <td>4.99</td>\n",
       "      <td>12</td>\n",
       "      <td>63.8/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemma_2_2b_it</th>\n",
       "      <td>70.830002</td>\n",
       "      <td>4.99</td>\n",
       "      <td>12</td>\n",
       "      <td>63.8/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Llama_3.1_8B_Instruct</th>\n",
       "      <td>70.089996</td>\n",
       "      <td>5.49</td>\n",
       "      <td>13</td>\n",
       "      <td>63.1/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>falcon_7b</th>\n",
       "      <td>70.089996</td>\n",
       "      <td>5.49</td>\n",
       "      <td>13</td>\n",
       "      <td>63.1/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemma_3_1b_it</th>\n",
       "      <td>70.089996</td>\n",
       "      <td>5.49</td>\n",
       "      <td>13</td>\n",
       "      <td>63.1/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zephyr_7b_beta</th>\n",
       "      <td>70.089996</td>\n",
       "      <td>5.49</td>\n",
       "      <td>13</td>\n",
       "      <td>63.1/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>flan_t5_xl</th>\n",
       "      <td>69.089996</td>\n",
       "      <td>3.78</td>\n",
       "      <td>11</td>\n",
       "      <td>62.2/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>flan_t5_small</th>\n",
       "      <td>67.500000</td>\n",
       "      <td>3.91</td>\n",
       "      <td>12</td>\n",
       "      <td>60.8/90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>flan_t5_base</th>\n",
       "      <td>63.060001</td>\n",
       "      <td>5.43</td>\n",
       "      <td>12</td>\n",
       "      <td>56.8/90</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          avg_accuracy  std_accuracy  count_embeddings  \\\n",
       "chat_model                                                               \n",
       "Falcon_H1_0.5B_Instruct      70.830002          4.99                12   \n",
       "Llama_3.2_1B                 70.830002          4.99                12   \n",
       "Llama_3.2_3B_Instruct        70.830002          4.99                12   \n",
       "Meta_Llama_3_8B_Instruct     70.830002          4.99                12   \n",
       "falcon_7b_instruct           70.830002          4.99                12   \n",
       "flan_t5_large                70.830002          4.99                12   \n",
       "gemma_2_2b                   70.830002          4.99                12   \n",
       "gemma_2_2b_it                70.830002          4.99                12   \n",
       "Llama_3.1_8B_Instruct        70.089996          5.49                13   \n",
       "falcon_7b                    70.089996          5.49                13   \n",
       "gemma_3_1b_it                70.089996          5.49                13   \n",
       "zephyr_7b_beta               70.089996          5.49                13   \n",
       "flan_t5_xl                   69.089996          3.78                11   \n",
       "flan_t5_small                67.500000          3.91                12   \n",
       "flan_t5_base                 63.060001          5.43                12   \n",
       "\n",
       "                         avg_raw_score  \n",
       "chat_model                              \n",
       "Falcon_H1_0.5B_Instruct        63.8/90  \n",
       "Llama_3.2_1B                   63.8/90  \n",
       "Llama_3.2_3B_Instruct          63.8/90  \n",
       "Meta_Llama_3_8B_Instruct       63.8/90  \n",
       "falcon_7b_instruct             63.8/90  \n",
       "flan_t5_large                  63.8/90  \n",
       "gemma_2_2b                     63.8/90  \n",
       "gemma_2_2b_it                  63.8/90  \n",
       "Llama_3.1_8B_Instruct          63.1/90  \n",
       "falcon_7b                      63.1/90  \n",
       "gemma_3_1b_it                  63.1/90  \n",
       "zephyr_7b_beta                 63.1/90  \n",
       "flan_t5_xl                     62.2/90  \n",
       "flan_t5_small                  60.8/90  \n",
       "flan_t5_base                   56.8/90  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Group by chat model and calculate average scores\n",
    "chat_stats = df.groupby('chat_model').agg({\n",
    "    'accuracy_percent': ['mean', 'std', 'count'],\n",
    "    'raw_score': lambda x: f\"{x.apply(lambda s: int(s.split('/')[0])).mean():.1f}/90\"\n",
    "}).round(2)\n",
    "\n",
    "# Flatten column names\n",
    "chat_stats.columns = ['avg_accuracy', 'std_accuracy', 'count_embeddings', 'avg_raw_score']\n",
    "\n",
    "# Sort by average accuracy (descending)\n",
    "chat_stats_sorted = chat_stats.sort_values('avg_accuracy', ascending=False)\n",
    "\n",
    "print(\"💬 Chat Model Performance Rankings:\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"{'Rank':<4} {'Chat Model':<35} {'Avg %':<8} {'Std':<6} {'Count':<6} {'Avg Raw':<10}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for rank, (model, row) in enumerate(chat_stats_sorted.iterrows(), 1):\n",
    "    print(f\"{rank:<4} {model:<35} {row['avg_accuracy']:<8} {row['std_accuracy']:<6} {row['count_embeddings']:<6} {row['avg_raw_score']:<10}\")\n",
    "\n",
    "# Display as DataFrame\n",
    "print(f\"\\n📊 Detailed Chat Model Statistics:\")\n",
    "chat_stats_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f209b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
