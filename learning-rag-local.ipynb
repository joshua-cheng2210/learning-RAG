{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-24T05:28:33.351626Z",
     "iopub.status.busy": "2025-09-24T05:28:33.351384Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # Cell 1: Core LangChain\n",
    "# !pip install -q langchain\n",
    "# !pip install torch\n",
    "# !pip install gc\n",
    "\n",
    "# # Cell 2: LangChain integrations\n",
    "# !pip install -q langchain-community langchain-huggingface langchain-chroma langchain_experimental\n",
    "# # !pip install -q langchain_google_genai\n",
    "# !pip install -q huggingface_hub\n",
    "# # Cell 3: ML libraries\n",
    "# !pip install -q sentence-transformers transformers chromadb\n",
    "\n",
    "# # Cell 4: Utilities\n",
    "# !pip install -q python-dotenv torch unstructured\n",
    "# !pip install -q hf_xet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-08-20T04:22:29.042814Z",
     "iopub.status.busy": "2025-08-20T04:22:29.042568Z",
     "iopub.status.idle": "2025-08-20T04:22:56.814097Z",
     "shell.execute_reply": "2025-08-20T04:22:56.813262Z",
     "shell.execute_reply.started": "2025-08-20T04:22:29.042791Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "# from langchain_google_genai import GoogleGenerativeAIEmbeddings  \n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import json\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from transformers import pipeline\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import gc\n",
    "import warnings\n",
    "from huggingface_hub import login\n",
    "from dotenv import load_dotenv\n",
    "import datetime\n",
    "from transformers.utils import logging\n",
    "logging.set_verbosity_error()\n",
    "logging.disable_progress_bar()\n",
    "\n",
    "# os.environ[\"HF_HUB_DISABLE_PROGRESS_BARS\"] = \"1\"\n",
    "# os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "# warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "# warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# # Add this to your existing imports cell\n",
    "# from transformers import logging\n",
    "# logging.set_verbosity_error()  # Only show errors, not info/warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-20T04:22:56.816080Z",
     "iopub.status.busy": "2025-08-20T04:22:56.815335Z",
     "iopub.status.idle": "2025-08-20T04:22:56.911760Z",
     "shell.execute_reply": "2025-08-20T04:22:56.911018Z",
     "shell.execute_reply.started": "2025-08-20T04:22:56.816057Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "# GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "HUGGING_FACE_API = os.getenv(\"HUGGING_FACE_TOKEN\")\n",
    "login(token=HUGGING_FACE_API)\n",
    "model_response_directory = f\"quiz_results\"\n",
    "raw_knowledge_directory = f\"books\"\n",
    "test_questions_directory = f\"test_questions.json\"\n",
    "persist_filepath = f\"model_results.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-20T04:22:56.913660Z",
     "iopub.status.busy": "2025-08-20T04:22:56.913442Z",
     "iopub.status.idle": "2025-08-20T04:22:56.924659Z",
     "shell.execute_reply": "2025-08-20T04:22:56.923741Z",
     "shell.execute_reply.started": "2025-08-20T04:22:56.913644Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class DatabaseManager:\n",
    "    def __init__(self, embedding_model_name=\"sentence-transformers/all-MiniLM-L12-v2\", \n",
    "                 embedding_model_type=\"huggingface\"):\n",
    "        \"\"\"\n",
    "        Initialize DatabaseManager with specified embedding model.\n",
    "        \n",
    "        Args:\n",
    "            embedding_model_name: Name of the embedding model\n",
    "            embedding_model_type: Type of model (\"huggingface\" or \"gemini\")\n",
    "        \"\"\"\n",
    "        self.embedding_model_name = embedding_model_name\n",
    "        self.embedding_model_type = embedding_model_type\n",
    "        \n",
    "        # Initialize embedding function based on type\n",
    "        # if embedding_model_type == \"gemini\":\n",
    "        #     api_key = os.getenv(\"GEMINI_API_KEY\")  # Changed from GEMINI_API_KEY\n",
    "        #     if not api_key:\n",
    "        #         raise ValueError(\"GEMINI_API_KEY environment variable is required for Gemini models\")\n",
    "            \n",
    "        #     # Extract model name (remove 'gemini/' prefix)\n",
    "        #     model_name = self.embedding_model_name.replace(\"gemini/\", \"\")\n",
    "        #     self.embedding_function = GoogleGenerativeAIEmbeddings(\n",
    "        #         model=model_name,\n",
    "        #         google_api_key=api_key\n",
    "        #     )\n",
    "        # el\n",
    "        if embedding_model_type == \"huggingface\":\n",
    "            self.embedding_function = HuggingFaceEmbeddings(model_name=embedding_model_name)\n",
    "        else:  # huggingface\n",
    "            self.embedding_model_type == \"huggingface\"\n",
    "            print(f\"{embedding_model_type} embedding_model_type not recognized. Using {self.embedding_model_type}\")\n",
    "            self.embedding_function = HuggingFaceEmbeddings(model_name=embedding_model_name)\n",
    "            \n",
    "        print(f\"Initialized DatabaseManager: Embedding embedding model: {self.embedding_model_name} ({self.embedding_model_type})\")\n",
    "        \n",
    "    # Rest of your DatabaseManager methods remain the same...\n",
    "    def load_documents(self, data_path):\n",
    "        \"\"\"Load documents from the specified directory.\"\"\"\n",
    "        try:\n",
    "            loader = DirectoryLoader(data_path, glob=\"*.md\")\n",
    "            documents = loader.load()\n",
    "            # print(f\"Loaded {len(documents)} documents from {data_path}\")\n",
    "            return documents\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading documents: {e}\")\n",
    "            return []\n",
    "\n",
    "    def split_text(self, documents):\n",
    "        \"\"\"Split documents into chunks.\"\"\"\n",
    "        try:\n",
    "            text_splitter = RecursiveCharacterTextSplitter(\n",
    "                chunk_size=500,\n",
    "                chunk_overlap=150,\n",
    "                length_function=len,\n",
    "                add_start_index=True,\n",
    "            )\n",
    "            chunks = text_splitter.split_documents(documents)\n",
    "            # print(f\"Split into {len(chunks)} chunks\")\n",
    "            return chunks\n",
    "        except Exception as e:\n",
    "            print(f\"Error splitting text: {e}\")\n",
    "            return []\n",
    "\n",
    "    def save_to_chroma(self, chunks, persist_directory):\n",
    "        \"\"\"Save document chunks to Chroma database.\"\"\"\n",
    "        # try:\n",
    "        #     existing_db = Chroma(persist_directory=persist_directory, embedding_function=self.embedding_function)\n",
    "        #     existing_db.delete_collection()\n",
    "        # except Exception as e:\n",
    "        #     pass\n",
    "            \n",
    "        try:\n",
    "            import time\n",
    "            for i in range(3):\n",
    "                try:\n",
    "                    # Create directory if it doesn't exist\n",
    "                    if os.path.exists(persist_directory):\n",
    "                        print(f\"attempt {i+1}: trying to remove {persist_directory}\")\n",
    "                        shutil.rmtree(persist_directory)\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    print(f\"error removing {persist_directory}: {e}\")\n",
    "                    if i < 2:\n",
    "                        gc.collect()\n",
    "                        time.sleep(1)\n",
    "                        \n",
    "                        \n",
    "            # if os.path.exists(persist_directory):\n",
    "            #     print(f\"path exist: {persist_directory}\")\n",
    "            # else:\n",
    "            #     print(f\"path dont exist: {persist_directory}\")\n",
    "                \n",
    "            print(f\"making directory: {persist_directory}\")\n",
    "            os.makedirs(persist_directory, exist_ok=True)\n",
    "            \n",
    "            db = Chroma.from_documents(\n",
    "                chunks, \n",
    "                self.embedding_function, \n",
    "                persist_directory=persist_directory\n",
    "            )\n",
    "            # print(f\"Saved {len(chunks)} chunks to Chroma database at {persist_directory}\")\n",
    "            return db\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving to Chroma: {e}\")\n",
    "            return None\n",
    "\n",
    "    def generate_data_store(self, data_path=\"books\", persist_directory=\"chroma\"):\n",
    "        \"\"\"Complete pipeline: load documents, split text, and save to database.\"\"\"\n",
    "        \n",
    "        # Load documents\n",
    "        documents = self.load_documents(data_path)\n",
    "        if not documents:\n",
    "            return False\n",
    "        \n",
    "        # Split into chunks\n",
    "        chunks = self.split_text(documents)\n",
    "        if not chunks:\n",
    "            return False\n",
    "        \n",
    "        # Save to database\n",
    "        db = self.save_to_chroma(chunks, persist_directory)\n",
    "        return db is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-20T04:22:56.926047Z",
     "iopub.status.busy": "2025-08-20T04:22:56.925715Z",
     "iopub.status.idle": "2025-08-20T04:22:56.956332Z",
     "shell.execute_reply": "2025-08-20T04:22:56.955389Z",
     "shell.execute_reply.started": "2025-08-20T04:22:56.926026Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class QueryEngine:\n",
    "    def __init__(self, persist_directory=\"chroma\", \n",
    "                 embedding_model_name=\"sentence-transformers/all-MiniLM-L12-v2\",\n",
    "                 embedding_model_type=\"huggingface\",\n",
    "                 text_model_name=\"google/flan-t5-base\"):\n",
    "        \"\"\"\n",
    "        Initialize QueryEngine with specified models.\n",
    "        \n",
    "        Args:\n",
    "            persist_directory: Path to the Chroma database\n",
    "            embedding_model_name: Name of the embedding model\n",
    "            embedding_model_type: Type of embedding model (\"huggingface\" or \"gemini\")\n",
    "            text_model_name: Name of the text generation model\n",
    "        \"\"\"\n",
    "        self.persist_directory = persist_directory\n",
    "        self.embedding_model_name = embedding_model_name\n",
    "        self.embedding_model_type = embedding_model_type\n",
    "        self.text_model_name = text_model_name\n",
    "        \n",
    "        # Initialize embedding function based on type\n",
    "        # if embedding_model_type == \"gemini\":\n",
    "        #     api_key = os.getenv(\"GEMINI_API_KEY\")  # Changed from GEMINI_API_KEY\n",
    "        #     if not api_key:\n",
    "        #         raise ValueError(\"GEMINI_API_KEY environment variable is required for Gemini models\")\n",
    "            \n",
    "        #     # Extract model name (remove 'gemini/' prefix)\n",
    "        #     model_name = self.embedding_model_name.replace(\"gemini/\", \"\")\n",
    "        #     self.embedding_function = GoogleGenerativeAIEmbeddings(\n",
    "        #         model=model_name,\n",
    "        #         google_api_key=api_key\n",
    "        #     )\n",
    "        # el\n",
    "        if embedding_model_type == \"huggingface\":\n",
    "            self.embedding_function = HuggingFaceEmbeddings(model_name=embedding_model_name)\n",
    "        else:  # huggingface\n",
    "            self.embedding_model_type == \"huggingface\"\n",
    "            print(f\"{embedding_model_type} embedding_model_type not recognized. Using {self.embedding_model_type}\")\n",
    "            self.embedding_function = HuggingFaceEmbeddings(model_name=embedding_model_name)\n",
    "        \n",
    "        # Initialize text generation model\n",
    "        if self.text_model_name.startswith(\"google/flan\"):\n",
    "            self.hf_pipeline = pipeline(\n",
    "                \"text2text-generation\",\n",
    "                model=self.text_model_name,\n",
    "                max_length=768,\n",
    "                max_new_tokens=100,\n",
    "            )\n",
    "        elif self.text_model_name.startswith(\"mistralai/\"):\n",
    "            self.hf_pipeline = pipeline(\n",
    "                \"text-generation\",  # Mistral uses text-generation\n",
    "                model=self.text_model_name,\n",
    "                max_new_tokens=100,     # Limit output length\n",
    "                do_sample=True,\n",
    "                temperature=0.3,        # Lower temp for more focused answers\n",
    "                pad_token_id=2,         # Mistral's pad token\n",
    "                return_full_text=False, # Only return generated text\n",
    "            )\n",
    "        elif self.text_model_name.startswith(\"gpt\") or self.text_model_name.startswith(\"distilgpt\"):\n",
    "            # Special handling for GPT-2 models to fix the token length issue\n",
    "            self.hf_pipeline = pipeline(\n",
    "                \"text-generation\",\n",
    "                model=self.text_model_name,\n",
    "                max_new_tokens=50,         # Generate only 50 new tokens\n",
    "                do_sample=True,\n",
    "                temperature=0.7,\n",
    "                pad_token_id=50256,\n",
    "                return_full_text=False,    # Only return generated text, not input\n",
    "            )\n",
    "        elif \"falcon\" in self.text_model_name.lower():\n",
    "            # 🔧 ADD: Handle Falcon models\n",
    "            self.hf_pipeline = pipeline(\n",
    "                \"text-generation\",\n",
    "                model=self.text_model_name,\n",
    "                max_new_tokens=100,\n",
    "                do_sample=True,\n",
    "                temperature=0.3,\n",
    "                return_full_text=False,\n",
    "                trust_remote_code=True  # Falcon needs this\n",
    "            )\n",
    "        elif \"zephyr\" in self.text_model_name.lower():\n",
    "            # 🔧 ADD: Handle Zephyr models\n",
    "            self.hf_pipeline = pipeline(\n",
    "                \"text-generation\",\n",
    "                model=self.text_model_name,\n",
    "                max_new_tokens=100,\n",
    "                do_sample=True,\n",
    "                temperature=0.3,\n",
    "                return_full_text=False,\n",
    "            )\n",
    "        elif \"gemma\" in self.text_model_name.lower():\n",
    "            import torch._dynamo\n",
    "            torch._dynamo.config.suppress_errors = True\n",
    "            \n",
    "            # 🔧 ADD: Force eager execution for P100 compatibility\n",
    "            os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:512\"\n",
    "            \n",
    "            # 🔧 ADD: Handle Gemma models\n",
    "            self.hf_pipeline = pipeline(\n",
    "                \"text-generation\",\n",
    "                model=self.text_model_name,\n",
    "                max_new_tokens=100,\n",
    "                do_sample=True,\n",
    "                temperature=0.3,\n",
    "                return_full_text=False,\n",
    "            )\n",
    "        elif \"llama\" in self.text_model_name.lower():\n",
    "            # 🔧 ADD: Handle Llama models\n",
    "            self.hf_pipeline = pipeline(\n",
    "                \"text-generation\",\n",
    "                model=self.text_model_name,\n",
    "                max_new_tokens=100,\n",
    "                do_sample=True,\n",
    "                temperature=0.3,\n",
    "                return_full_text=False,\n",
    "            )\n",
    "        else:\n",
    "            self.text_model_name = \"google/flan-t5-large\"\n",
    "            print(f\"{embedding_model_name} text_model_name not recognized. Using {self.text_model_name}\")\n",
    "            self.hf_pipeline = pipeline(\n",
    "                \"text2text-generation\",\n",
    "                model=self.text_model_name,\n",
    "                max_length=768,\n",
    "                max_new_tokens=100,\n",
    "            )\n",
    "        \n",
    "        self.model = HuggingFacePipeline(pipeline=self.hf_pipeline)\n",
    "        \n",
    "        # Initialize database\n",
    "        self.db = Chroma(persist_directory=persist_directory, \n",
    "                        embedding_function=self.embedding_function)\n",
    "    \n",
    "        self.PROMPT_TEMPLATE = \"\"\"\n",
    "            Answer the question based only on the following context:\n",
    "\n",
    "            {context}\n",
    "\n",
    "            ---\n",
    "\n",
    "            Answer the question based on the above context: {question}\n",
    "            here are the options:\n",
    "            {options}\n",
    "\n",
    "            Respond only the Letter of the correct options like A, B, C and D. Do not inlcude the source.\n",
    "            \"\"\"\n",
    "        # prompt 2: \n",
    "        # \"\"\"\n",
    "        # You are answering questions about Alice in Wonderland based on the provided context.\n",
    "\n",
    "        # CONTEXT:\n",
    "        # {context}\n",
    "        \n",
    "        # QUESTION: {question}\n",
    "        \n",
    "        # OPTIONS:\n",
    "        # {options}\n",
    "        \n",
    "        # INSTRUCTIONS:\n",
    "        # - Read the context carefully\n",
    "        # - Answer based ONLY on the information provided in the context.\n",
    "        # - Respond with ONLY the letter (A, B, C, or D) of the correct answer\n",
    "        # - Do not include explanations or sources\n",
    "        # \"\"\"\n",
    "\n",
    "        # prompt 3: \n",
    "        # \"\"\"\n",
    "        # <s>[INST] You are answering questions about Alice in Wonderland. \n",
    "\n",
    "        # Context: {context_text}\n",
    "        # Question: {question}\n",
    "        # Options: {options_text}\n",
    "        \n",
    "        # INSTRUCTIONS:\n",
    "        # - Read the context carefully\n",
    "        # - Answer based ONLY on the information provided in the context.\n",
    "        # - Respond with ONLY the letter (A, B, C, or D) of the correct answer\n",
    "        # - Do not include explanations or sources\n",
    "        # [/INST]\"\"\"\n",
    "        \n",
    "        # print(f\"QueryEngine initialized:\")\n",
    "        # print(f\"  Embedding: {embedding_model_name} ({embedding_model_type})\")\n",
    "        # print(f\"  Text Generation: {text_model_name}\")\n",
    "        # print(f\"  Database: {persist_directory}\")\n",
    "        print(f\"Initialized QueryEngine: embedding model: {embedding_model_name} ({embedding_model_type}); chat model : {text_model_name}\")\n",
    "\n",
    "    # Rest of your QueryEngine methods remain the same...\n",
    "    \n",
    "    def load_quiz_data(self, quiz_file_path='test_questions.json'):\n",
    "        \"\"\"Load quiz data from JSON file.\"\"\"\n",
    "        try:\n",
    "            with open(quiz_file_path, 'r', encoding='utf-8') as file:\n",
    "                data = json.load(file)\n",
    "                # print(f\"Loaded {len(data)} questions from {quiz_file_path}\")\n",
    "                return data\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Error: {quiz_file_path} file not found!\")\n",
    "            return []\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error parsing JSON: {e}\")\n",
    "            return []\n",
    " \n",
    "    def semantic_search_database(self, query, k=5):\n",
    "        \"\"\"Search the database for relevant documents.\"\"\"\n",
    "        if self.db is None:\n",
    "            return []\n",
    "        \n",
    "        try:\n",
    "            results = self.db.similarity_search_with_relevance_scores(query, k=k)\n",
    "            return results\n",
    "        except Exception as e:\n",
    "            print(f\"Error searching database: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def filter_response(self, response):\n",
    "        edit_response = response.replace('-', '').strip()\n",
    "        return edit_response\n",
    "\n",
    "    def generate_response(self, question, options, context_text):\n",
    "        \"\"\"Generate a response using the LLM.\"\"\"\n",
    "        # Format the prompt\n",
    "        options_text = \"\\n\".join(options) if isinstance(options, list) else str(options)\n",
    "        prompt = self.PROMPT_TEMPLATE.format(\n",
    "            context=context_text, \n",
    "            question=question, \n",
    "            options=options_text\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            # Use the HuggingFace model to generate response\n",
    "            response_text = self.model.invoke(prompt)\n",
    "            response_text = self.filter_response(response_text)\n",
    "            return response_text\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating response: {e}\")\n",
    "            return \"Error generating response.\"\n",
    "    \n",
    "    def query_single_question(self, question, options=None, show_context=False):\n",
    "        \"\"\"Query a single question and return the response.\"\"\"\n",
    "        # Search the database\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\", UserWarning)\n",
    "        results = self.semantic_search_database(question, k=5)\n",
    "        \n",
    "        if not results:\n",
    "            return {\n",
    "                'question': question,\n",
    "                'response': 'No relevant context found.',\n",
    "                'context': '',\n",
    "                'sources': []\n",
    "            }\n",
    "        \n",
    "        # Prepare context from search results\n",
    "        context_text = \"\\n\\n---\\n\\n\".join([doc.page_content for doc, _score in results])\n",
    "        # sources = [doc.metadata.get(\"source\", \"Unknown\") for doc, _score in results]\n",
    "        sources = [(_score, doc.metadata.get(\"source\", \"Unknown\"), doc.page_content) for doc, _score in results]\n",
    "        all_scores = [_score for doc, _score in results]\n",
    "        avg = sum(all_scores) / len(all_scores) if all_scores else 0\n",
    "\n",
    "        \n",
    "        # Generate response\n",
    "        response_text = self.generate_response(question, options or [], context_text)\n",
    "        \n",
    "        result = {\n",
    "            'question': question,\n",
    "            'response': response_text.replace('-', '').strip(),\n",
    "            'sources': sources,\n",
    "            \"avg relevance sources\" : avg\n",
    "        }\n",
    "        \n",
    "        if show_context:\n",
    "            result['context'] = context_text\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def run_quiz(self, quiz_file_path='test_questions.json', show_details=False, limit=None):\n",
    "        \"\"\"Run the complete quiz and return results.\"\"\"\n",
    "        # Load quiz data\n",
    "        quiz_data = self.load_quiz_data(quiz_file_path)\n",
    "        \n",
    "        if not quiz_data:\n",
    "            print(f\"No quiz data loaded. quiz_file_path = {quiz_file_path} Exiting.\")\n",
    "            return []\n",
    "        \n",
    "        # Limit questions if specified\n",
    "        if limit:\n",
    "            quiz_data = quiz_data[:limit]\n",
    "            # print(f\"Running quiz with {limit} questions.\")\n",
    "        \n",
    "        results = []\n",
    "        correct_count = 0\n",
    "        \n",
    "        for i, question_data in enumerate(quiz_data, 1):\n",
    "            # print(f\"Question {i} of {len(quiz_data)}\")\n",
    "            \n",
    "            question_id = question_data.get(\"id\", i)\n",
    "            question = question_data[\"question\"]\n",
    "            options = question_data[\"options\"]\n",
    "            correct_answer = question_data[\"answer\"]\n",
    "            \n",
    "            # Query the database and generate response\n",
    "            result = self.query_single_question(question, options, show_context=False)\n",
    "            \n",
    "            # Add quiz-specific information\n",
    "            result.update({\n",
    "                'id': question_id,\n",
    "                'options': options,\n",
    "                'correct_answer': correct_answer,\n",
    "                'response' : result['response'],\n",
    "                'is_correct': result['response'].strip().upper() == correct_answer.upper()\n",
    "            })\n",
    "\n",
    "            if result[\"is_correct\"] == False and len(result[\"response\"]) != 1:\n",
    "                if result[\"correct_answer\"].upper().strip() == \"A\":\n",
    "                    alternate_correct_answer = result[\"options\"][0][4:].replace('-', '').strip()\n",
    "                elif result[\"correct_answer\"].upper().strip() == \"B\":\n",
    "                    alternate_correct_answer = result[\"options\"][1][4:].replace('-', '').strip()\n",
    "                elif result[\"correct_answer\"].upper().strip() == \"C\":\n",
    "                    alternate_correct_answer = result[\"options\"][2][4:].replace('-', '').strip()\n",
    "                elif result[\"correct_answer\"].upper().strip() == \"D\":\n",
    "                    alternate_correct_answer = result[\"options\"][3][4:].replace('-', '').strip()\n",
    "                else:\n",
    "                    alternate_correct_answer = \"\"\n",
    "\n",
    "                if alternate_correct_answer.upper() == result[\"response\"].upper():\n",
    "                    result[\"is_correct\"] = True\n",
    "                else:\n",
    "                    if result[\"response\"].upper().startswith(alternate_correct_answer.upper()):\n",
    "                        result[\"response\"] = alternate_correct_answer\n",
    "                        result[\"is_correct\"] = True\n",
    "                    else:\n",
    "                        result[\"is_correct\"] = False\n",
    "\n",
    "            if result['is_correct']:\n",
    "                correct_count += 1\n",
    "            \n",
    "            results.append(result)\n",
    "            \n",
    "        \n",
    "        # Summary\n",
    "        accuracy = (correct_count / len(quiz_data)) * 100 if quiz_data else 0\n",
    "        print(f\"\\nQuiz Summary:\")\n",
    "        print(f\"Correct Answers: {correct_count} / {len(quiz_data)}. Accuracy: {accuracy:.1f}%\")\n",
    "        self.save_results(self.embedding_model_name, self.text_model_name, results, filepath=persist_filepath)\n",
    "        return results\n",
    "    \n",
    "    def set_prompt_template(self, new_template):\n",
    "        \"\"\"Set a custom prompt template.\"\"\"\n",
    "        self.PROMPT_TEMPLATE = new_template\n",
    "\n",
    "    def save_results(self, embedding_model, text_model, result, filepath=persist_filepath):\n",
    "        \"\"\"Save quiz results to a JSON file.\"\"\"\n",
    "        try:\n",
    "            with open(filepath, 'r') as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            output = {\n",
    "                \"timestamp\" : datetime.datetime.now(),\n",
    "                \"results\" : result\n",
    "            }\n",
    "            \n",
    "            if embedding_model in data:\n",
    "                embedding_model_info = data[embedding_model]\n",
    "                if text_model in embedding_model_info:\n",
    "                    embedding_model_info[text_model].append(output)\n",
    "                else:\n",
    "                    embedding_model_info[text_model] = [output]\n",
    "            else:\n",
    "                data[embedding_model] = {\n",
    "                    text_model: [output]\n",
    "                }\n",
    "\n",
    "            with open(filepath, 'w', encoding='utf-8') as file:\n",
    "                json.dump(data, file, indent=4)\n",
    "                return\n",
    "\n",
    "            # with open(filepath, 'w', encoding='utf-8') as file:\n",
    "            #     json.dump(output, file, indent=4)\n",
    "            # print(f\"Results saved to {filepath}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving results: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-20T04:22:56.957618Z",
     "iopub.status.busy": "2025-08-20T04:22:56.957308Z",
     "iopub.status.idle": "2025-08-20T04:22:56.975007Z",
     "shell.execute_reply": "2025-08-20T04:22:56.974326Z",
     "shell.execute_reply.started": "2025-08-20T04:22:56.957586Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "EMBEDDING_MODEL_OPTIONS = [\n",
    "    \"sentence-transformers/all-MiniLM-L6-v2\", # success\n",
    "    \"sentence-transformers/all-mpnet-base-v2\", # success\n",
    "    \"BAAI/bge-m3\",\n",
    "    \"BAAI/bge-large-en\", # success\n",
    "    \"BAAI/bge-base-en-v1.5\",\n",
    "    \"BAAI/bge-large-en-v1.5\",\n",
    "    \"intfloat/e5-base-v2\", # success\n",
    "    \"sentence-transformers/static-retrieval-mrl-en-v1\", # success\n",
    "    \"sentence-transformers/all-MiniLM-L12-v2\", # success # best one so far\n",
    "    # \"gemini/embedding-001\",       # Older Gemini model # horrible\n",
    "    # \"gemini/text-embedding-005\",  # New Gemini model\n",
    "    \"nomic-ai/nomic-embed-text-v1.5\",\n",
    "    \"sentence-transformers/multi-qa-mpnet-base-dot-v1\",\n",
    "    \"sentence-transformers/multi-qa-mpnet-base-cos-v1\",\n",
    "    \"hkunlp/instructor-large\",\n",
    "    \"hkunlp/instructor-xl\"\n",
    "]\n",
    "\n",
    "TEXT_GENERATION_MODEL_OPTIONS = [\n",
    "    \"google/flan-t5-small\",\n",
    "    \"google/flan-t5-base\", # have been using this for default development testing\n",
    "    \"google/flan-t5-large\",\n",
    "    \"google/flan-t5-xl\",\n",
    "    \"tiiuae/Falcon3-7B-Base\",\n",
    "    \"tiiuae/Falcon3-1B-Instruct\",\n",
    "    \"tiiuae/Falcon3-3B-Instruct\",\n",
    "    \"tiiuae/Falcon3-7B-Instruct\",\n",
    "    \"tiiuae/Falcon3-10B-Instruct\",\n",
    "    \"tiiuae/Falcon-H1-0.5B-Instruct\",\n",
    "    \"HuggingFaceH4/zephyr-7b-beta\",\n",
    "    \"google/gemma-3-1b-it\",\n",
    "    \"google/gemma-2-2b\",\n",
    "    \"google/gemma-2-2b-it\",\n",
    "    \"meta-llama/Llama-3.1-8B-Instruct\", \n",
    "    \"meta-llama/Llama-3.2-3B-Instruct\", \n",
    "    \"meta-llama/Meta-Llama-3-8B-Instruct\", \n",
    "    \"meta-llama/Llama-3.2-1B\", \n",
    "]\n",
    "\n",
    "# Fixed model types to match all embedding models (all are HuggingFace)\n",
    "EMBEDDING_MODEL_TYPES = [\n",
    "    \"huggingface\",  # 0 - all-MiniLM-L6-v2\n",
    "    \"huggingface\",  # 1 - all-mpnet-base-v2\n",
    "    \"huggingface\",  # 2 - bge-m3\n",
    "    \"huggingface\",  # 3 - bge-large-en\n",
    "    \"huggingface\",  # 4 - bge-base-en-v1.5\n",
    "    \"huggingface\",  # 5 - bge-large-en-v1.5\n",
    "    \"huggingface\",  # 6 - e5-base-v2 (Fixed from \"gemini\")\n",
    "    \"huggingface\",  # 7 - static-retrieval-mrl-en-v1\n",
    "    \"huggingface\",  # 8 - all-MiniLM-L12-v2 (Your best!)\n",
    "    \"huggingface\",  # 9 - nomic-embed-text-v1.5\n",
    "    \"huggingface\",  # 10 - multi-qa-mpnet-base-dot-v1\n",
    "    \"huggingface\",  # 11 - multi-qa-mpnet-base-cos-v1\n",
    "    \"huggingface\",  # 12 - instructor-large\n",
    "    \"huggingface\",  # 13 - instructor-xl\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-20T04:22:56.976254Z",
     "iopub.status.busy": "2025-08-20T04:22:56.975959Z",
     "iopub.status.idle": "2025-08-20T04:22:56.999258Z",
     "shell.execute_reply": "2025-08-20T04:22:56.998475Z",
     "shell.execute_reply.started": "2025-08-20T04:22:56.976234Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def list_models():\n",
    "    \"\"\"List all available models.\"\"\"\n",
    "    print(\"Available Embedding Models (EMBEDDING_MODEL_OPTIONS):\")\n",
    "    for i, (model, model_type) in enumerate(zip(EMBEDDING_MODEL_OPTIONS, EMBEDDING_MODEL_TYPES)):\n",
    "        print(f\"  {i}: {model} ({model_type})\")\n",
    "    \n",
    "    print(\"\\nAvailable Text Generation Models (TEXT_GENERATION_MODEL_OPTIONS):\")\n",
    "    for i, model in enumerate(TEXT_GENERATION_MODEL_OPTIONS):\n",
    "        print(f\"  {i}: {model}\")\n",
    "        \n",
    "def clear_cuda_memory():\n",
    "    \"\"\"Clear CUDA memory and run garbage collection.\"\"\"\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "        # Force garbage collection\n",
    "        gc.collect()\n",
    "        \n",
    "        # Clear cache again after garbage collection\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    else:\n",
    "        gc.collect()\n",
    "        \n",
    "def print_results_summary():\n",
    "    if os.path.exists(model_response_directory):\n",
    "        for model_response_fp in os.listdir(model_response_directory):\n",
    "            avg_relevance_sources = []\n",
    "            count = 0\n",
    "            num_questions = 0\n",
    "            with open(os.path.join(model_response_directory, model_response_fp), \"r\") as f:\n",
    "                model_responses = json.load(f)\n",
    "                for response in model_responses:\n",
    "                    if response[\"is_correct\"] == True:\n",
    "                        count += 1\n",
    "                    num_questions += 1\n",
    "                    avg_relevance_sources.append(response[\"avg relevance sources\"])\n",
    "            print(f\"Model: {model_response_fp}, Correct: {count}/{num_questions}, Avg Relevance: {sum(avg_relevance_sources) / len(avg_relevance_sources) if avg_relevance_sources else 0}\")\n",
    "                        \n",
    "def main(mode=\"create\", embedding_model_index=0, text_generation_model_index=-1, save_result=1):\n",
    "    print(\"=\" * 80)\n",
    "    clear_cuda_memory()\n",
    "    embedding_model_index = embedding_model_index\n",
    "    \n",
    "    # Get selected models\n",
    "    embedding_model = EMBEDDING_MODEL_OPTIONS[embedding_model_index]\n",
    "    embedding_model_type = EMBEDDING_MODEL_TYPES[embedding_model_index]\n",
    "    \n",
    "    db_name = \"chroma\"\n",
    "    db_data_path = f\"{db_name}/{embedding_model.split('/')[-1].replace('/', '_').replace('-', '_')}\"\n",
    "    \n",
    "    if text_generation_model_index != -1:\n",
    "        text_model = TEXT_GENERATION_MODEL_OPTIONS[text_generation_model_index]\n",
    "        result_file_path = f\"quiz_results/{embedding_model.split('/')[-1].replace('/', '_').replace('-', '_')}--{text_model.split('/')[-1].replace('/', '_').replace('-', '_')}_quiz_results.json\"\n",
    "    \n",
    "    def create_mode():\n",
    "        print(f\"Using embedding model         : {embedding_model} ({embedding_model_type})\")\n",
    "        # os.makedirs(db_name, exist_ok=True)\n",
    "        # os.makedirs(db_data_path, exist_ok=True)\n",
    "        db_manager = DatabaseManager(embedding_model_name=embedding_model, \n",
    "                                   embedding_model_type=embedding_model_type)\n",
    "        db_manager.generate_data_store(data_path=raw_knowledge_directory, \n",
    "                                                persist_directory=db_data_path)\n",
    "\n",
    "    def quiz_mode():\n",
    "        print(\"Running Alice in Wonderland quiz...\")\n",
    "        print(f\"Using embedding model         : {embedding_model} ({embedding_model_type})\")\n",
    "        print(f\"Using text generation model   : {text_model}\")\n",
    "        os.makedirs(\"quiz_results\", exist_ok=True)\n",
    "        query_engine = QueryEngine(persist_directory=db_data_path,\n",
    "                                 embedding_model_name=embedding_model,\n",
    "                                 embedding_model_type=embedding_model_type,\n",
    "                                 text_model_name=text_model)\n",
    "        \n",
    "        # Run the quiz\n",
    "        results = query_engine.run_quiz(test_questions_directory)\n",
    "        \n",
    "        # Rest of quiz_mode code remains the same...\n",
    "        if results:\n",
    "            if save_result==1:\n",
    "                with open(result_file_path, \"w\") as f:\n",
    "                    json.dump(results, f, indent=4)\n",
    "\n",
    "    if mode==\"create\":\n",
    "        create_mode()\n",
    "    elif mode==\"quiz\":\n",
    "        quiz_mode()\n",
    "    clear_cuda_memory()\n",
    "\n",
    "def run_mains(test_embedding_models=[], test_text_generation_models=[]):\n",
    "    counter = 0\n",
    "    for embedding_model_index in test_embedding_models:\n",
    "        try:\n",
    "            main(mode=\"create\", embedding_model_index=embedding_model_index)\n",
    "            print(f\"successfully created db with {EMBEDDING_MODEL_OPTIONS[embedding_model_index]}\")\n",
    "        except Exception as e:\n",
    "            print(f\"failed to create db with {EMBEDDING_MODEL_OPTIONS[embedding_model_index]}\")\n",
    "            print(e)\n",
    "            if os.path.exists(\"chroma\"):\n",
    "                shutil.rmtree(\"chroma\")\n",
    "            continue\n",
    "        \n",
    "        for text_generation_model_index in test_text_generation_models:\n",
    "            counter += 1\n",
    "            try: \n",
    "                main(mode=\"quiz\", embedding_model_index=embedding_model_index, text_generation_model_index=text_generation_model_index)\n",
    "                print(f\"successfully ran quiz with {EMBEDDING_MODEL_OPTIONS[embedding_model_index]} and {TEXT_GENERATION_MODEL_OPTIONS[text_generation_model_index]}. {counter} / {len(test_embedding_models) * len(test_text_generation_models)} models combination tested. testing {embedding_model_index + 1} / {len(test_embedding_models)} embedding models. tested {text_generation_model_index + 1} / {len(test_text_generation_models)} chat models.\")\n",
    "                print()\n",
    "                # print_results_summary()\n",
    "            except Exception as e:\n",
    "                print(f\"failed to run quiz with {EMBEDDING_MODEL_OPTIONS[embedding_model_index]} and {TEXT_GENERATION_MODEL_OPTIONS[text_generation_model_index]}. {counter} / {len(test_embedding_models) * len(test_text_generation_models)} models combination tested. testing {embedding_model_index + 1} / {len(test_embedding_models)} embedding models. tested {text_generation_model_index + 1} / {len(test_text_generation_models)} chat models.\")\n",
    "                print(e)\n",
    "                continue\n",
    "        if os.path.exists(\"chroma\"):\n",
    "            shutil.rmtree(\"chroma\")\n",
    "\n",
    "def run_mains_smart():\n",
    "    try:\n",
    "        incomplete_model_combinations = {}\n",
    "        with open(persist_filepath, 'r') as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        # getting all the combinations that are not in the file\n",
    "        for i, embedding_model in enumerate(EMBEDDING_MODEL_OPTIONS):\n",
    "            if embedding_model not in data:\n",
    "                incomplete_model_combinations[i] = list(range(len(TEXT_GENERATION_MODEL_OPTIONS)))\n",
    "            else:\n",
    "                for j, text_model in enumerate(TEXT_GENERATION_MODEL_OPTIONS):\n",
    "                    if text_model not in data[embedding_model]:\n",
    "                        if i in incomplete_model_combinations:\n",
    "                            incomplete_model_combinations[i].append(j)\n",
    "                        else:\n",
    "                            incomplete_model_combinations[i] = [j]\n",
    "\n",
    "        for embedding_model_index, text_generation_model_indices in incomplete_model_combinations.items():\n",
    "            run_mains(test_embedding_models=[embedding_model_index], test_text_generation_models=text_generation_model_indices)\n",
    "      \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "    \n",
    "            \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-20T04:22:57.000298Z",
     "iopub.status.busy": "2025-08-20T04:22:57.000061Z",
     "iopub.status.idle": "2025-08-20T04:22:57.020725Z",
     "shell.execute_reply": "2025-08-20T04:22:57.020002Z",
     "shell.execute_reply.started": "2025-08-20T04:22:57.000280Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Embedding Models (EMBEDDING_MODEL_OPTIONS):\n",
      "  0: sentence-transformers/all-MiniLM-L6-v2 (huggingface)\n",
      "  1: sentence-transformers/all-mpnet-base-v2 (huggingface)\n",
      "  2: BAAI/bge-m3 (huggingface)\n",
      "  3: BAAI/bge-large-en (huggingface)\n",
      "  4: BAAI/bge-base-en-v1.5 (huggingface)\n",
      "  5: BAAI/bge-large-en-v1.5 (huggingface)\n",
      "  6: intfloat/e5-base-v2 (huggingface)\n",
      "  7: sentence-transformers/static-retrieval-mrl-en-v1 (huggingface)\n",
      "  8: sentence-transformers/all-MiniLM-L12-v2 (huggingface)\n",
      "  9: nomic-ai/nomic-embed-text-v1.5 (huggingface)\n",
      "  10: sentence-transformers/multi-qa-mpnet-base-dot-v1 (huggingface)\n",
      "  11: sentence-transformers/multi-qa-mpnet-base-cos-v1 (huggingface)\n",
      "  12: hkunlp/instructor-large (huggingface)\n",
      "  13: hkunlp/instructor-xl (huggingface)\n",
      "\n",
      "Available Text Generation Models (TEXT_GENERATION_MODEL_OPTIONS):\n",
      "  0: google/flan-t5-small\n",
      "  1: google/flan-t5-base\n",
      "  2: google/flan-t5-large\n",
      "  3: google/flan-t5-xl\n",
      "  4: tiiuae/Falcon3-7B-Base\n",
      "  5: tiiuae/Falcon3-1B-Instruct\n",
      "  6: tiiuae/Falcon3-3B-Instruct\n",
      "  7: tiiuae/Falcon3-7B-Instruct\n",
      "  8: tiiuae/Falcon3-10B-Instruct\n",
      "  9: tiiuae/Falcon-H1-0.5B-Instruct\n",
      "  10: HuggingFaceH4/zephyr-7b-beta\n",
      "  11: google/gemma-3-1b-it\n",
      "  12: google/gemma-2-2b\n",
      "  13: google/gemma-2-2b-it\n",
      "  14: meta-llama/Llama-3.1-8B-Instruct\n",
      "  15: meta-llama/Llama-3.2-3B-Instruct\n",
      "  16: meta-llama/Meta-Llama-3-8B-Instruct\n",
      "  17: meta-llama/Llama-3.2-1B\n"
     ]
    }
   ],
   "source": [
    "list_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-08-20T04:56:04.690Z"
    },
    "scrolled": true,
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Using embedding model         : sentence-transformers/all-MiniLM-L6-v2 (huggingface)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8105b78555d4200882c34769e7e3c97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Admin\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b386b31bf1be4782a48f5e9ea182f044",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eacf5c5df71c41b7a3b4c33d397e44d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "508281429b9841c6a3555eca560361e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f9beae5fe664b56a2583ec74506f717",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eef88812606343bea2e2d270492f019f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a71657400934ee28f36425eea586663",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3369266c22a94f7f8891b50ae2f19db1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abc2020bf4684c86b949ca8efda1a6f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c5d4ac8313f4d669644f5981d2e3387",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15ee0c3668f64a028290783229968888",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized DatabaseManager: Embedding embedding model: sentence-transformers/all-MiniLM-L6-v2 (huggingface)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libmagic is unavailable but assists in filetype detection. Please consider installing libmagic for better results.\n",
      "Error loading file books\\alice_in_wonderland.md\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading documents: partition_md() is not available because one or more dependencies are not installed. Use: pip install \"unstructured[md]\" (including quotes) to install the required dependencies\n",
      "successfully created db with sentence-transformers/all-MiniLM-L6-v2\n",
      "================================================================================\n",
      "Running Alice in Wonderland quiz...\n",
      "Using embedding model         : sentence-transformers/all-MiniLM-L6-v2 (huggingface)\n",
      "Using text generation model   : google/flan-t5-small\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d40ad51579314962b441c1a29bb6d43f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Admin\\.cache\\huggingface\\hub\\models--google--flan-t5-small. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "717be267ea6847118f724761ad247cc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/308M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b51cf9eb89464d24b4d7d15c1fdac71a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a13e11f4bee4499a56c61ab6f49a963",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfa2d6e4320547e7a3eeb81225e12ee0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a921328153434b979ad6e93ab05418fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "107a8d916b0243ddae2489046c21d87b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized QueryEngine: embedding model: sentence-transformers/all-MiniLM-L6-v2 (huggingface); chat model : google/flan-t5-small\n",
      "\n",
      "Quiz Summary:\n",
      "Correct Answers: 0 / 90. Accuracy: 0.0%\n",
      "Error saving results: Object of type datetime is not JSON serializable\n",
      "successfully ran quiz with sentence-transformers/all-MiniLM-L6-v2 and google/flan-t5-small. 1 / 18 models combination tested. testing 1 / 1 embedding models. tested 1 / 18 chat models.\n",
      "\n",
      "================================================================================\n",
      "Running Alice in Wonderland quiz...\n",
      "Using embedding model         : sentence-transformers/all-MiniLM-L6-v2 (huggingface)\n",
      "Using text generation model   : google/flan-t5-base\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26bc537ff5834ea293cbe511f1766c61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Admin\\.cache\\huggingface\\hub\\models--google--flan-t5-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af1dca879b8846f5b304d1623176a440",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4920842525540d9ae7911859b9f0bdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "741ac0920e72474489ed27eb25fc4aaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da9c59d9ed664741a9dad2b8b92f3836",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f3ad3b2da5e4749a174afbf94cc43a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd23cfb5372949119ae1a4ec870ce74c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized QueryEngine: embedding model: sentence-transformers/all-MiniLM-L6-v2 (huggingface); chat model : google/flan-t5-base\n",
      "\n",
      "Quiz Summary:\n",
      "Correct Answers: 0 / 90. Accuracy: 0.0%\n",
      "Error saving results: Expecting value: line 5 column 30 (char 127)\n",
      "successfully ran quiz with sentence-transformers/all-MiniLM-L6-v2 and google/flan-t5-base. 2 / 18 models combination tested. testing 1 / 1 embedding models. tested 2 / 18 chat models.\n",
      "\n",
      "================================================================================\n",
      "Running Alice in Wonderland quiz...\n",
      "Using embedding model         : sentence-transformers/all-MiniLM-L6-v2 (huggingface)\n",
      "Using text generation model   : google/flan-t5-large\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "447b2ce773a44e22843f44be5a738cc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/662 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Admin\\.cache\\huggingface\\hub\\models--google--flan-t5-large. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0feca449a8a34ec39f1942522733e226",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.13G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea410899c1e347b5b89edfb8b0262a0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73f3bde0e96f476e81aecbe74d7729d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3a79013ab7b45e4a003e72d38d1657d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb47e7a95aec4bf092f9e5ae3854e1e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e073155ff01479f8b22ca8c8c11c64e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized QueryEngine: embedding model: sentence-transformers/all-MiniLM-L6-v2 (huggingface); chat model : google/flan-t5-large\n",
      "\n",
      "Quiz Summary:\n",
      "Correct Answers: 0 / 90. Accuracy: 0.0%\n",
      "Error saving results: Expecting value: line 5 column 30 (char 127)\n",
      "successfully ran quiz with sentence-transformers/all-MiniLM-L6-v2 and google/flan-t5-large. 3 / 18 models combination tested. testing 1 / 1 embedding models. tested 3 / 18 chat models.\n",
      "\n",
      "================================================================================\n",
      "Running Alice in Wonderland quiz...\n",
      "Using embedding model         : sentence-transformers/all-MiniLM-L6-v2 (huggingface)\n",
      "Using text generation model   : google/flan-t5-xl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2334426dd08f414989a39a6e5de3ba0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Admin\\.cache\\huggingface\\hub\\models--google--flan-t5-xl. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ab1e03749c147fd9c23eb51976443a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8080c777e42846619aed9a0e60a48798",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "102922d9cfc5408c91c7c92ea697399f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/1.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc37b5f5c324425a926a0234d8c1bf11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/9.45G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9750be430a04d2c84acbd2c7ad26009",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbb645b3746e46af8bbca4135905e71f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "573f135252cd4aa49c4cf868d55f4017",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1985eb56f6554dec975745df839b4337",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db5c777c0fa847598fd770df65a12a29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7dce8a026114ccd8879a8bd0e67ad96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized QueryEngine: embedding model: sentence-transformers/all-MiniLM-L6-v2 (huggingface); chat model : google/flan-t5-xl\n",
      "\n",
      "Quiz Summary:\n",
      "Correct Answers: 0 / 90. Accuracy: 0.0%\n",
      "Error saving results: Expecting value: line 5 column 30 (char 127)\n",
      "successfully ran quiz with sentence-transformers/all-MiniLM-L6-v2 and google/flan-t5-xl. 4 / 18 models combination tested. testing 1 / 1 embedding models. tested 4 / 18 chat models.\n",
      "\n",
      "================================================================================\n",
      "Running Alice in Wonderland quiz...\n",
      "Using embedding model         : sentence-transformers/all-MiniLM-L6-v2 (huggingface)\n",
      "Using text generation model   : tiiuae/Falcon3-7B-Base\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e88d589711841f09b29afcde7e27554",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/659 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Admin\\.cache\\huggingface\\hub\\models--tiiuae--Falcon3-7B-Base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80d71c4634d74e6bb56f945d9fb1bb8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2478f1af08514bd5aa42f1a9a842d2c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "103a06b662bc4e41844b9c62c5b1810f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.22G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba007c92bc5849eca332fb2a05438335",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb63c1d5244d4fbf994147cbebe6028f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "010bfdc57a7f4df5b767133f438ff4f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/805M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run_mains_smart()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_results_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 8066456,
     "sourceId": 12760370,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8066490,
     "sourceId": 12760417,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8101637,
     "sourceId": 12812875,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
