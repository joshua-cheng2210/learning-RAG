{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-19T01:43:39.939108Z",
     "iopub.status.busy": "2025-08-19T01:43:39.938851Z",
     "iopub.status.idle": "2025-08-19T01:45:41.437618Z",
     "shell.execute_reply": "2025-08-19T01:45:41.436869Z",
     "shell.execute_reply.started": "2025-08-19T01:43:39.939088Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "bigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\n",
      "datasets 3.6.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.5.1 which is incompatible.\n",
      "cesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
      "pandas-gbq 0.29.1 requires google-api-core<3.0.0,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\n",
      "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
      "plotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\n",
      "dataproc-spark-connect 0.7.5 requires google-api-core>=2.19, but you have google-api-core 1.34.1 which is incompatible.\n",
      "bigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\n",
      "bigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\n",
      "mlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mRequirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement gc (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for gc\u001b[0m\u001b[31m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.2/209.2 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.8/19.8 MB\u001b[0m \u001b[31m65.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m56.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m561.5/561.5 kB\u001b[0m \u001b[31m33.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m443.5/443.5 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m74.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.1/103.1 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m71.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.5/52.5 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.0/64.0 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.7/149.7 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.4/71.4 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m91.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m453.1/453.1 kB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "datasets 3.6.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.5.1 which is incompatible.\n",
      "cesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
      "google-generativeai 0.8.5 requires google-ai-generativelanguage==0.6.15, but you have google-ai-generativelanguage 0.6.18 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m56.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m59.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m44.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m62.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.6/167.6 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m43.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.8/207.8 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "ypy-websocket 0.8.4 requires aiofiles<23,>=22.1.0, but you have aiofiles 24.1.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Cell 1: Core LangChain\n",
    "!pip install -q langchain\n",
    "!pip install torch gc\n",
    "\n",
    "# Cell 2: LangChain integrations\n",
    "!pip install -q langchain-community langchain-huggingface langchain-chroma langchain_google_genai langchain_experimental\n",
    "\n",
    "# Cell 3: ML libraries\n",
    "!pip install -q sentence-transformers transformers chromadb\n",
    "\n",
    "# Cell 4: Utilities\n",
    "!pip install -q python-dotenv torch unstructured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-08-19T01:45:41.439427Z",
     "iopub.status.busy": "2025-08-19T01:45:41.439094Z",
     "iopub.status.idle": "2025-08-19T01:46:08.341738Z",
     "shell.execute_reply": "2025-08-19T01:46:08.341141Z",
     "shell.execute_reply.started": "2025-08-19T01:45:41.439404Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-19 01:45:53.538110: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1755567953.738053      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1755567953.794459      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings  \n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import json\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from transformers import pipeline\n",
    "import os\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import gc\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-19T01:46:08.343201Z",
     "iopub.status.busy": "2025-08-19T01:46:08.342613Z",
     "iopub.status.idle": "2025-08-19T01:46:08.351685Z",
     "shell.execute_reply": "2025-08-19T01:46:08.350999Z",
     "shell.execute_reply.started": "2025-08-19T01:46:08.343156Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class DatabaseManager:\n",
    "    def __init__(self, embedding_model_name=\"sentence-transformers/all-MiniLM-L12-v2\", \n",
    "                 embedding_model_type=\"huggingface\"):\n",
    "        \"\"\"\n",
    "        Initialize DatabaseManager with specified embedding model.\n",
    "        \n",
    "        Args:\n",
    "            embedding_model_name: Name of the embedding model\n",
    "            embedding_model_type: Type of model (\"huggingface\" or \"gemini\")\n",
    "        \"\"\"\n",
    "        self.embedding_model_name = embedding_model_name\n",
    "        self.embedding_model_type = embedding_model_type\n",
    "        \n",
    "        # Initialize embedding function based on type\n",
    "        if embedding_model_type == \"gemini\":\n",
    "            api_key = os.getenv(\"GEMINI_API_KEY\")  # Changed from GEMINI_API_KEY\n",
    "            if not api_key:\n",
    "                raise ValueError(\"GEMINI_API_KEY environment variable is required for Gemini models\")\n",
    "            \n",
    "            # Extract model name (remove 'gemini/' prefix)\n",
    "            model_name = self.embedding_model_name.replace(\"gemini/\", \"\")\n",
    "            self.embedding_function = GoogleGenerativeAIEmbeddings(\n",
    "                model=model_name,\n",
    "                google_api_key=api_key\n",
    "            )\n",
    "        elif embedding_model_type == \"huggingface\":\n",
    "            self.embedding_function = HuggingFaceEmbeddings(model_name=embedding_model_name)\n",
    "        else:  # huggingface\n",
    "            embedding_model_type == \"huggingface\"\n",
    "            self.embedding_function = HuggingFaceEmbeddings(model_name=embedding_model_name)\n",
    "        \n",
    "    # Rest of your DatabaseManager methods remain the same...\n",
    "    def load_documents(self, data_path):\n",
    "        \"\"\"Load documents from the specified directory.\"\"\"\n",
    "        try:\n",
    "            loader = DirectoryLoader(data_path, glob=\"*.md\")\n",
    "            documents = loader.load()\n",
    "            # print(f\"Loaded {len(documents)} documents from {data_path}\")\n",
    "            return documents\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading documents: {e}\")\n",
    "            return []\n",
    "\n",
    "    def split_text(self, documents):\n",
    "        \"\"\"Split documents into chunks.\"\"\"\n",
    "        try:\n",
    "            text_splitter = RecursiveCharacterTextSplitter(\n",
    "                chunk_size=500,\n",
    "                chunk_overlap=150,\n",
    "                length_function=len,\n",
    "                add_start_index=True,\n",
    "            )\n",
    "            chunks = text_splitter.split_documents(documents)\n",
    "            # print(f\"Split into {len(chunks)} chunks\")\n",
    "            return chunks\n",
    "        except Exception as e:\n",
    "            print(f\"Error splitting text: {e}\")\n",
    "            return []\n",
    "\n",
    "    def save_to_chroma(self, chunks, persist_directory):\n",
    "        \"\"\"Save document chunks to Chroma database.\"\"\"\n",
    "        try:\n",
    "            # Create directory if it doesn't exist\n",
    "            os.makedirs(persist_directory, exist_ok=True)\n",
    "            \n",
    "            db = Chroma.from_documents(\n",
    "                chunks, \n",
    "                self.embedding_function, \n",
    "                persist_directory=persist_directory\n",
    "            )\n",
    "            # print(f\"Saved {len(chunks)} chunks to Chroma database at {persist_directory}\")\n",
    "            return db\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving to Chroma: {e}\")\n",
    "            return None\n",
    "\n",
    "    def generate_data_store(self, data_path=\"books\", persist_directory=\"chroma\"):\n",
    "        \"\"\"Complete pipeline: load documents, split text, and save to database.\"\"\"\n",
    "        # print(f\"Starting data store generation...\")\n",
    "        # print(f\"Data path: {data_path}\")\n",
    "        # print(f\"Persist directory: {persist_directory}\")\n",
    "        print(f\"Embedding embedding model: {embedding_model_name} ({embedding_model_type})\")\n",
    "        \n",
    "        # Load documents\n",
    "        documents = self.load_documents(data_path)\n",
    "        if not documents:\n",
    "            return False\n",
    "        \n",
    "        # Split into chunks\n",
    "        chunks = self.split_text(documents)\n",
    "        if not chunks:\n",
    "            return False\n",
    "        \n",
    "        # Save to database\n",
    "        db = self.save_to_chroma(chunks, persist_directory)\n",
    "        return db is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-19T01:46:08.353882Z",
     "iopub.status.busy": "2025-08-19T01:46:08.353598Z",
     "iopub.status.idle": "2025-08-19T01:46:08.812445Z",
     "shell.execute_reply": "2025-08-19T01:46:08.811757Z",
     "shell.execute_reply.started": "2025-08-19T01:46:08.353855Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class QueryEngine:\n",
    "    def __init__(self, persist_directory=\"chroma\", \n",
    "                 embedding_model_name=\"sentence-transformers/all-MiniLM-L12-v2\",\n",
    "                 embedding_model_type=\"huggingface\",\n",
    "                 text_model_name=\"google/flan-t5-base\"):\n",
    "        \"\"\"\n",
    "        Initialize QueryEngine with specified models.\n",
    "        \n",
    "        Args:\n",
    "            persist_directory: Path to the Chroma database\n",
    "            embedding_model_name: Name of the embedding model\n",
    "            embedding_model_type: Type of embedding model (\"huggingface\" or \"gemini\")\n",
    "            text_model_name: Name of the text generation model\n",
    "        \"\"\"\n",
    "        self.persist_directory = persist_directory\n",
    "        self.embedding_model_name = embedding_model_name\n",
    "        self.embedding_model_type = embedding_model_type\n",
    "        self.text_model_name = text_model_name\n",
    "        \n",
    "        # Initialize embedding function based on type\n",
    "        if embedding_model_type == \"gemini\":\n",
    "            api_key = os.getenv(\"GEMINI_API_KEY\")  # Changed from GEMINI_API_KEY\n",
    "            if not api_key:\n",
    "                raise ValueError(\"GEMINI_API_KEY environment variable is required for Gemini models\")\n",
    "            \n",
    "            # Extract model name (remove 'gemini/' prefix)\n",
    "            model_name = self.embedding_model_name.replace(\"gemini/\", \"\")\n",
    "            self.embedding_function = GoogleGenerativeAIEmbeddings(\n",
    "                model=model_name,\n",
    "                google_api_key=api_key\n",
    "            )\n",
    "        elif embedding_model_type == \"huggingface\":\n",
    "            self.embedding_function = HuggingFaceEmbeddings(model_name=embedding_model_name)\n",
    "        else:  # huggingface\n",
    "            embedding_model_type == \"huggingface\"\n",
    "            self.embedding_function = HuggingFaceEmbeddings(model_name=embedding_model_name)\n",
    "        \n",
    "        # Initialize text generation model\n",
    "        if self.text_model_name.startswith(\"google/flan\"):\n",
    "            self.hf_pipeline = pipeline(\n",
    "                \"text2text-generation\",\n",
    "                model=self.text_model_name,\n",
    "                max_length=768,\n",
    "                max_new_tokens=100,\n",
    "                truncation=True,  # 🔧 FIX: This prevents 704 > 512 error\n",
    "            )\n",
    "        elif self.text_model_name.startswith(\"mistralai/\"):\n",
    "            self.hf_pipeline = pipeline(\n",
    "                \"text-generation\",  # Mistral uses text-generation\n",
    "                model=self.text_model_name,\n",
    "                max_new_tokens=100,     # Limit output length\n",
    "                do_sample=True,\n",
    "                temperature=0.3,        # Lower temp for more focused answers\n",
    "                pad_token_id=2,         # Mistral's pad token\n",
    "                truncation=True,\n",
    "                return_full_text=False, # Only return generated text\n",
    "            )\n",
    "        elif self.text_model_name.startswith(\"gpt\") or self.text_model_name.startswith(\"distilgpt\"):\n",
    "            # Special handling for GPT-2 models to fix the token length issue\n",
    "            self.hf_pipeline = pipeline(\n",
    "                \"text-generation\",\n",
    "                model=self.text_model_name,\n",
    "                max_new_tokens=50,         # Generate only 50 new tokens\n",
    "                do_sample=True,\n",
    "                temperature=0.7,\n",
    "                pad_token_id=50256,\n",
    "                truncation=True,           # Truncate long inputs\n",
    "                return_full_text=False,    # Only return generated text, not input\n",
    "            )\n",
    "        elif \"falcon\" in self.text_model_name.lower():\n",
    "            # 🔧 ADD: Handle Falcon models\n",
    "            self.hf_pipeline = pipeline(\n",
    "                \"text-generation\",\n",
    "                model=self.text_model_name,\n",
    "                max_new_tokens=100,\n",
    "                do_sample=True,\n",
    "                temperature=0.3,\n",
    "                truncation=True,\n",
    "                return_full_text=False,\n",
    "                trust_remote_code=True  # Falcon needs this\n",
    "            )\n",
    "        elif \"zephyr\" in self.text_model_name.lower():\n",
    "            # 🔧 ADD: Handle Zephyr models\n",
    "            self.hf_pipeline = pipeline(\n",
    "                \"text-generation\",\n",
    "                model=self.text_model_name,\n",
    "                max_new_tokens=100,\n",
    "                do_sample=True,\n",
    "                temperature=0.3,\n",
    "                truncation=True,\n",
    "                return_full_text=False,\n",
    "            )\n",
    "        elif \"gemma\" in self.text_model_name.lower():\n",
    "            # 🔧 ADD: Handle Gemma models\n",
    "            self.hf_pipeline = pipeline(\n",
    "                \"text-generation\",\n",
    "                model=self.text_model_name,\n",
    "                max_new_tokens=100,\n",
    "                do_sample=True,\n",
    "                temperature=0.3,\n",
    "                truncation=True,\n",
    "                return_full_text=False,\n",
    "            )\n",
    "        elif \"llama\" in self.text_model_name.lower():\n",
    "            # 🔧 ADD: Handle Llama models\n",
    "            self.hf_pipeline = pipeline(\n",
    "                \"text-generation\",\n",
    "                model=self.text_model_name,\n",
    "                max_new_tokens=100,\n",
    "                do_sample=True,\n",
    "                temperature=0.3,\n",
    "                truncation=True,\n",
    "                return_full_text=False,\n",
    "            )\n",
    "        else:\n",
    "            self.text_model_name = \"google/flan-t5-large\"\n",
    "            self.hf_pipeline = pipeline(\n",
    "                \"text2text-generation\",\n",
    "                model=self.text_model_name,\n",
    "                max_length=768,\n",
    "                max_new_tokens=100,\n",
    "                truncation=True,  # 🔧 FIX: Add truncation to fallback too\n",
    "            )\n",
    "        \n",
    "        self.model = HuggingFacePipeline(pipeline=self.hf_pipeline)\n",
    "        \n",
    "        # Initialize database\n",
    "        self.db = Chroma(persist_directory=persist_directory, \n",
    "                        embedding_function=self.embedding_function)\n",
    "    \n",
    "        self.PROMPT_TEMPLATE = \"\"\"\n",
    "            Answer the question based only on the following context:\n",
    "\n",
    "            {context}\n",
    "\n",
    "            ---\n",
    "\n",
    "            Answer the question based on the above context: {question}\n",
    "            here are the options:\n",
    "            {options}\n",
    "\n",
    "            Respond only the Letter of the correct options like A, B, C and D. Do not inlcude the source.\n",
    "            \"\"\"\n",
    "        # prompt 2: \n",
    "        # \"\"\"\n",
    "        # You are answering questions about Alice in Wonderland based on the provided context.\n",
    "\n",
    "        # CONTEXT:\n",
    "        # {context}\n",
    "        \n",
    "        # QUESTION: {question}\n",
    "        \n",
    "        # OPTIONS:\n",
    "        # {options}\n",
    "        \n",
    "        # INSTRUCTIONS:\n",
    "        # - Read the context carefully\n",
    "        # - Answer based ONLY on the information provided in the context.\n",
    "        # - Respond with ONLY the letter (A, B, C, or D) of the correct answer\n",
    "        # - Do not include explanations or sources\n",
    "        # \"\"\"\n",
    "\n",
    "        # prompt 3: \n",
    "        # \"\"\"\n",
    "        # <s>[INST] You are answering questions about Alice in Wonderland. \n",
    "\n",
    "        # Context: {context_text}\n",
    "        # Question: {question}\n",
    "        # Options: {options_text}\n",
    "        \n",
    "        # INSTRUCTIONS:\n",
    "        # - Read the context carefully\n",
    "        # - Answer based ONLY on the information provided in the context.\n",
    "        # - Respond with ONLY the letter (A, B, C, or D) of the correct answer\n",
    "        # - Do not include explanations or sources\n",
    "        # [/INST]\"\"\"\n",
    "        \n",
    "        # print(f\"QueryEngine initialized:\")\n",
    "        # print(f\"  Embedding: {embedding_model_name} ({embedding_model_type})\")\n",
    "        # print(f\"  Text Generation: {text_model_name}\")\n",
    "        # print(f\"  Database: {persist_directory}\")\n",
    "        print(f\"Initialized embedding model: {embedding_model_name} ({embedding_model_type}) with chat model : {text_model_name}\")\n",
    "\n",
    "    # Rest of your QueryEngine methods remain the same...\n",
    "    \n",
    "    def load_quiz_data(self, quiz_file_path='test_questions.json'):\n",
    "        \"\"\"Load quiz data from JSON file.\"\"\"\n",
    "        try:\n",
    "            with open(quiz_file_path, 'r', encoding='utf-8') as file:\n",
    "                data = json.load(file)\n",
    "                # print(f\"Loaded {len(data)} questions from {quiz_file_path}\")\n",
    "                return data\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Error: {quiz_file_path} file not found!\")\n",
    "            return []\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error parsing JSON: {e}\")\n",
    "            return []\n",
    " \n",
    "    def semantic_search_database(self, query, k=5):\n",
    "        \"\"\"Search the database for relevant documents.\"\"\"\n",
    "        if self.db is None:\n",
    "            return []\n",
    "        \n",
    "        try:\n",
    "            results = self.db.similarity_search_with_relevance_scores(query, k=k)\n",
    "            return results\n",
    "        except Exception as e:\n",
    "            print(f\"Error searching database: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def filter_response(self, response):\n",
    "        edit_response = response.replace('-', '').strip()\n",
    "        return edit_response\n",
    "\n",
    "    def generate_response(self, question, options, context_text):\n",
    "        \"\"\"Generate a response using the LLM.\"\"\"\n",
    "        # Format the prompt\n",
    "        options_text = \"\\n\".join(options) if isinstance(options, list) else str(options)\n",
    "        prompt = self.PROMPT_TEMPLATE.format(\n",
    "            context=context_text, \n",
    "            question=question, \n",
    "            options=options_text\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            # Use the HuggingFace model to generate response\n",
    "            response_text = self.model.invoke(prompt)\n",
    "            response_text = self.filter_response(response_text)\n",
    "            return response_text\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating response: {e}\")\n",
    "            return \"Error generating response.\"\n",
    "    \n",
    "    def query_single_question(self, question, options=None, show_context=False):\n",
    "        \"\"\"Query a single question and return the response.\"\"\"\n",
    "        # Search the database\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\", UserWarning)\n",
    "        results = self.semantic_search_database(question, k=5)\n",
    "        \n",
    "        if not results:\n",
    "            return {\n",
    "                'question': question,\n",
    "                'response': 'No relevant context found.',\n",
    "                'context': '',\n",
    "                'sources': []\n",
    "            }\n",
    "        \n",
    "        # Prepare context from search results\n",
    "        context_text = \"\\n\\n---\\n\\n\".join([doc.page_content for doc, _score in results])\n",
    "        # sources = [doc.metadata.get(\"source\", \"Unknown\") for doc, _score in results]\n",
    "        sources = [(_score, doc.metadata.get(\"source\", \"Unknown\"), doc.page_content) for doc, _score in results]\n",
    "        all_scores = [_score for doc, _score in results]\n",
    "        avg = sum(all_scores) / len(all_scores) if all_scores else 0\n",
    "\n",
    "        \n",
    "        # Generate response\n",
    "        response_text = self.generate_response(question, options or [], context_text)\n",
    "        \n",
    "        result = {\n",
    "            'question': question,\n",
    "            'response': response_text.replace('-', '').strip(),\n",
    "            'sources': sources,\n",
    "            \"avg relevance sources\" : avg\n",
    "        }\n",
    "        \n",
    "        if show_context:\n",
    "            result['context'] = context_text\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def run_quiz(self, quiz_file_path='test_questions.json', show_details=False, limit=None):\n",
    "        \"\"\"Run the complete quiz and return results.\"\"\"\n",
    "        # Load quiz data\n",
    "        quiz_data = self.load_quiz_data(quiz_file_path)\n",
    "        \n",
    "        if not quiz_data:\n",
    "            print(f\"No quiz data loaded. quiz_file_path = {quiz_file_path} Exiting.\")\n",
    "            return []\n",
    "        \n",
    "        # Limit questions if specified\n",
    "        if limit:\n",
    "            quiz_data = quiz_data[:limit]\n",
    "            # print(f\"Running quiz with {limit} questions.\")\n",
    "        \n",
    "        results = []\n",
    "        correct_count = 0\n",
    "        \n",
    "        for i, question_data in enumerate(quiz_data, 1):\n",
    "            # print(f\"Question {i} of {len(quiz_data)}\")\n",
    "            \n",
    "            question_id = question_data.get(\"id\", i)\n",
    "            question = question_data[\"question\"]\n",
    "            options = question_data[\"options\"]\n",
    "            correct_answer = question_data[\"answer\"]\n",
    "            \n",
    "            # Query the database and generate response\n",
    "            result = self.query_single_question(question, options, show_context=False)\n",
    "            \n",
    "            # Add quiz-specific information\n",
    "            result.update({\n",
    "                'id': question_id,\n",
    "                'options': options,\n",
    "                'correct_answer': correct_answer,\n",
    "                'response' : result['response'],\n",
    "                'is_correct': result['response'].strip().upper() == correct_answer.upper()\n",
    "            })\n",
    "\n",
    "            if result[\"is_correct\"] == False and len(result[\"response\"]) != 1:\n",
    "                if result[\"correct_answer\"].upper().strip() == \"A\":\n",
    "                    alternate_correct_answer = result[\"options\"][0][4:].replace('-', '').strip()\n",
    "                elif result[\"correct_answer\"].upper().strip() == \"B\":\n",
    "                    alternate_correct_answer = result[\"options\"][1][4:].replace('-', '').strip()\n",
    "                elif result[\"correct_answer\"].upper().strip() == \"C\":\n",
    "                    alternate_correct_answer = result[\"options\"][2][4:].replace('-', '').strip()\n",
    "                elif result[\"correct_answer\"].upper().strip() == \"D\":\n",
    "                    alternate_correct_answer = result[\"options\"][3][4:].replace('-', '').strip()\n",
    "                else:\n",
    "                    alternate_correct_answer = \"\"\n",
    "\n",
    "                if alternate_correct_answer.upper() == result[\"response\"].upper():\n",
    "                    result[\"is_correct\"] = True\n",
    "                else:\n",
    "                    if result[\"response\"].upper().startswith(alternate_correct_answer.upper()):\n",
    "                        result[\"response\"] = alternate_correct_answer\n",
    "                        result[\"is_correct\"] = True\n",
    "                    else:\n",
    "                        result[\"is_correct\"] = False\n",
    "\n",
    "            if result['is_correct']:\n",
    "                correct_count += 1\n",
    "            \n",
    "            results.append(result)\n",
    "            \n",
    "        \n",
    "        # Summary\n",
    "        accuracy = (correct_count / len(quiz_data)) * 100 if quiz_data else 0\n",
    "        print(f\"\\nQuiz Summary:\")\n",
    "        print(f\"Correct Answers: {correct_count} / {len(quiz_data)}. Accuracy: {accuracy:.1f}%\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def set_prompt_template(self, new_template):\n",
    "        \"\"\"Set a custom prompt template.\"\"\"\n",
    "        self.PROMPT_TEMPLATE = new_template\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-19T01:46:08.813362Z",
     "iopub.status.busy": "2025-08-19T01:46:08.813167Z",
     "iopub.status.idle": "2025-08-19T01:46:08.832355Z",
     "shell.execute_reply": "2025-08-19T01:46:08.831728Z",
     "shell.execute_reply.started": "2025-08-19T01:46:08.813348Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "EMBEDDING_MODEL_OPTIONS = [\n",
    "    \"sentence-transformers/all-MiniLM-L6-v2\", # success\n",
    "    \"sentence-transformers/all-mpnet-base-v2\", # success\n",
    "    \"BAAI/bge-m3\",\n",
    "    \"BAAI/bge-large-en\", # success\n",
    "    \"BAAI/bge-base-en-v1.5\",\n",
    "    \"BAAI/bge-large-en-v1.5\",\n",
    "    \"intfloat/e5-base-v2\", # success\n",
    "    \"sentence-transformers/static-retrieval-mrl-en-v1\", # success\n",
    "    \"sentence-transformers/all-MiniLM-L12-v2\", # success # best one so far\n",
    "    # \"gemini/embedding-001\",       # Older Gemini model # horrible\n",
    "    # \"gemini/text-embedding-005\",  # New Gemini model\n",
    "    \"nomic-ai/nomic-embed-text-v1.5\",\n",
    "    \"sentence-transformers/multi-qa-mpnet-base-dot-v1\",\n",
    "    \"sentence-transformers/multi-qa-mpnet-base-cos-v1\",\n",
    "    \"hkunlp/instructor-large\",\n",
    "    \"hkunlp/instructor-xl\"\n",
    "]\n",
    "\n",
    "TEXT_GENERATION_MODEL_OPTIONS = [\n",
    "    \"google/flan-t5-small\",\n",
    "    \"google/flan-t5-base\", # have been using this for default development testing\n",
    "    \"google/flan-t5-large\",\n",
    "    \"google/flan-t5-xl\",\n",
    "    \"tiiuae/falcon-7b\",\n",
    "    \"tiiuae/Falcon-H1-0.5B-Instruct\",\n",
    "    \"tiiuae/falcon-7b-instruct\",\n",
    "    # \"mistralai/Mistral-7B-Instruct-v0.3\", # not free\n",
    "    # \"mistralai/Mistral-7B-Instruct-v0.2\", # not free\n",
    "    # \"mistralai/Mistral-Small-3.2-24B-Instruct-2506\", # not free\n",
    "    \"HuggingFaceH4/zephyr-7b-beta\",\n",
    "    \"google/gemma-3-1b-it\",\n",
    "    \"google/gemma-2-2b\",\n",
    "    \"google/gemma-2-2b-it\",\n",
    "    \"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "    \"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "    \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    \"meta-llama/Llama-3.2-1B\",\n",
    "    # \"gpt2\",\n",
    "    # \"distilgpt2\",\n",
    "]\n",
    "\n",
    "# Fixed model types to match all embedding models (all are HuggingFace)\n",
    "EMBEDDING_MODEL_TYPES = [\n",
    "    \"huggingface\",  # 0 - all-MiniLM-L6-v2\n",
    "    \"huggingface\",  # 1 - all-mpnet-base-v2\n",
    "    \"huggingface\",  # 2 - bge-m3\n",
    "    \"huggingface\",  # 3 - bge-large-en\n",
    "    \"huggingface\",  # 4 - bge-base-en-v1.5\n",
    "    \"huggingface\",  # 5 - bge-large-en-v1.5\n",
    "    \"huggingface\",  # 6 - e5-base-v2 (Fixed from \"gemini\")\n",
    "    \"huggingface\",  # 7 - static-retrieval-mrl-en-v1\n",
    "    \"huggingface\",  # 8 - all-MiniLM-L12-v2 (Your best!)\n",
    "    \"huggingface\",  # 9 - nomic-embed-text-v1.5\n",
    "    \"huggingface\",  # 10 - multi-qa-mpnet-base-dot-v1\n",
    "    \"huggingface\",  # 11 - multi-qa-mpnet-base-cos-v1\n",
    "    \"huggingface\",  # 12 - instructor-large\n",
    "    \"huggingface\",  # 13 - instructor-xl\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-19T01:46:08.833321Z",
     "iopub.status.busy": "2025-08-19T01:46:08.833088Z",
     "iopub.status.idle": "2025-08-19T01:46:08.852083Z",
     "shell.execute_reply": "2025-08-19T01:46:08.851314Z",
     "shell.execute_reply.started": "2025-08-19T01:46:08.833295Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def list_models():\n",
    "    \"\"\"List all available models.\"\"\"\n",
    "    print(\"Available Embedding Models:\")\n",
    "    for i, (model, model_type) in enumerate(zip(EMBEDDING_MODEL_OPTIONS, EMBEDDING_MODEL_TYPES)):\n",
    "        print(f\"  {i}: {model} ({model_type})\")\n",
    "    \n",
    "    print(\"\\nAvailable Text Generation Models:\")\n",
    "    for i, model in enumerate(TEXT_GENERATION_MODEL_OPTIONS):\n",
    "        print(f\"  {i}: {model}\")\n",
    "        \n",
    "def clear_cuda_memory():\n",
    "    \"\"\"Clear CUDA memory and run garbage collection.\"\"\"\n",
    "    \n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        # print(\"🧹 Clearing CUDA memory...\")\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "        # print(f\"💾 GPU Memory before cleanup: {torch.cuda.memory_allocated()/1024**3:.2f} GB\")\n",
    "        \n",
    "        # Force garbage collection\n",
    "        gc.collect()\n",
    "        \n",
    "        # Clear cache again after garbage collection\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        # print(f\"✅ GPU Memory after cleanup: {torch.cuda.memory_allocated()/1024**3:.2f} GB\")\n",
    "        # print(f\"📊 GPU Memory cached: {torch.cuda.memory_reserved()/1024**3:.2f} GB\")\n",
    "    else:\n",
    "        # print(\"🖥️  No CUDA device available - running on CPU\")\n",
    "        # Still run garbage collection for CPU\n",
    "        gc.collect()\n",
    "        \n",
    "def main(mode=\"create\", embedding_model_index=0, text_generation_model_index=-1, save_result=1):\n",
    "    clear_cuda_memory()\n",
    "    embedding_model_index = embedding_model_index\n",
    "    \n",
    "    raw_knowledge_directory = \"/kaggle/input/text-for-summarizing/books\"\n",
    "    test_questions_directory = \"/kaggle/input/test-questions/test_questions.json\"\n",
    "    \n",
    "    os.makedirs(\"chroma\", exist_ok=True)\n",
    "    os.makedirs(\"quiz_results\", exist_ok=True)\n",
    "    \n",
    "    # Get selected models\n",
    "    embedding_model = EMBEDDING_MODEL_OPTIONS[embedding_model_index]\n",
    "    embedding_model_type = EMBEDDING_MODEL_TYPES[embedding_model_index]\n",
    "    \n",
    "    db_data_path = f\"chroma/{embedding_model.split('/')[-1].replace('/', '_').replace('-', '_')}\"\n",
    "    \n",
    "    if text_generation_model_index != -1:\n",
    "        text_model = TEXT_GENERATION_MODEL_OPTIONS[text_generation_model_index]\n",
    "        result_file_path = f\"quiz_results/{embedding_model.split('/')[-1].replace('/', '_').replace('-', '_')}--{text_model.split('/')[-1].replace('/', '_').replace('-', '_')}_quiz_results.json\"\n",
    "    \n",
    "    def create_mode():\n",
    "        print(f\"Using embedding model         : {embedding_model} ({embedding_model_type})\")\n",
    "        if os.path.exists(db_data_path):\n",
    "            shutil.rmtree(db_data_path)\n",
    "        os.makedirs(db_data_path, exist_ok=True)\n",
    "        db_manager = DatabaseManager(embedding_model_name=embedding_model, \n",
    "                                   embedding_model_type=embedding_model_type)\n",
    "        db_manager.generate_data_store(data_path=raw_knowledge_directory, \n",
    "                                                persist_directory=db_data_path)\n",
    "\n",
    "    def quiz_mode():\n",
    "        print(\"Running Alice in Wonderland quiz...\")\n",
    "        print(f\"Using embedding model         : {embedding_model} ({embedding_model_type})\")\n",
    "        print(f\"Using text generation model   : {text_model}\")\n",
    "        query_engine = QueryEngine(persist_directory=db_data_path,\n",
    "                                 embedding_model_name=embedding_model,\n",
    "                                 embedding_model_type=embedding_model_type,\n",
    "                                 text_model_name=text_model)\n",
    "        \n",
    "        # Run the quiz\n",
    "        results = query_engine.run_quiz(test_questions_directory)\n",
    "        \n",
    "        # Rest of quiz_mode code remains the same...\n",
    "        if results:\n",
    "            print(\"\\n\" + \"=\"*50)\n",
    "            \n",
    "            if save_result==1:\n",
    "                with open(result_file_path, \"w\") as f:\n",
    "                    json.dump(results, f, indent=4)\n",
    "\n",
    "    if mode==\"create\":\n",
    "        create_mode()\n",
    "    elif mode==\"quiz\":\n",
    "        quiz_mode()\n",
    "    clear_cuda_memory()\n",
    "\n",
    "def run_mains(test_embedding_models=[], test_text_generation_models=[]):\n",
    "    for embedding_model_index in test_embedding_models:\n",
    "        try:\n",
    "            main(mode=\"create\", embedding_model_index=embedding_model_index, text_generation_model_index=0)\n",
    "        except:\n",
    "            print(f\"failed to create db with {EMBEDDING_MODEL_OPTIONS[embedding_model_index]}\")\n",
    "            continue\n",
    "        \n",
    "        for text_generation_model_index in test_text_generation_models:\n",
    "            try: \n",
    "                main(mode=\"quiz\", embedding_model_index=embedding_model_index, text_generation_model_index=text_generation_model_index)\n",
    "                print(f\"\")\n",
    "            except:\n",
    "                print(f\"failed to run quiz with {EMBEDDING_MODEL_OPTIONS[embedding_model_index]} and {TEXT_GENERATION_MODEL_OPTIONS[text_generation_model_index]}\")\n",
    "                continue\n",
    "        if os.path.exists(\"chroma\"):\n",
    "            if os.path.exists(\"chroma\"):\n",
    "                shutil.rmtree(\"chroma\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-19T01:46:08.853128Z",
     "iopub.status.busy": "2025-08-19T01:46:08.852882Z",
     "iopub.status.idle": "2025-08-19T01:46:08.869588Z",
     "shell.execute_reply": "2025-08-19T01:46:08.868922Z",
     "shell.execute_reply.started": "2025-08-19T01:46:08.853106Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Embedding Models:\n",
      "  0: sentence-transformers/all-MiniLM-L6-v2 (huggingface)\n",
      "  1: sentence-transformers/all-mpnet-base-v2 (huggingface)\n",
      "  2: BAAI/bge-m3 (huggingface)\n",
      "  3: BAAI/bge-large-en (huggingface)\n",
      "  4: BAAI/bge-base-en-v1.5 (huggingface)\n",
      "  5: BAAI/bge-large-en-v1.5 (huggingface)\n",
      "  6: intfloat/e5-base-v2 (huggingface)\n",
      "  7: sentence-transformers/static-retrieval-mrl-en-v1 (huggingface)\n",
      "  8: sentence-transformers/all-MiniLM-L12-v2 (huggingface)\n",
      "  9: nomic-ai/nomic-embed-text-v1.5 (huggingface)\n",
      "  10: sentence-transformers/multi-qa-mpnet-base-dot-v1 (huggingface)\n",
      "  11: sentence-transformers/multi-qa-mpnet-base-cos-v1 (huggingface)\n",
      "  12: hkunlp/instructor-large (huggingface)\n",
      "  13: hkunlp/instructor-xl (huggingface)\n",
      "\n",
      "Available Text Generation Models:\n",
      "  0: google/flan-t5-small\n",
      "  1: google/flan-t5-base\n",
      "  2: google/flan-t5-large\n",
      "  3: google/flan-t5-xl\n",
      "  4: tiiuae/falcon-7b\n",
      "  5: tiiuae/Falcon-H1-0.5B-Instruct\n",
      "  6: tiiuae/falcon-7b-instruct\n",
      "  7: HuggingFaceH4/zephyr-7b-beta\n",
      "  8: google/gemma-3-1b-it\n",
      "  9: google/gemma-2-2b\n",
      "  10: google/gemma-2-2b-it\n",
      "  11: meta-llama/Llama-3.1-8B-Instruct\n",
      "  12: meta-llama/Llama-3.2-3B-Instruct\n",
      "  13: meta-llama/Meta-Llama-3-8B-Instruct\n",
      "  14: meta-llama/Llama-3.2-1B\n"
     ]
    }
   ],
   "source": [
    "list_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-08-19T02:02:26.899Z"
    },
    "scrolled": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# run_mains(\n",
    "#     test_embedding_models=[13],\n",
    "#     test_text_generation_models=[3]\n",
    "# )\n",
    "run_mains(\n",
    "    test_embedding_models=[13],\n",
    "    test_text_generation_models=[]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-19T01:50:29.487326Z",
     "iopub.status.busy": "2025-08-19T01:50:29.487077Z",
     "iopub.status.idle": "2025-08-19T01:51:18.133787Z",
     "shell.execute_reply": "2025-08-19T01:51:18.133116Z",
     "shell.execute_reply.started": "2025-08-19T01:50:29.487306Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using embedding model: sentence-transformers/all-MiniLM-L12-v2 (huggingface)\n",
      "Using text generation model: google/flan-t5-small\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0b627d94f2344df9dda4522eece77ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3d5922c5c1e4169866ad65acb279ce6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a75882e575464de0b9df85f1e2eb1645",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f065a660e6f34afc981357bc2f27ccb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbbbf4ae92d14ec7a4b14b4f257bd6a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/615 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "409982ff43fb48c48b2a2bf6071d1614",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/133M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb6de307322749f48ad0c2ff23824dde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/352 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d97369d855c4993adedb4dea8362bb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62c83ce0f9ba4697b6bdbce1fcfc813c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36d09b61bdd5404b8dfc4d8f24ff4d8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6ef7fd038f8426d879e8aa463b06dca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized huggingface embedding model: sentence-transformers/all-MiniLM-L12-v2\n",
      "Embedding model: sentence-transformers/all-MiniLM-L12-v2 (huggingface)\n",
      "Using embedding model: sentence-transformers/all-MiniLM-L12-v2 (huggingface)\n",
      "Using text generation model: google/flan-t5-small\n",
      "Running Alice in Wonderland quiz...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd740cdbf2cb43e98ea42af348a5bde8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5939578215544e87904b5abe08e18fc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/308M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fbfdc7ef93c4aadb7d719e7027aa711",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "077a539e4a9f4d15887a9b7550c83ab2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b610f2bb29f94499a04a134bc85799cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c1052f7ca5c499c8db2b43352fdd173",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dee3239517a24755ac72567b8e24b56d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized huggingface embedding model: sentence-transformers/all-MiniLM-L12-v2 with chat model : google/flan-t5-small\n",
      "\n",
      "Quiz Summary:\n",
      "Correct Answers: 8 / 90. Accuracy: 8.9%\n",
      "\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "run_mains(\n",
    "    test_embedding_models=[8],\n",
    "    test_text_generation_models=[0]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-08-19T02:02:26.895Z",
     "iopub.execute_input": "2025-08-19T01:51:18.136873Z",
     "iopub.status.busy": "2025-08-19T01:51:18.136638Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using embedding model: sentence-transformers/all-MiniLM-L12-v2 (huggingface)\n",
      "Using text generation model: google/flan-t5-small\n",
      "Initialized huggingface embedding model: sentence-transformers/all-MiniLM-L12-v2\n",
      "Embedding model: sentence-transformers/all-MiniLM-L12-v2 (huggingface)\n",
      "Error saving to Chroma: Query error: Database error: error returned from database: (code: 1032) attempt to write a readonly database\n",
      "Using embedding model: sentence-transformers/all-MiniLM-L12-v2 (huggingface)\n",
      "Using text generation model: google/flan-t5-base\n",
      "Running Alice in Wonderland quiz...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f00e33dfb9b4bdb853c618ca1770f1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28932d3ebf1f46eebfc869f995d23bc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5da405385914441cb379bcab918eb387",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80e222a5b6ef42c2b1f6f129bc4f3fbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbb4c3e4ba3e427d93c08560c7a12f52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa3b3f87b5144cf5bc37edfa14554e5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f35ef7a63674e218311c91132c61f33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized huggingface embedding model: sentence-transformers/all-MiniLM-L12-v2 with chat model : google/flan-t5-base\n",
      "\n",
      "Quiz Summary:\n",
      "Correct Answers: 7 / 90. Accuracy: 7.8%\n",
      "\n",
      "==================================================\n",
      "\n",
      "Using embedding model: sentence-transformers/all-MiniLM-L12-v2 (huggingface)\n",
      "Using text generation model: google/flan-t5-large\n",
      "Running Alice in Wonderland quiz...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b26fae25b99d4d1a8944feb359a98dec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/662 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2297963b425e49008dc19b72823167ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.13G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85ca782e37404d218412abc24b2f0760",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c5317dd14604b538abecd5b52b02449",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a223558a22ab494092a6ea1f0ec05c89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "962725718dc2487ebdec47866745467b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff2f81f19dbe426d9eff238062dcd10b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized huggingface embedding model: sentence-transformers/all-MiniLM-L12-v2 with chat model : google/flan-t5-large\n",
      "\n",
      "Quiz Summary:\n",
      "Correct Answers: 11 / 90. Accuracy: 12.2%\n",
      "\n",
      "==================================================\n",
      "\n",
      "Using embedding model: sentence-transformers/all-MiniLM-L12-v2 (huggingface)\n",
      "Using text generation model: google/flan-t5-xl\n",
      "Running Alice in Wonderland quiz...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4b0fd217e8a4f14aa7e98ce85fff026",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized huggingface embedding model: sentence-transformers/all-MiniLM-L12-v2 with chat model : google/flan-t5-xl\n",
      "\n",
      "Quiz Summary:\n",
      "Correct Answers: 9 / 90. Accuracy: 10.0%\n",
      "\n",
      "==================================================\n",
      "\n",
      "Using embedding model: sentence-transformers/all-MiniLM-L12-v2 (huggingface)\n",
      "Using text generation model: tiiuae/falcon-7b\n",
      "Running Alice in Wonderland quiz...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8956ed16cbdc4aa8829102e51f809cbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a570ea5de854187bd234655fd265031",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "configuration_falcon.py: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/tiiuae/falcon-7b:\n",
      "- configuration_falcon.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbc341c51d9a46f0b931607c1c747d84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modeling_falcon.py: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/tiiuae/falcon-7b:\n",
      "- modeling_falcon.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab3219bd382442a6898f94721b2410d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6db4f80d704d4b5eba63a246e322204a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e70b6c037b7e4844a7152645cd8474e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/4.48G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92006a7b3c5e4995b8a6aa05906611b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/9.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run_mains(\n",
    "    test_embedding_models=[8],\n",
    "    test_text_generation_models=[i for i in range(1, len(TEXT_GENERATION_MODEL_OPTIONS))]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-08-19T02:02:26.897Z"
    },
    "scrolled": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model_response_directory = f\"/kaggle/working/quiz_results\"\n",
    "for model_response_fp in os.listdir(model_response_directory):\n",
    "    avg_relevance_sources = []\n",
    "    count = 0\n",
    "    num_questions = 0\n",
    "    with open(os.path.join(model_response_directory, model_response_fp), \"r\") as f:\n",
    "        model_responses = json.load(f)\n",
    "        for response in model_responses:\n",
    "            if response[\"is_correct\"] == True:\n",
    "                count += 1\n",
    "            num_questions += 1\n",
    "            avg_relevance_sources.append(response[\"avg relevance sources\"])\n",
    "    print(f\"Model: {model_response_fp}, Correct: {count}/{num_questions}, Avg Relevance: {sum(avg_relevance_sources) / len(avg_relevance_sources) if avg_relevance_sources else 0}\")\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 8066456,
     "sourceId": 12760370,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8066490,
     "sourceId": 12760417,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
