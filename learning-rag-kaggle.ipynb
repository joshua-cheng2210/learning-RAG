{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-24T05:28:33.351626Z",
     "iopub.status.busy": "2025-09-24T05:28:33.351384Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "bigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\n",
      "datasets 3.6.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.5.1 which is incompatible.\n",
      "cesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
      "pandas-gbq 0.29.1 requires google-api-core<3.0.0,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\n",
      "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
      "plotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\n",
      "dataproc-spark-connect 0.7.5 requires google-api-core>=2.19, but you have google-api-core 1.34.1 which is incompatible.\n",
      "bigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\n",
      "bigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\n",
      "mlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mRequirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement gc (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for gc\u001b[0m\u001b[31m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m69.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.2/209.2 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.8/19.8 MB\u001b[0m \u001b[31m47.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m563.3/563.3 kB\u001b[0m \u001b[31m38.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m38.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m447.5/447.5 kB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m72.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.1/103.1 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m97.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.7/65.7 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.9/131.9 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m208.0/208.0 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.4/71.4 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.0/322.0 kB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m89.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m453.1/453.1 kB\u001b[0m \u001b[31m30.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Core LangChain\n",
    "!pip install -q langchain\n",
    "!pip install torch gc\n",
    "\n",
    "# Cell 2: LangChain integrations\n",
    "!pip install -q langchain-community langchain-huggingface langchain-chroma langchain_experimental\n",
    "# !pip install -q langchain_google_genai\n",
    "!pip install -q huggingface_hub\n",
    "# Cell 3: ML libraries\n",
    "!pip install -q sentence-transformers transformers chromadb\n",
    "\n",
    "# Cell 4: Utilities\n",
    "!pip install -q python-dotenv torch unstructured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-08-20T04:22:29.042814Z",
     "iopub.status.busy": "2025-08-20T04:22:29.042568Z",
     "iopub.status.idle": "2025-08-20T04:22:56.814097Z",
     "shell.execute_reply": "2025-08-20T04:22:56.813262Z",
     "shell.execute_reply.started": "2025-08-20T04:22:29.042791Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-20 04:22:40.809622: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1755663760.996602      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1755663761.047017      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "# from langchain_google_genai import GoogleGenerativeAIEmbeddings  \n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import json\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from transformers import pipeline\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import gc\n",
    "import warnings\n",
    "from huggingface_hub import login\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# os.environ[\"HF_HUB_DISABLE_PROGRESS_BARS\"] = \"1\"\n",
    "# os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "# warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "# warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# # Add this to your existing imports cell\n",
    "# from transformers import logging\n",
    "# logging.set_verbosity_error()  # Only show errors, not info/warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-20T04:22:56.816080Z",
     "iopub.status.busy": "2025-08-20T04:22:56.815335Z",
     "iopub.status.idle": "2025-08-20T04:22:56.911760Z",
     "shell.execute_reply": "2025-08-20T04:22:56.911018Z",
     "shell.execute_reply.started": "2025-08-20T04:22:56.816057Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "gold_stash_fp = \"/kaggle/input/gold-stash/.env\"\n",
    "load_dotenv(gold_stash_fp)\n",
    "# GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "HUGGING_FACE_API = os.getenv(\"HUGGING_FACE_TOKEN\")\n",
    "login(token=HUGGING_FACE_API)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-20T04:22:56.913660Z",
     "iopub.status.busy": "2025-08-20T04:22:56.913442Z",
     "iopub.status.idle": "2025-08-20T04:22:56.924659Z",
     "shell.execute_reply": "2025-08-20T04:22:56.923741Z",
     "shell.execute_reply.started": "2025-08-20T04:22:56.913644Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class DatabaseManager:\n",
    "    def __init__(self, embedding_model_name=\"sentence-transformers/all-MiniLM-L12-v2\", \n",
    "                 embedding_model_type=\"huggingface\"):\n",
    "        \"\"\"\n",
    "        Initialize DatabaseManager with specified embedding model.\n",
    "        \n",
    "        Args:\n",
    "            embedding_model_name: Name of the embedding model\n",
    "            embedding_model_type: Type of model (\"huggingface\" or \"gemini\")\n",
    "        \"\"\"\n",
    "        self.embedding_model_name = embedding_model_name\n",
    "        self.embedding_model_type = embedding_model_type\n",
    "        \n",
    "        # Initialize embedding function based on type\n",
    "        # if embedding_model_type == \"gemini\":\n",
    "        #     api_key = os.getenv(\"GEMINI_API_KEY\")  # Changed from GEMINI_API_KEY\n",
    "        #     if not api_key:\n",
    "        #         raise ValueError(\"GEMINI_API_KEY environment variable is required for Gemini models\")\n",
    "            \n",
    "        #     # Extract model name (remove 'gemini/' prefix)\n",
    "        #     model_name = self.embedding_model_name.replace(\"gemini/\", \"\")\n",
    "        #     self.embedding_function = GoogleGenerativeAIEmbeddings(\n",
    "        #         model=model_name,\n",
    "        #         google_api_key=api_key\n",
    "        #     )\n",
    "        # el\n",
    "        if embedding_model_type == \"huggingface\":\n",
    "            self.embedding_function = HuggingFaceEmbeddings(model_name=embedding_model_name)\n",
    "        else:  # huggingface\n",
    "            self.embedding_model_type == \"huggingface\"\n",
    "            print(f\"{embedding_model_type} embedding_model_type not recognized. Using {self.embedding_model_type}\")\n",
    "            self.embedding_function = HuggingFaceEmbeddings(model_name=embedding_model_name)\n",
    "            \n",
    "        print(f\"Initialized DatabaseManager: Embedding embedding model: {self.embedding_model_name} ({self.embedding_model_type})\")\n",
    "        \n",
    "    # Rest of your DatabaseManager methods remain the same...\n",
    "    def load_documents(self, data_path):\n",
    "        \"\"\"Load documents from the specified directory.\"\"\"\n",
    "        try:\n",
    "            loader = DirectoryLoader(data_path, glob=\"*.md\")\n",
    "            documents = loader.load()\n",
    "            # print(f\"Loaded {len(documents)} documents from {data_path}\")\n",
    "            return documents\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading documents: {e}\")\n",
    "            return []\n",
    "\n",
    "    def split_text(self, documents):\n",
    "        \"\"\"Split documents into chunks.\"\"\"\n",
    "        try:\n",
    "            text_splitter = RecursiveCharacterTextSplitter(\n",
    "                chunk_size=500,\n",
    "                chunk_overlap=150,\n",
    "                length_function=len,\n",
    "                add_start_index=True,\n",
    "            )\n",
    "            chunks = text_splitter.split_documents(documents)\n",
    "            # print(f\"Split into {len(chunks)} chunks\")\n",
    "            return chunks\n",
    "        except Exception as e:\n",
    "            print(f\"Error splitting text: {e}\")\n",
    "            return []\n",
    "\n",
    "    def save_to_chroma(self, chunks, persist_directory):\n",
    "        \"\"\"Save document chunks to Chroma database.\"\"\"\n",
    "        # try:\n",
    "        #     existing_db = Chroma(persist_directory=persist_directory, embedding_function=self.embedding_function)\n",
    "        #     existing_db.delete_collection()\n",
    "        # except Exception as e:\n",
    "        #     pass\n",
    "            \n",
    "        try:\n",
    "            import time\n",
    "            for i in range(3):\n",
    "                try:\n",
    "                    # Create directory if it doesn't exist\n",
    "                    if os.path.exists(persist_directory):\n",
    "                        print(f\"attempt {i+1}: trying to remove {persist_directory}\")\n",
    "                        shutil.rmtree(persist_directory)\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    print(f\"error removing {persist_directory}: {e}\")\n",
    "                    if i < 2:\n",
    "                        gc.collect()\n",
    "                        time.sleep(1)\n",
    "                        \n",
    "                        \n",
    "            # if os.path.exists(persist_directory):\n",
    "            #     print(f\"path exist: {persist_directory}\")\n",
    "            # else:\n",
    "            #     print(f\"path dont exist: {persist_directory}\")\n",
    "                \n",
    "            print(f\"making directory: {persist_directory}\")\n",
    "            os.makedirs(persist_directory, exist_ok=True)\n",
    "            \n",
    "            db = Chroma.from_documents(\n",
    "                chunks, \n",
    "                self.embedding_function, \n",
    "                persist_directory=persist_directory\n",
    "            )\n",
    "            # print(f\"Saved {len(chunks)} chunks to Chroma database at {persist_directory}\")\n",
    "            return db\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving to Chroma: {e}\")\n",
    "            return None\n",
    "\n",
    "    def generate_data_store(self, data_path=\"books\", persist_directory=\"chroma\"):\n",
    "        \"\"\"Complete pipeline: load documents, split text, and save to database.\"\"\"\n",
    "        \n",
    "        # Load documents\n",
    "        documents = self.load_documents(data_path)\n",
    "        if not documents:\n",
    "            return False\n",
    "        \n",
    "        # Split into chunks\n",
    "        chunks = self.split_text(documents)\n",
    "        if not chunks:\n",
    "            return False\n",
    "        \n",
    "        # Save to database\n",
    "        db = self.save_to_chroma(chunks, persist_directory)\n",
    "        return db is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-20T04:22:56.926047Z",
     "iopub.status.busy": "2025-08-20T04:22:56.925715Z",
     "iopub.status.idle": "2025-08-20T04:22:56.956332Z",
     "shell.execute_reply": "2025-08-20T04:22:56.955389Z",
     "shell.execute_reply.started": "2025-08-20T04:22:56.926026Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class QueryEngine:\n",
    "    def __init__(self, persist_directory=\"chroma\", \n",
    "                 embedding_model_name=\"sentence-transformers/all-MiniLM-L12-v2\",\n",
    "                 embedding_model_type=\"huggingface\",\n",
    "                 text_model_name=\"google/flan-t5-base\"):\n",
    "        \"\"\"\n",
    "        Initialize QueryEngine with specified models.\n",
    "        \n",
    "        Args:\n",
    "            persist_directory: Path to the Chroma database\n",
    "            embedding_model_name: Name of the embedding model\n",
    "            embedding_model_type: Type of embedding model (\"huggingface\" or \"gemini\")\n",
    "            text_model_name: Name of the text generation model\n",
    "        \"\"\"\n",
    "        self.persist_directory = persist_directory\n",
    "        self.embedding_model_name = embedding_model_name\n",
    "        self.embedding_model_type = embedding_model_type\n",
    "        self.text_model_name = text_model_name\n",
    "        \n",
    "        # Initialize embedding function based on type\n",
    "        # if embedding_model_type == \"gemini\":\n",
    "        #     api_key = os.getenv(\"GEMINI_API_KEY\")  # Changed from GEMINI_API_KEY\n",
    "        #     if not api_key:\n",
    "        #         raise ValueError(\"GEMINI_API_KEY environment variable is required for Gemini models\")\n",
    "            \n",
    "        #     # Extract model name (remove 'gemini/' prefix)\n",
    "        #     model_name = self.embedding_model_name.replace(\"gemini/\", \"\")\n",
    "        #     self.embedding_function = GoogleGenerativeAIEmbeddings(\n",
    "        #         model=model_name,\n",
    "        #         google_api_key=api_key\n",
    "        #     )\n",
    "        # el\n",
    "        if embedding_model_type == \"huggingface\":\n",
    "            self.embedding_function = HuggingFaceEmbeddings(model_name=embedding_model_name)\n",
    "        else:  # huggingface\n",
    "            self.embedding_model_type == \"huggingface\"\n",
    "            print(f\"{embedding_model_type} embedding_model_type not recognized. Using {self.embedding_model_type}\")\n",
    "            self.embedding_function = HuggingFaceEmbeddings(model_name=embedding_model_name)\n",
    "        \n",
    "        # Initialize text generation model\n",
    "        if self.text_model_name.startswith(\"google/flan\"):\n",
    "            self.hf_pipeline = pipeline(\n",
    "                \"text2text-generation\",\n",
    "                model=self.text_model_name,\n",
    "                max_length=768,\n",
    "                max_new_tokens=100,\n",
    "            )\n",
    "        elif self.text_model_name.startswith(\"mistralai/\"):\n",
    "            self.hf_pipeline = pipeline(\n",
    "                \"text-generation\",  # Mistral uses text-generation\n",
    "                model=self.text_model_name,\n",
    "                max_new_tokens=100,     # Limit output length\n",
    "                do_sample=True,\n",
    "                temperature=0.3,        # Lower temp for more focused answers\n",
    "                pad_token_id=2,         # Mistral's pad token\n",
    "                return_full_text=False, # Only return generated text\n",
    "            )\n",
    "        elif self.text_model_name.startswith(\"gpt\") or self.text_model_name.startswith(\"distilgpt\"):\n",
    "            # Special handling for GPT-2 models to fix the token length issue\n",
    "            self.hf_pipeline = pipeline(\n",
    "                \"text-generation\",\n",
    "                model=self.text_model_name,\n",
    "                max_new_tokens=50,         # Generate only 50 new tokens\n",
    "                do_sample=True,\n",
    "                temperature=0.7,\n",
    "                pad_token_id=50256,\n",
    "                return_full_text=False,    # Only return generated text, not input\n",
    "            )\n",
    "        elif \"falcon\" in self.text_model_name.lower():\n",
    "            # 🔧 ADD: Handle Falcon models\n",
    "            self.hf_pipeline = pipeline(\n",
    "                \"text-generation\",\n",
    "                model=self.text_model_name,\n",
    "                max_new_tokens=100,\n",
    "                do_sample=True,\n",
    "                temperature=0.3,\n",
    "                return_full_text=False,\n",
    "                trust_remote_code=True  # Falcon needs this\n",
    "            )\n",
    "        elif \"zephyr\" in self.text_model_name.lower():\n",
    "            # 🔧 ADD: Handle Zephyr models\n",
    "            self.hf_pipeline = pipeline(\n",
    "                \"text-generation\",\n",
    "                model=self.text_model_name,\n",
    "                max_new_tokens=100,\n",
    "                do_sample=True,\n",
    "                temperature=0.3,\n",
    "                return_full_text=False,\n",
    "            )\n",
    "        elif \"gemma\" in self.text_model_name.lower():\n",
    "            import torch._dynamo\n",
    "            torch._dynamo.config.suppress_errors = True\n",
    "            \n",
    "            # 🔧 ADD: Force eager execution for P100 compatibility\n",
    "            os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:512\"\n",
    "            \n",
    "            # 🔧 ADD: Handle Gemma models\n",
    "            self.hf_pipeline = pipeline(\n",
    "                \"text-generation\",\n",
    "                model=self.text_model_name,\n",
    "                max_new_tokens=100,\n",
    "                do_sample=True,\n",
    "                temperature=0.3,\n",
    "                return_full_text=False,\n",
    "            )\n",
    "        elif \"llama\" in self.text_model_name.lower():\n",
    "            # 🔧 ADD: Handle Llama models\n",
    "            self.hf_pipeline = pipeline(\n",
    "                \"text-generation\",\n",
    "                model=self.text_model_name,\n",
    "                max_new_tokens=100,\n",
    "                do_sample=True,\n",
    "                temperature=0.3,\n",
    "                return_full_text=False,\n",
    "            )\n",
    "        else:\n",
    "            self.text_model_name = \"google/flan-t5-large\"\n",
    "            print(f\"{embedding_model_name} text_model_name not recognized. Using {self.text_model_name}\")\n",
    "            self.hf_pipeline = pipeline(\n",
    "                \"text2text-generation\",\n",
    "                model=self.text_model_name,\n",
    "                max_length=768,\n",
    "                max_new_tokens=100,\n",
    "            )\n",
    "        \n",
    "        self.model = HuggingFacePipeline(pipeline=self.hf_pipeline)\n",
    "        \n",
    "        # Initialize database\n",
    "        self.db = Chroma(persist_directory=persist_directory, \n",
    "                        embedding_function=self.embedding_function)\n",
    "    \n",
    "        self.PROMPT_TEMPLATE = \"\"\"\n",
    "            Answer the question based only on the following context:\n",
    "\n",
    "            {context}\n",
    "\n",
    "            ---\n",
    "\n",
    "            Answer the question based on the above context: {question}\n",
    "            here are the options:\n",
    "            {options}\n",
    "\n",
    "            Respond only the Letter of the correct options like A, B, C and D. Do not inlcude the source.\n",
    "            \"\"\"\n",
    "        # prompt 2: \n",
    "        # \"\"\"\n",
    "        # You are answering questions about Alice in Wonderland based on the provided context.\n",
    "\n",
    "        # CONTEXT:\n",
    "        # {context}\n",
    "        \n",
    "        # QUESTION: {question}\n",
    "        \n",
    "        # OPTIONS:\n",
    "        # {options}\n",
    "        \n",
    "        # INSTRUCTIONS:\n",
    "        # - Read the context carefully\n",
    "        # - Answer based ONLY on the information provided in the context.\n",
    "        # - Respond with ONLY the letter (A, B, C, or D) of the correct answer\n",
    "        # - Do not include explanations or sources\n",
    "        # \"\"\"\n",
    "\n",
    "        # prompt 3: \n",
    "        # \"\"\"\n",
    "        # <s>[INST] You are answering questions about Alice in Wonderland. \n",
    "\n",
    "        # Context: {context_text}\n",
    "        # Question: {question}\n",
    "        # Options: {options_text}\n",
    "        \n",
    "        # INSTRUCTIONS:\n",
    "        # - Read the context carefully\n",
    "        # - Answer based ONLY on the information provided in the context.\n",
    "        # - Respond with ONLY the letter (A, B, C, or D) of the correct answer\n",
    "        # - Do not include explanations or sources\n",
    "        # [/INST]\"\"\"\n",
    "        \n",
    "        # print(f\"QueryEngine initialized:\")\n",
    "        # print(f\"  Embedding: {embedding_model_name} ({embedding_model_type})\")\n",
    "        # print(f\"  Text Generation: {text_model_name}\")\n",
    "        # print(f\"  Database: {persist_directory}\")\n",
    "        print(f\"Initialized QueryEngine: embedding model: {embedding_model_name} ({embedding_model_type}); chat model : {text_model_name}\")\n",
    "\n",
    "    # Rest of your QueryEngine methods remain the same...\n",
    "    \n",
    "    def load_quiz_data(self, quiz_file_path='test_questions.json'):\n",
    "        \"\"\"Load quiz data from JSON file.\"\"\"\n",
    "        try:\n",
    "            with open(quiz_file_path, 'r', encoding='utf-8') as file:\n",
    "                data = json.load(file)\n",
    "                # print(f\"Loaded {len(data)} questions from {quiz_file_path}\")\n",
    "                return data\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Error: {quiz_file_path} file not found!\")\n",
    "            return []\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error parsing JSON: {e}\")\n",
    "            return []\n",
    " \n",
    "    def semantic_search_database(self, query, k=5):\n",
    "        \"\"\"Search the database for relevant documents.\"\"\"\n",
    "        if self.db is None:\n",
    "            return []\n",
    "        \n",
    "        try:\n",
    "            results = self.db.similarity_search_with_relevance_scores(query, k=k)\n",
    "            return results\n",
    "        except Exception as e:\n",
    "            print(f\"Error searching database: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def filter_response(self, response):\n",
    "        edit_response = response.replace('-', '').strip()\n",
    "        return edit_response\n",
    "\n",
    "    def generate_response(self, question, options, context_text):\n",
    "        \"\"\"Generate a response using the LLM.\"\"\"\n",
    "        # Format the prompt\n",
    "        options_text = \"\\n\".join(options) if isinstance(options, list) else str(options)\n",
    "        prompt = self.PROMPT_TEMPLATE.format(\n",
    "            context=context_text, \n",
    "            question=question, \n",
    "            options=options_text\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            # Use the HuggingFace model to generate response\n",
    "            response_text = self.model.invoke(prompt)\n",
    "            response_text = self.filter_response(response_text)\n",
    "            return response_text\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating response: {e}\")\n",
    "            return \"Error generating response.\"\n",
    "    \n",
    "    def query_single_question(self, question, options=None, show_context=False):\n",
    "        \"\"\"Query a single question and return the response.\"\"\"\n",
    "        # Search the database\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\", UserWarning)\n",
    "        results = self.semantic_search_database(question, k=5)\n",
    "        \n",
    "        if not results:\n",
    "            return {\n",
    "                'question': question,\n",
    "                'response': 'No relevant context found.',\n",
    "                'context': '',\n",
    "                'sources': []\n",
    "            }\n",
    "        \n",
    "        # Prepare context from search results\n",
    "        context_text = \"\\n\\n---\\n\\n\".join([doc.page_content for doc, _score in results])\n",
    "        # sources = [doc.metadata.get(\"source\", \"Unknown\") for doc, _score in results]\n",
    "        sources = [(_score, doc.metadata.get(\"source\", \"Unknown\"), doc.page_content) for doc, _score in results]\n",
    "        all_scores = [_score for doc, _score in results]\n",
    "        avg = sum(all_scores) / len(all_scores) if all_scores else 0\n",
    "\n",
    "        \n",
    "        # Generate response\n",
    "        response_text = self.generate_response(question, options or [], context_text)\n",
    "        \n",
    "        result = {\n",
    "            'question': question,\n",
    "            'response': response_text.replace('-', '').strip(),\n",
    "            'sources': sources,\n",
    "            \"avg relevance sources\" : avg\n",
    "        }\n",
    "        \n",
    "        if show_context:\n",
    "            result['context'] = context_text\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def run_quiz(self, quiz_file_path='test_questions.json', show_details=False, limit=None):\n",
    "        \"\"\"Run the complete quiz and return results.\"\"\"\n",
    "        # Load quiz data\n",
    "        quiz_data = self.load_quiz_data(quiz_file_path)\n",
    "        \n",
    "        if not quiz_data:\n",
    "            print(f\"No quiz data loaded. quiz_file_path = {quiz_file_path} Exiting.\")\n",
    "            return []\n",
    "        \n",
    "        # Limit questions if specified\n",
    "        if limit:\n",
    "            quiz_data = quiz_data[:limit]\n",
    "            # print(f\"Running quiz with {limit} questions.\")\n",
    "        \n",
    "        results = []\n",
    "        correct_count = 0\n",
    "        \n",
    "        for i, question_data in enumerate(quiz_data, 1):\n",
    "            # print(f\"Question {i} of {len(quiz_data)}\")\n",
    "            \n",
    "            question_id = question_data.get(\"id\", i)\n",
    "            question = question_data[\"question\"]\n",
    "            options = question_data[\"options\"]\n",
    "            correct_answer = question_data[\"answer\"]\n",
    "            \n",
    "            # Query the database and generate response\n",
    "            result = self.query_single_question(question, options, show_context=False)\n",
    "            \n",
    "            # Add quiz-specific information\n",
    "            result.update({\n",
    "                'id': question_id,\n",
    "                'options': options,\n",
    "                'correct_answer': correct_answer,\n",
    "                'response' : result['response'],\n",
    "                'is_correct': result['response'].strip().upper() == correct_answer.upper()\n",
    "            })\n",
    "\n",
    "            if result[\"is_correct\"] == False and len(result[\"response\"]) != 1:\n",
    "                if result[\"correct_answer\"].upper().strip() == \"A\":\n",
    "                    alternate_correct_answer = result[\"options\"][0][4:].replace('-', '').strip()\n",
    "                elif result[\"correct_answer\"].upper().strip() == \"B\":\n",
    "                    alternate_correct_answer = result[\"options\"][1][4:].replace('-', '').strip()\n",
    "                elif result[\"correct_answer\"].upper().strip() == \"C\":\n",
    "                    alternate_correct_answer = result[\"options\"][2][4:].replace('-', '').strip()\n",
    "                elif result[\"correct_answer\"].upper().strip() == \"D\":\n",
    "                    alternate_correct_answer = result[\"options\"][3][4:].replace('-', '').strip()\n",
    "                else:\n",
    "                    alternate_correct_answer = \"\"\n",
    "\n",
    "                if alternate_correct_answer.upper() == result[\"response\"].upper():\n",
    "                    result[\"is_correct\"] = True\n",
    "                else:\n",
    "                    if result[\"response\"].upper().startswith(alternate_correct_answer.upper()):\n",
    "                        result[\"response\"] = alternate_correct_answer\n",
    "                        result[\"is_correct\"] = True\n",
    "                    else:\n",
    "                        result[\"is_correct\"] = False\n",
    "\n",
    "            if result['is_correct']:\n",
    "                correct_count += 1\n",
    "            \n",
    "            results.append(result)\n",
    "            \n",
    "        \n",
    "        # Summary\n",
    "        accuracy = (correct_count / len(quiz_data)) * 100 if quiz_data else 0\n",
    "        print(f\"\\nQuiz Summary:\")\n",
    "        print(f\"Correct Answers: {correct_count} / {len(quiz_data)}. Accuracy: {accuracy:.1f}%\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def set_prompt_template(self, new_template):\n",
    "        \"\"\"Set a custom prompt template.\"\"\"\n",
    "        self.PROMPT_TEMPLATE = new_template\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-20T04:22:56.957618Z",
     "iopub.status.busy": "2025-08-20T04:22:56.957308Z",
     "iopub.status.idle": "2025-08-20T04:22:56.975007Z",
     "shell.execute_reply": "2025-08-20T04:22:56.974326Z",
     "shell.execute_reply.started": "2025-08-20T04:22:56.957586Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "EMBEDDING_MODEL_OPTIONS = [\n",
    "    \"sentence-transformers/all-MiniLM-L6-v2\", # success\n",
    "    \"sentence-transformers/all-mpnet-base-v2\", # success\n",
    "    \"BAAI/bge-m3\",\n",
    "    \"BAAI/bge-large-en\", # success\n",
    "    \"BAAI/bge-base-en-v1.5\",\n",
    "    \"BAAI/bge-large-en-v1.5\",\n",
    "    \"intfloat/e5-base-v2\", # success\n",
    "    \"sentence-transformers/static-retrieval-mrl-en-v1\", # success\n",
    "    \"sentence-transformers/all-MiniLM-L12-v2\", # success # best one so far\n",
    "    # \"gemini/embedding-001\",       # Older Gemini model # horrible\n",
    "    # \"gemini/text-embedding-005\",  # New Gemini model\n",
    "    \"nomic-ai/nomic-embed-text-v1.5\",\n",
    "    \"sentence-transformers/multi-qa-mpnet-base-dot-v1\",\n",
    "    \"sentence-transformers/multi-qa-mpnet-base-cos-v1\",\n",
    "    \"hkunlp/instructor-large\",\n",
    "    \"hkunlp/instructor-xl\"\n",
    "]\n",
    "\n",
    "TEXT_GENERATION_MODEL_OPTIONS = [\n",
    "    \"google/flan-t5-small\",\n",
    "    \"google/flan-t5-base\", # have been using this for default development testing\n",
    "    \"google/flan-t5-large\",\n",
    "    \"google/flan-t5-xl\",\n",
    "    \"tiiuae/Falcon3-7B-Base\",\n",
    "    \"tiiuae/Falcon3-1B-Instruct\",\n",
    "    \"tiiuae/Falcon3-3B-Instruct\",\n",
    "    \"tiiuae/Falcon3-7B-Instruct\",\n",
    "    \"tiiuae/Falcon3-10B-Instruct\",\n",
    "    \"tiiuae/Falcon-H1-0.5B-Instruct\",\n",
    "    \"HuggingFaceH4/zephyr-7b-beta\",\n",
    "    \"google/gemma-3-1b-it\",\n",
    "    \"google/gemma-2-2b\",\n",
    "    \"google/gemma-2-2b-it\",\n",
    "    \"meta-llama/Llama-3.1-8B-Instruct\", \n",
    "    \"meta-llama/Llama-3.2-3B-Instruct\", \n",
    "    \"meta-llama/Meta-Llama-3-8B-Instruct\", \n",
    "    \"meta-llama/Llama-3.2-1B\", \n",
    "]\n",
    "\n",
    "# Fixed model types to match all embedding models (all are HuggingFace)\n",
    "EMBEDDING_MODEL_TYPES = [\n",
    "    \"huggingface\",  # 0 - all-MiniLM-L6-v2\n",
    "    \"huggingface\",  # 1 - all-mpnet-base-v2\n",
    "    \"huggingface\",  # 2 - bge-m3\n",
    "    \"huggingface\",  # 3 - bge-large-en\n",
    "    \"huggingface\",  # 4 - bge-base-en-v1.5\n",
    "    \"huggingface\",  # 5 - bge-large-en-v1.5\n",
    "    \"huggingface\",  # 6 - e5-base-v2 (Fixed from \"gemini\")\n",
    "    \"huggingface\",  # 7 - static-retrieval-mrl-en-v1\n",
    "    \"huggingface\",  # 8 - all-MiniLM-L12-v2 (Your best!)\n",
    "    \"huggingface\",  # 9 - nomic-embed-text-v1.5\n",
    "    \"huggingface\",  # 10 - multi-qa-mpnet-base-dot-v1\n",
    "    \"huggingface\",  # 11 - multi-qa-mpnet-base-cos-v1\n",
    "    \"huggingface\",  # 12 - instructor-large\n",
    "    \"huggingface\",  # 13 - instructor-xl\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-20T04:22:56.976254Z",
     "iopub.status.busy": "2025-08-20T04:22:56.975959Z",
     "iopub.status.idle": "2025-08-20T04:22:56.999258Z",
     "shell.execute_reply": "2025-08-20T04:22:56.998475Z",
     "shell.execute_reply.started": "2025-08-20T04:22:56.976234Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def list_models():\n",
    "    \"\"\"List all available models.\"\"\"\n",
    "    print(\"Available Embedding Models (EMBEDDING_MODEL_OPTIONS):\")\n",
    "    for i, (model, model_type) in enumerate(zip(EMBEDDING_MODEL_OPTIONS, EMBEDDING_MODEL_TYPES)):\n",
    "        print(f\"  {i}: {model} ({model_type})\")\n",
    "    \n",
    "    print(\"\\nAvailable Text Generation Models (TEXT_GENERATION_MODEL_OPTIONS):\")\n",
    "    for i, model in enumerate(TEXT_GENERATION_MODEL_OPTIONS):\n",
    "        print(f\"  {i}: {model}\")\n",
    "        \n",
    "def clear_cuda_memory():\n",
    "    \"\"\"Clear CUDA memory and run garbage collection.\"\"\"\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "        # Force garbage collection\n",
    "        gc.collect()\n",
    "        \n",
    "        # Clear cache again after garbage collection\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    else:\n",
    "        gc.collect()\n",
    "        \n",
    "def print_results_summary():\n",
    "    model_response_directory = f\"/kaggle/working/quiz_results\"\n",
    "    if os.path.exists(model_response_directory):\n",
    "        for model_response_fp in os.listdir(model_response_directory):\n",
    "            avg_relevance_sources = []\n",
    "            count = 0\n",
    "            num_questions = 0\n",
    "            with open(os.path.join(model_response_directory, model_response_fp), \"r\") as f:\n",
    "                model_responses = json.load(f)\n",
    "                for response in model_responses:\n",
    "                    if response[\"is_correct\"] == True:\n",
    "                        count += 1\n",
    "                    num_questions += 1\n",
    "                    avg_relevance_sources.append(response[\"avg relevance sources\"])\n",
    "            print(f\"Model: {model_response_fp}, Correct: {count}/{num_questions}, Avg Relevance: {sum(avg_relevance_sources) / len(avg_relevance_sources) if avg_relevance_sources else 0}\")\n",
    "                        \n",
    "def main(mode=\"create\", embedding_model_index=0, text_generation_model_index=-1, save_result=1):\n",
    "    print(\"=\" * 80)\n",
    "    clear_cuda_memory()\n",
    "    embedding_model_index = embedding_model_index\n",
    "    \n",
    "    raw_knowledge_directory = \"/kaggle/input/text-for-summarizing/books\"\n",
    "    test_questions_directory = \"/kaggle/input/test-questions/test_questions.json\"\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Get selected models\n",
    "    embedding_model = EMBEDDING_MODEL_OPTIONS[embedding_model_index]\n",
    "    embedding_model_type = EMBEDDING_MODEL_TYPES[embedding_model_index]\n",
    "    \n",
    "    db_name = \"chroma\"\n",
    "    db_data_path = f\"{db_name}/{embedding_model.split('/')[-1].replace('/', '_').replace('-', '_')}\"\n",
    "    \n",
    "    if text_generation_model_index != -1:\n",
    "        text_model = TEXT_GENERATION_MODEL_OPTIONS[text_generation_model_index]\n",
    "        result_file_path = f\"quiz_results/{embedding_model.split('/')[-1].replace('/', '_').replace('-', '_')}--{text_model.split('/')[-1].replace('/', '_').replace('-', '_')}_quiz_results.json\"\n",
    "    \n",
    "    def create_mode():\n",
    "        print(f\"Using embedding model         : {embedding_model} ({embedding_model_type})\")\n",
    "        # os.makedirs(db_name, exist_ok=True)\n",
    "        # os.makedirs(db_data_path, exist_ok=True)\n",
    "        db_manager = DatabaseManager(embedding_model_name=embedding_model, \n",
    "                                   embedding_model_type=embedding_model_type)\n",
    "        db_manager.generate_data_store(data_path=raw_knowledge_directory, \n",
    "                                                persist_directory=db_data_path)\n",
    "\n",
    "    def quiz_mode():\n",
    "        print(\"Running Alice in Wonderland quiz...\")\n",
    "        print(f\"Using embedding model         : {embedding_model} ({embedding_model_type})\")\n",
    "        print(f\"Using text generation model   : {text_model}\")\n",
    "        os.makedirs(\"quiz_results\", exist_ok=True)\n",
    "        query_engine = QueryEngine(persist_directory=db_data_path,\n",
    "                                 embedding_model_name=embedding_model,\n",
    "                                 embedding_model_type=embedding_model_type,\n",
    "                                 text_model_name=text_model)\n",
    "        \n",
    "        # Run the quiz\n",
    "        results = query_engine.run_quiz(test_questions_directory)\n",
    "        \n",
    "        # Rest of quiz_mode code remains the same...\n",
    "        if results:\n",
    "            if save_result==1:\n",
    "                with open(result_file_path, \"w\") as f:\n",
    "                    json.dump(results, f, indent=4)\n",
    "\n",
    "    if mode==\"create\":\n",
    "        create_mode()\n",
    "    elif mode==\"quiz\":\n",
    "        quiz_mode()\n",
    "    clear_cuda_memory()\n",
    "\n",
    "def run_mains(test_embedding_models=[], test_text_generation_models=[]):\n",
    "    counter = 0\n",
    "    for embedding_model_index in test_embedding_models:\n",
    "        try:\n",
    "            main(mode=\"create\", embedding_model_index=embedding_model_index)\n",
    "            print(f\"successfully created db with {EMBEDDING_MODEL_OPTIONS[embedding_model_index]}\")\n",
    "        except Exception as e:\n",
    "            print(f\"failed to create db with {EMBEDDING_MODEL_OPTIONS[embedding_model_index]}\")\n",
    "            print(e)\n",
    "            if os.path.exists(\"chroma\"):\n",
    "                shutil.rmtree(\"chroma\")\n",
    "            continue\n",
    "        \n",
    "        for text_generation_model_index in test_text_generation_models:\n",
    "            counter += 1\n",
    "            try: \n",
    "                main(mode=\"quiz\", embedding_model_index=embedding_model_index, text_generation_model_index=text_generation_model_index)\n",
    "                print(f\"successfully ran quiz with {EMBEDDING_MODEL_OPTIONS[embedding_model_index]} and {TEXT_GENERATION_MODEL_OPTIONS[text_generation_model_index]}. {counter} / {len(test_embedding_models) * len(test_text_generation_models)} models combination tested. testing {embedding_model_index + 1} / {len(test_embedding_models)} embedding models. tested {text_generation_model_index + 1} / {len(test_text_generation_models)} chat models.\")\n",
    "                print()\n",
    "                print_results_summary()\n",
    "            except Exception as e:\n",
    "                print(f\"failed to run quiz with {EMBEDDING_MODEL_OPTIONS[embedding_model_index]} and {TEXT_GENERATION_MODEL_OPTIONS[text_generation_model_index]}. {counter} / {len(test_embedding_models) * len(test_text_generation_models)} models combination tested. testing {embedding_model_index + 1} / {len(test_embedding_models)} embedding models. tested {text_generation_model_index + 1} / {len(test_text_generation_models)} chat models.\")\n",
    "                print(e)\n",
    "                continue\n",
    "        if os.path.exists(\"chroma\"):\n",
    "            shutil.rmtree(\"chroma\")\n",
    "            \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-20T04:22:57.000298Z",
     "iopub.status.busy": "2025-08-20T04:22:57.000061Z",
     "iopub.status.idle": "2025-08-20T04:22:57.020725Z",
     "shell.execute_reply": "2025-08-20T04:22:57.020002Z",
     "shell.execute_reply.started": "2025-08-20T04:22:57.000280Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Embedding Models (EMBEDDING_MODEL_OPTIONS):\n",
      "  0: sentence-transformers/all-MiniLM-L6-v2 (huggingface)\n",
      "  1: sentence-transformers/all-mpnet-base-v2 (huggingface)\n",
      "  2: BAAI/bge-m3 (huggingface)\n",
      "  3: BAAI/bge-large-en (huggingface)\n",
      "  4: BAAI/bge-base-en-v1.5 (huggingface)\n",
      "  5: BAAI/bge-large-en-v1.5 (huggingface)\n",
      "  6: intfloat/e5-base-v2 (huggingface)\n",
      "  7: sentence-transformers/static-retrieval-mrl-en-v1 (huggingface)\n",
      "  8: sentence-transformers/all-MiniLM-L12-v2 (huggingface)\n",
      "  9: nomic-ai/nomic-embed-text-v1.5 (huggingface)\n",
      "  10: sentence-transformers/multi-qa-mpnet-base-dot-v1 (huggingface)\n",
      "  11: sentence-transformers/multi-qa-mpnet-base-cos-v1 (huggingface)\n",
      "  12: hkunlp/instructor-large (huggingface)\n",
      "  13: hkunlp/instructor-xl (huggingface)\n",
      "\n",
      "Available Text Generation Models (TEXT_GENERATION_MODEL_OPTIONS):\n",
      "  0: google/flan-t5-small\n",
      "  1: google/flan-t5-base\n",
      "  2: google/flan-t5-large\n",
      "  3: google/flan-t5-xl\n",
      "  4: tiiuae/Falcon3-7B-Base\n",
      "  5: tiiuae/Falcon3-1B-Instruct\n",
      "  6: tiiuae/Falcon3-3B-Instruct\n",
      "  7: tiiuae/Falcon3-7B-Instruct\n",
      "  8: tiiuae/Falcon3-10B-Instruct\n",
      "  9: tiiuae/Falcon-H1-0.5B-Instruct\n",
      "  10: HuggingFaceH4/zephyr-7b-beta\n",
      "  11: google/gemma-3-1b-it\n",
      "  12: google/gemma-2-2b\n",
      "  13: google/gemma-2-2b-it\n",
      "  14: meta-llama/Llama-3.1-8B-Instruct\n",
      "  15: meta-llama/Llama-3.2-3B-Instruct\n",
      "  16: meta-llama/Meta-Llama-3-8B-Instruct\n",
      "  17: meta-llama/Llama-3.2-1B\n"
     ]
    }
   ],
   "source": [
    "list_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-20T04:22:57.021917Z",
     "iopub.status.busy": "2025-08-20T04:22:57.021588Z",
     "iopub.status.idle": "2025-08-20T04:22:57.035515Z",
     "shell.execute_reply": "2025-08-20T04:22:57.034721Z",
     "shell.execute_reply.started": "2025-08-20T04:22:57.021892Z"
    },
    "scrolled": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# run_mains(\n",
    "#     test_embedding_models=[i for i in range(len(EMBEDDING_MODEL_OPTIONS))],\n",
    "#     test_text_generation_models=[i for i in range(len(TEXT_GENERATION_MODEL_OPTIONS)) if \"flan\" in TEXT_GENERATION_MODEL_OPTIONS[i]]\n",
    "# )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-08-20T04:56:04.688Z",
     "iopub.execute_input": "2025-08-20T04:22:57.047038Z",
     "iopub.status.busy": "2025-08-20T04:22:57.046800Z"
    },
    "scrolled": true,
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Using embedding model         : sentence-transformers/all-MiniLM-L6-v2 (huggingface)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1032d5cfb43e443a91f0098180f9d8cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b4483b3c04c4d9cb1cbeb2bcfaeda2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f5d3751140c4214a1484e132b533cdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df8d7b10f2f24f5ba902ab23eb28a39c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20b4a1ad36cb4ed79a9aa233b23aa57d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d18f5b930f5d4315adb9f78e110f4037",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20a5dc2b7d2440879047c065214bacb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eaf674ea60bc4fa8b4ae0d74c3ea7106",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f45a4f1ef786432d83297a275ccc0100",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9eeb2266200e40db83e6e9a2d69d678f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb1826d98d4a45b68bcaed8a199235bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized DatabaseManager: Embedding embedding model: sentence-transformers/all-MiniLM-L6-v2 (huggingface)\n",
      "making directory: chroma/all_MiniLM_L6_v2\n",
      "successfully created db with sentence-transformers/all-MiniLM-L6-v2\n",
      "================================================================================\n",
      "Running Alice in Wonderland quiz...\n",
      "Using embedding model         : sentence-transformers/all-MiniLM-L6-v2 (huggingface)\n",
      "Using text generation model   : google/gemma-3-1b-it\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69b8235be197452fa96fed980f41879d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/899 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e5b4f3a17c6486a84ab3ab95847ec3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1ffa10a2e4b4a5a80abeb906e47ad85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/215 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1887538499604dfe9fb0540cfce4fc1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.16M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7a69c866b29485890bb47c1abd569ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.69M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3796cc5c1b949d48c2d8e0c0fe13bd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/33.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d37f4c82907e41f2a552a95fb1e0ff88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/35.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce5b04dcdee44f20800810e353521eff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/662 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized QueryEngine: embedding model: sentence-transformers/all-MiniLM-L6-v2 (huggingface); chat model : google/gemma-3-1b-it\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0820 04:23:59.583000 36 torch/_inductor/utils.py:1137] [0/0] Not enough SMs to use max_autotune_gemm mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error generating response: backend='inductor' raised:\n",
      "RuntimeError: Found Tesla P100-PCIE-16GB which is too old to be supported by the triton GPU compiler, which is used as the backend. Triton only supports devices of CUDA Capability >= 7.0, but your device is of CUDA capability 6.0\n",
      "\n",
      "Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
      "\n",
      "\n",
      "You can suppress this exception and fall back to eager by setting:\n",
      "    import torch._dynamo\n",
      "    torch._dynamo.config.suppress_errors = True\n",
      "\n",
      "Error generating response: backend='inductor' raised:\n",
      "RuntimeError: Found Tesla P100-PCIE-16GB which is too old to be supported by the triton GPU compiler, which is used as the backend. Triton only supports devices of CUDA Capability >= 7.0, but your device is of CUDA capability 6.0\n",
      "\n",
      "Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
      "\n",
      "\n",
      "You can suppress this exception and fall back to eager by setting:\n",
      "    import torch._dynamo\n",
      "    torch._dynamo.config.suppress_errors = True\n",
      "\n",
      "Error generating response: backend='inductor' raised:\n",
      "RuntimeError: Found Tesla P100-PCIE-16GB which is too old to be supported by the triton GPU compiler, which is used as the backend. Triton only supports devices of CUDA Capability >= 7.0, but your device is of CUDA capability 6.0\n",
      "\n",
      "Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
      "\n",
      "\n",
      "You can suppress this exception and fall back to eager by setting:\n",
      "    import torch._dynamo\n",
      "    torch._dynamo.config.suppress_errors = True\n",
      "\n",
      "Error generating response: backend='inductor' raised:\n",
      "RuntimeError: Found Tesla P100-PCIE-16GB which is too old to be supported by the triton GPU compiler, which is used as the backend. Triton only supports devices of CUDA Capability >= 7.0, but your device is of CUDA capability 6.0\n",
      "\n",
      "Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
      "\n",
      "\n",
      "You can suppress this exception and fall back to eager by setting:\n",
      "    import torch._dynamo\n",
      "    torch._dynamo.config.suppress_errors = True\n",
      "\n",
      "Error generating response: backend='inductor' raised:\n",
      "RuntimeError: Found Tesla P100-PCIE-16GB which is too old to be supported by the triton GPU compiler, which is used as the backend. Triton only supports devices of CUDA Capability >= 7.0, but your device is of CUDA capability 6.0\n",
      "\n",
      "Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
      "\n",
      "\n",
      "You can suppress this exception and fall back to eager by setting:\n",
      "    import torch._dynamo\n",
      "    torch._dynamo.config.suppress_errors = True\n",
      "\n",
      "Error generating response: backend='inductor' raised:\n",
      "RuntimeError: Found Tesla P100-PCIE-16GB which is too old to be supported by the triton GPU compiler, which is used as the backend. Triton only supports devices of CUDA Capability >= 7.0, but your device is of CUDA capability 6.0\n",
      "\n",
      "Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
      "\n",
      "\n",
      "You can suppress this exception and fall back to eager by setting:\n",
      "    import torch._dynamo\n",
      "    torch._dynamo.config.suppress_errors = True\n",
      "\n",
      "Error generating response: backend='inductor' raised:\n",
      "RuntimeError: Found Tesla P100-PCIE-16GB which is too old to be supported by the triton GPU compiler, which is used as the backend. Triton only supports devices of CUDA Capability >= 7.0, but your device is of CUDA capability 6.0\n",
      "\n",
      "Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
      "\n",
      "\n",
      "You can suppress this exception and fall back to eager by setting:\n",
      "    import torch._dynamo\n",
      "    torch._dynamo.config.suppress_errors = True\n",
      "\n",
      "Error generating response: backend='inductor' raised:\n",
      "RuntimeError: Found Tesla P100-PCIE-16GB which is too old to be supported by the triton GPU compiler, which is used as the backend. Triton only supports devices of CUDA Capability >= 7.0, but your device is of CUDA capability 6.0\n",
      "\n",
      "Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
      "\n",
      "\n",
      "You can suppress this exception and fall back to eager by setting:\n",
      "    import torch._dynamo\n",
      "    torch._dynamo.config.suppress_errors = True\n",
      "\n",
      "Error generating response: backend='inductor' raised:\n",
      "RuntimeError: Found Tesla P100-PCIE-16GB which is too old to be supported by the triton GPU compiler, which is used as the backend. Triton only supports devices of CUDA Capability >= 7.0, but your device is of CUDA capability 6.0\n",
      "\n",
      "Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
      "\n",
      "\n",
      "You can suppress this exception and fall back to eager by setting:\n",
      "    import torch._dynamo\n",
      "    torch._dynamo.config.suppress_errors = True\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error generating response: backend='inductor' raised:\n",
      "RuntimeError: Found Tesla P100-PCIE-16GB which is too old to be supported by the triton GPU compiler, which is used as the backend. Triton only supports devices of CUDA Capability >= 7.0, but your device is of CUDA capability 6.0\n",
      "\n",
      "Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
      "\n",
      "\n",
      "You can suppress this exception and fall back to eager by setting:\n",
      "    import torch._dynamo\n",
      "    torch._dynamo.config.suppress_errors = True\n",
      "\n",
      "Error generating response: backend='inductor' raised:\n",
      "RuntimeError: Found Tesla P100-PCIE-16GB which is too old to be supported by the triton GPU compiler, which is used as the backend. Triton only supports devices of CUDA Capability >= 7.0, but your device is of CUDA capability 6.0\n",
      "\n",
      "Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
      "\n",
      "\n",
      "You can suppress this exception and fall back to eager by setting:\n",
      "    import torch._dynamo\n",
      "    torch._dynamo.config.suppress_errors = True\n",
      "\n",
      "Error generating response: backend='inductor' raised:\n",
      "RuntimeError: Found Tesla P100-PCIE-16GB which is too old to be supported by the triton GPU compiler, which is used as the backend. Triton only supports devices of CUDA Capability >= 7.0, but your device is of CUDA capability 6.0\n",
      "\n",
      "Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
      "\n",
      "\n",
      "You can suppress this exception and fall back to eager by setting:\n",
      "    import torch._dynamo\n",
      "    torch._dynamo.config.suppress_errors = True\n",
      "\n",
      "Error generating response: backend='inductor' raised:\n",
      "RuntimeError: Found Tesla P100-PCIE-16GB which is too old to be supported by the triton GPU compiler, which is used as the backend. Triton only supports devices of CUDA Capability >= 7.0, but your device is of CUDA capability 6.0\n",
      "\n",
      "Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
      "\n",
      "\n",
      "You can suppress this exception and fall back to eager by setting:\n",
      "    import torch._dynamo\n",
      "    torch._dynamo.config.suppress_errors = True\n",
      "\n",
      "Error generating response: backend='inductor' raised:\n",
      "RuntimeError: Found Tesla P100-PCIE-16GB which is too old to be supported by the triton GPU compiler, which is used as the backend. Triton only supports devices of CUDA Capability >= 7.0, but your device is of CUDA capability 6.0\n",
      "\n",
      "Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
      "\n",
      "\n",
      "You can suppress this exception and fall back to eager by setting:\n",
      "    import torch._dynamo\n",
      "    torch._dynamo.config.suppress_errors = True\n",
      "\n",
      "Error generating response: backend='inductor' raised:\n",
      "RuntimeError: Found Tesla P100-PCIE-16GB which is too old to be supported by the triton GPU compiler, which is used as the backend. Triton only supports devices of CUDA Capability >= 7.0, but your device is of CUDA capability 6.0\n",
      "\n",
      "Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
      "\n",
      "\n",
      "You can suppress this exception and fall back to eager by setting:\n",
      "    import torch._dynamo\n",
      "    torch._dynamo.config.suppress_errors = True\n",
      "\n",
      "Error generating response: backend='inductor' raised:\n",
      "RuntimeError: Found Tesla P100-PCIE-16GB which is too old to be supported by the triton GPU compiler, which is used as the backend. Triton only supports devices of CUDA Capability >= 7.0, but your device is of CUDA capability 6.0\n",
      "\n",
      "Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
      "\n",
      "\n",
      "You can suppress this exception and fall back to eager by setting:\n",
      "    import torch._dynamo\n",
      "    torch._dynamo.config.suppress_errors = True\n",
      "\n",
      "Error generating response: backend='inductor' raised:\n",
      "RuntimeError: Found Tesla P100-PCIE-16GB which is too old to be supported by the triton GPU compiler, which is used as the backend. Triton only supports devices of CUDA Capability >= 7.0, but your device is of CUDA capability 6.0\n",
      "\n",
      "Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
      "\n",
      "\n",
      "You can suppress this exception and fall back to eager by setting:\n",
      "    import torch._dynamo\n",
      "    torch._dynamo.config.suppress_errors = True\n",
      "\n",
      "Error generating response: backend='inductor' raised:\n",
      "RuntimeError: Found Tesla P100-PCIE-16GB which is too old to be supported by the triton GPU compiler, which is used as the backend. Triton only supports devices of CUDA Capability >= 7.0, but your device is of CUDA capability 6.0\n",
      "\n",
      "Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
      "\n",
      "\n",
      "You can suppress this exception and fall back to eager by setting:\n",
      "    import torch._dynamo\n",
      "    torch._dynamo.config.suppress_errors = True\n",
      "\n",
      "Error generating response: backend='inductor' raised:\n",
      "RuntimeError: Found Tesla P100-PCIE-16GB which is too old to be supported by the triton GPU compiler, which is used as the backend. Triton only supports devices of CUDA Capability >= 7.0, but your device is of CUDA capability 6.0\n",
      "\n",
      "Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
      "\n",
      "\n",
      "You can suppress this exception and fall back to eager by setting:\n",
      "    import torch._dynamo\n",
      "    torch._dynamo.config.suppress_errors = True\n",
      "\n",
      "Error generating response: backend='inductor' raised:\n",
      "RuntimeError: Found Tesla P100-PCIE-16GB which is too old to be supported by the triton GPU compiler, which is used as the backend. Triton only supports devices of CUDA Capability >= 7.0, but your device is of CUDA capability 6.0\n",
      "\n",
      "Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
      "\n",
      "\n",
      "You can suppress this exception and fall back to eager by setting:\n",
      "    import torch._dynamo\n",
      "    torch._dynamo.config.suppress_errors = True\n",
      "\n",
      "Error generating response: backend='inductor' raised:\n",
      "RuntimeError: Found Tesla P100-PCIE-16GB which is too old to be supported by the triton GPU compiler, which is used as the backend. Triton only supports devices of CUDA Capability >= 7.0, but your device is of CUDA capability 6.0\n",
      "\n",
      "Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
      "\n",
      "\n",
      "You can suppress this exception and fall back to eager by setting:\n",
      "    import torch._dynamo\n",
      "    torch._dynamo.config.suppress_errors = True\n",
      "\n",
      "Error generating response: backend='inductor' raised:\n",
      "RuntimeError: Found Tesla P100-PCIE-16GB which is too old to be supported by the triton GPU compiler, which is used as the backend. Triton only supports devices of CUDA Capability >= 7.0, but your device is of CUDA capability 6.0\n",
      "\n",
      "Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
      "\n",
      "\n",
      "You can suppress this exception and fall back to eager by setting:\n",
      "    import torch._dynamo\n",
      "    torch._dynamo.config.suppress_errors = True\n",
      "\n",
      "Error generating response: backend='inductor' raised:\n",
      "RuntimeError: Found Tesla P100-PCIE-16GB which is too old to be supported by the triton GPU compiler, which is used as the backend. Triton only supports devices of CUDA Capability >= 7.0, but your device is of CUDA capability 6.0\n",
      "\n",
      "Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
      "\n",
      "\n",
      "You can suppress this exception and fall back to eager by setting:\n",
      "    import torch._dynamo\n",
      "    torch._dynamo.config.suppress_errors = True\n",
      "\n",
      "Error generating response: backend='inductor' raised:\n",
      "RuntimeError: Found Tesla P100-PCIE-16GB which is too old to be supported by the triton GPU compiler, which is used as the backend. Triton only supports devices of CUDA Capability >= 7.0, but your device is of CUDA capability 6.0\n",
      "\n",
      "Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
      "\n",
      "\n",
      "You can suppress this exception and fall back to eager by setting:\n",
      "    import torch._dynamo\n",
      "    torch._dynamo.config.suppress_errors = True\n",
      "\n",
      "Error generating response: backend='inductor' raised:\n",
      "RuntimeError: Found Tesla P100-PCIE-16GB which is too old to be supported by the triton GPU compiler, which is used as the backend. Triton only supports devices of CUDA Capability >= 7.0, but your device is of CUDA capability 6.0\n",
      "\n",
      "Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
      "\n",
      "\n",
      "You can suppress this exception and fall back to eager by setting:\n",
      "    import torch._dynamo\n",
      "    torch._dynamo.config.suppress_errors = True\n",
      "\n",
      "Error generating response: backend='inductor' raised:\n",
      "RuntimeError: Found Tesla P100-PCIE-16GB which is too old to be supported by the triton GPU compiler, which is used as the backend. Triton only supports devices of CUDA Capability >= 7.0, but your device is of CUDA capability 6.0\n",
      "\n",
      "Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
      "\n",
      "\n",
      "You can suppress this exception and fall back to eager by setting:\n",
      "    import torch._dynamo\n",
      "    torch._dynamo.config.suppress_errors = True\n",
      "\n",
      "Error generating response: backend='inductor' raised:\n",
      "RuntimeError: Found Tesla P100-PCIE-16GB which is too old to be supported by the triton GPU compiler, which is used as the backend. Triton only supports devices of CUDA Capability >= 7.0, but your device is of CUDA capability 6.0\n",
      "\n",
      "Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
      "\n",
      "\n",
      "You can suppress this exception and fall back to eager by setting:\n",
      "    import torch._dynamo\n",
      "    torch._dynamo.config.suppress_errors = True\n",
      "\n",
      "Error generating response: backend='inductor' raised:\n",
      "RuntimeError: Found Tesla P100-PCIE-16GB which is too old to be supported by the triton GPU compiler, which is used as the backend. Triton only supports devices of CUDA Capability >= 7.0, but your device is of CUDA capability 6.0\n",
      "\n",
      "Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
      "\n",
      "\n",
      "You can suppress this exception and fall back to eager by setting:\n",
      "    import torch._dynamo\n",
      "    torch._dynamo.config.suppress_errors = True\n",
      "\n",
      "Error generating response: backend='inductor' raised:\n",
      "RuntimeError: Found Tesla P100-PCIE-16GB which is too old to be supported by the triton GPU compiler, which is used as the backend. Triton only supports devices of CUDA Capability >= 7.0, but your device is of CUDA capability 6.0\n",
      "\n",
      "Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
      "\n",
      "\n",
      "You can suppress this exception and fall back to eager by setting:\n",
      "    import torch._dynamo\n",
      "    torch._dynamo.config.suppress_errors = True\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_36/952522206.py:201: UserWarning: Relevance scores must be between 0 and 1, got [(Document(id='454180a2-d550-4774-926d-1a70930547a9', metadata={'source': '/kaggle/input/text-for-summarizing/books/alice_in_wonderland.md', 'start_index': 23538}, page_content='CHAPTER III. A Caucus-Race and a Long Tale\\n\\nThey were indeed a queer-looking party that assembled on the bank—the birds with draggled feathers, the animals with their fur clinging close to them, and all dripping wet, cross, and uncomfortable.'), 0.36227207337790246), (Document(id='e5123963-088f-4781-99de-0d9fab8dfc40', metadata={'start_index': 26447, 'source': '/kaggle/input/text-for-summarizing/books/alice_in_wonderland.md'}, page_content='“What I was going to say,” said the Dodo in an offended tone, “was, that the best thing to get us dry would be a Caucus-race.”\\n\\n“What is a Caucus-race?” said Alice; not that she wanted much to know, but the Dodo had paused as if it thought that somebody ought to speak, and no one else seemed inclined to say anything.\\n\\n“Why,” said the Dodo, “the best way to explain it is to do it.” (And, as you might like to try the thing yourself, some winter day, I will tell you how the Dodo managed it.)'), 0.1561679993071421), (Document(id='0bd663d8-6e5f-4030-a6dc-126c2b20352d', metadata={'source': '/kaggle/input/text-for-summarizing/books/alice_in_wonderland.md', 'start_index': 23275}, page_content='It was high time to go, for the pool was getting quite crowded with the birds and animals that had fallen into it: there were a Duck and a Dodo, a Lory and an Eaglet, and several other curious creatures. Alice led the way, and the whole party swam to the shore.\\n\\nCHAPTER III. A Caucus-Race and a Long Tale'), 0.09357866486300914), (Document(id='66e6ab57-719d-4fe4-a557-13e5adc30415', metadata={'source': '/kaggle/input/text-for-summarizing/books/alice_in_wonderland.md', 'start_index': 26942}, page_content='First it marked out a race-course, in a sort of circle, (“the exact shape doesn’t matter,” it said,) and then all the party were placed along the course, here and there. There was no “One, two, three, and away,” but they began running when they liked, and left off when they liked, so that it was not easy to know when the race was over. However, when they had been running half an hour or so, and were quite dry again, the Dodo suddenly called out “The race is over!” and they all crowded round it,'), 0.040511652196639436), (Document(id='874d7ed1-3870-4cd4-bd21-e37b1e5cb6af', metadata={'source': '/kaggle/input/text-for-summarizing/books/alice_in_wonderland.md', 'start_index': 124059}, page_content='The twelve jurors were all writing very busily on slates. “What are they doing?” Alice whispered to the Gryphon. “They can’t have anything to put down yet, before the trial’s begun.”\\n\\n“They’re putting down their names,” the Gryphon whispered in reply, “for fear they should forget them before the end of the trial.”'), -0.08629169340802445)]\n",
      "  results = self.db.similarity_search_with_relevance_scores(query, k=k)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error generating response: backend='inductor' raised:\n",
      "RuntimeError: Found Tesla P100-PCIE-16GB which is too old to be supported by the triton GPU compiler, which is used as the backend. Triton only supports devices of CUDA Capability >= 7.0, but your device is of CUDA capability 6.0\n",
      "\n",
      "Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
      "\n",
      "\n",
      "You can suppress this exception and fall back to eager by setting:\n",
      "    import torch._dynamo\n",
      "    torch._dynamo.config.suppress_errors = True\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_36/952522206.py:201: UserWarning: Relevance scores must be between 0 and 1, got [(Document(id='454180a2-d550-4774-926d-1a70930547a9', metadata={'start_index': 23538, 'source': '/kaggle/input/text-for-summarizing/books/alice_in_wonderland.md'}, page_content='CHAPTER III. A Caucus-Race and a Long Tale\\n\\nThey were indeed a queer-looking party that assembled on the bank—the birds with draggled feathers, the animals with their fur clinging close to them, and all dripping wet, cross, and uncomfortable.'), 0.362647812532377), (Document(id='e5123963-088f-4781-99de-0d9fab8dfc40', metadata={'source': '/kaggle/input/text-for-summarizing/books/alice_in_wonderland.md', 'start_index': 26447}, page_content='“What I was going to say,” said the Dodo in an offended tone, “was, that the best thing to get us dry would be a Caucus-race.”\\n\\n“What is a Caucus-race?” said Alice; not that she wanted much to know, but the Dodo had paused as if it thought that somebody ought to speak, and no one else seemed inclined to say anything.\\n\\n“Why,” said the Dodo, “the best way to explain it is to do it.” (And, as you might like to try the thing yourself, some winter day, I will tell you how the Dodo managed it.)'), 0.230255331199835), (Document(id='0bd663d8-6e5f-4030-a6dc-126c2b20352d', metadata={'source': '/kaggle/input/text-for-summarizing/books/alice_in_wonderland.md', 'start_index': 23275}, page_content='It was high time to go, for the pool was getting quite crowded with the birds and animals that had fallen into it: there were a Duck and a Dodo, a Lory and an Eaglet, and several other curious creatures. Alice led the way, and the whole party swam to the shore.\\n\\nCHAPTER III. A Caucus-Race and a Long Tale'), 0.1394906599387754), (Document(id='66e6ab57-719d-4fe4-a557-13e5adc30415', metadata={'start_index': 26942, 'source': '/kaggle/input/text-for-summarizing/books/alice_in_wonderland.md'}, page_content='First it marked out a race-course, in a sort of circle, (“the exact shape doesn’t matter,” it said,) and then all the party were placed along the course, here and there. There was no “One, two, three, and away,” but they began running when they liked, and left off when they liked, so that it was not easy to know when the race was over. However, when they had been running half an hour or so, and were quite dry again, the Dodo suddenly called out “The race is over!” and they all crowded round it,'), 0.023556817977677058), (Document(id='5a3845f9-a556-4aab-87d1-8e7cd2decb0e', metadata={'source': '/kaggle/input/text-for-summarizing/books/alice_in_wonderland.md', 'start_index': 27294}, page_content='they had been running half an hour or so, and were quite dry again, the Dodo suddenly called out “The race is over!” and they all crowded round it, panting, and asking, “But who has won?”'), -0.05882282206583733)]\n",
      "  results = self.db.similarity_search_with_relevance_scores(query, k=k)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error generating response: backend='inductor' raised:\n",
      "RuntimeError: Found Tesla P100-PCIE-16GB which is too old to be supported by the triton GPU compiler, which is used as the backend. Triton only supports devices of CUDA Capability >= 7.0, but your device is of CUDA capability 6.0\n",
      "\n",
      "Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
      "\n",
      "\n",
      "You can suppress this exception and fall back to eager by setting:\n",
      "    import torch._dynamo\n",
      "    torch._dynamo.config.suppress_errors = True\n",
      "\n",
      "Error generating response: backend='inductor' raised:\n",
      "RuntimeError: Found Tesla P100-PCIE-16GB which is too old to be supported by the triton GPU compiler, which is used as the backend. Triton only supports devices of CUDA Capability >= 7.0, but your device is of CUDA capability 6.0\n",
      "\n",
      "Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
      "\n",
      "\n",
      "You can suppress this exception and fall back to eager by setting:\n",
      "    import torch._dynamo\n",
      "    torch._dynamo.config.suppress_errors = True\n",
      "\n",
      "Error generating response: backend='inductor' raised:\n",
      "RuntimeError: Found Tesla P100-PCIE-16GB which is too old to be supported by the triton GPU compiler, which is used as the backend. Triton only supports devices of CUDA Capability >= 7.0, but your device is of CUDA capability 6.0\n",
      "\n",
      "Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
      "\n",
      "\n",
      "You can suppress this exception and fall back to eager by setting:\n",
      "    import torch._dynamo\n",
      "    torch._dynamo.config.suppress_errors = True\n",
      "\n",
      "Error generating response: backend='inductor' raised:\n",
      "RuntimeError: Found Tesla P100-PCIE-16GB which is too old to be supported by the triton GPU compiler, which is used as the backend. Triton only supports devices of CUDA Capability >= 7.0, but your device is of CUDA capability 6.0\n",
      "\n",
      "Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
      "\n",
      "\n",
      "You can suppress this exception and fall back to eager by setting:\n",
      "    import torch._dynamo\n",
      "    torch._dynamo.config.suppress_errors = True\n",
      "\n",
      "Error generating response: backend='inductor' raised:\n",
      "RuntimeError: Found Tesla P100-PCIE-16GB which is too old to be supported by the triton GPU compiler, which is used as the backend. Triton only supports devices of CUDA Capability >= 7.0, but your device is of CUDA capability 6.0\n",
      "\n",
      "Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
      "\n",
      "\n",
      "You can suppress this exception and fall back to eager by setting:\n",
      "    import torch._dynamo\n",
      "    torch._dynamo.config.suppress_errors = True\n",
      "\n",
      "Error generating response: backend='inductor' raised:\n",
      "RuntimeError: Found Tesla P100-PCIE-16GB which is too old to be supported by the triton GPU compiler, which is used as the backend. Triton only supports devices of CUDA Capability >= 7.0, but your device is of CUDA capability 6.0\n",
      "\n",
      "Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
      "\n",
      "\n",
      "You can suppress this exception and fall back to eager by setting:\n",
      "    import torch._dynamo\n",
      "    torch._dynamo.config.suppress_errors = True\n",
      "\n",
      "Error generating response: backend='inductor' raised:\n",
      "RuntimeError: Found Tesla P100-PCIE-16GB which is too old to be supported by the triton GPU compiler, which is used as the backend. Triton only supports devices of CUDA Capability >= 7.0, but your device is of CUDA capability 6.0\n",
      "\n",
      "Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
      "\n",
      "\n",
      "You can suppress this exception and fall back to eager by setting:\n",
      "    import torch._dynamo\n",
      "    torch._dynamo.config.suppress_errors = True\n",
      "\n",
      "Error generating response: backend='inductor' raised:\n",
      "RuntimeError: Found Tesla P100-PCIE-16GB which is too old to be supported by the triton GPU compiler, which is used as the backend. Triton only supports devices of CUDA Capability >= 7.0, but your device is of CUDA capability 6.0\n",
      "\n",
      "Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
      "\n",
      "\n",
      "You can suppress this exception and fall back to eager by setting:\n",
      "    import torch._dynamo\n",
      "    torch._dynamo.config.suppress_errors = True\n",
      "\n",
      "Error generating response: backend='inductor' raised:\n",
      "RuntimeError: Found Tesla P100-PCIE-16GB which is too old to be supported by the triton GPU compiler, which is used as the backend. Triton only supports devices of CUDA Capability >= 7.0, but your device is of CUDA capability 6.0\n",
      "\n",
      "Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
      "\n",
      "\n",
      "You can suppress this exception and fall back to eager by setting:\n",
      "    import torch._dynamo\n",
      "    torch._dynamo.config.suppress_errors = True\n",
      "\n",
      "Error generating response: backend='inductor' raised:\n",
      "RuntimeError: Found Tesla P100-PCIE-16GB which is too old to be supported by the triton GPU compiler, which is used as the backend. Triton only supports devices of CUDA Capability >= 7.0, but your device is of CUDA capability 6.0\n",
      "\n",
      "Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
      "\n",
      "\n",
      "You can suppress this exception and fall back to eager by setting:\n",
      "    import torch._dynamo\n",
      "    torch._dynamo.config.suppress_errors = True\n",
      "\n",
      "Error generating response: backend='inductor' raised:\n",
      "RuntimeError: Found Tesla P100-PCIE-16GB which is too old to be supported by the triton GPU compiler, which is used as the backend. Triton only supports devices of CUDA Capability >= 7.0, but your device is of CUDA capability 6.0\n",
      "\n",
      "Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
      "\n",
      "\n",
      "You can suppress this exception and fall back to eager by setting:\n",
      "    import torch._dynamo\n",
      "    torch._dynamo.config.suppress_errors = True\n",
      "\n",
      "Error generating response: backend='inductor' raised:\n",
      "RuntimeError: Found Tesla P100-PCIE-16GB which is too old to be supported by the triton GPU compiler, which is used as the backend. Triton only supports devices of CUDA Capability >= 7.0, but your device is of CUDA capability 6.0\n",
      "\n",
      "Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
      "\n",
      "\n",
      "You can suppress this exception and fall back to eager by setting:\n",
      "    import torch._dynamo\n",
      "    torch._dynamo.config.suppress_errors = True\n",
      "\n",
      "Error generating response: backend='inductor' raised:\n",
      "RuntimeError: Found Tesla P100-PCIE-16GB which is too old to be supported by the triton GPU compiler, which is used as the backend. Triton only supports devices of CUDA Capability >= 7.0, but your device is of CUDA capability 6.0\n",
      "\n",
      "Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
      "\n",
      "\n",
      "You can suppress this exception and fall back to eager by setting:\n",
      "    import torch._dynamo\n",
      "    torch._dynamo.config.suppress_errors = True\n",
      "\n",
      "Error generating response: backend='inductor' raised:\n",
      "RuntimeError: Found Tesla P100-PCIE-16GB which is too old to be supported by the triton GPU compiler, which is used as the backend. Triton only supports devices of CUDA Capability >= 7.0, but your device is of CUDA capability 6.0\n",
      "\n",
      "Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
      "\n",
      "\n",
      "You can suppress this exception and fall back to eager by setting:\n",
      "    import torch._dynamo\n",
      "    torch._dynamo.config.suppress_errors = True\n",
      "\n",
      "Error generating response: backend='inductor' raised:\n",
      "RuntimeError: Found Tesla P100-PCIE-16GB which is too old to be supported by the triton GPU compiler, which is used as the backend. Triton only supports devices of CUDA Capability >= 7.0, but your device is of CUDA capability 6.0\n",
      "\n",
      "Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
      "\n",
      "\n",
      "You can suppress this exception and fall back to eager by setting:\n",
      "    import torch._dynamo\n",
      "    torch._dynamo.config.suppress_errors = True\n",
      "\n",
      "Error generating response: backend='inductor' raised:\n",
      "RuntimeError: Found Tesla P100-PCIE-16GB which is too old to be supported by the triton GPU compiler, which is used as the backend. Triton only supports devices of CUDA Capability >= 7.0, but your device is of CUDA capability 6.0\n",
      "\n",
      "Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
      "\n",
      "\n",
      "You can suppress this exception and fall back to eager by setting:\n",
      "    import torch._dynamo\n",
      "    torch._dynamo.config.suppress_errors = True\n",
      "\n",
      "Error generating response: backend='inductor' raised:\n",
      "RuntimeError: Found Tesla P100-PCIE-16GB which is too old to be supported by the triton GPU compiler, which is used as the backend. Triton only supports devices of CUDA Capability >= 7.0, but your device is of CUDA capability 6.0\n",
      "\n",
      "Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
      "\n",
      "\n",
      "You can suppress this exception and fall back to eager by setting:\n",
      "    import torch._dynamo\n",
      "    torch._dynamo.config.suppress_errors = True\n",
      "\n",
      "Error generating response: backend='inductor' raised:\n",
      "RuntimeError: Found Tesla P100-PCIE-16GB which is too old to be supported by the triton GPU compiler, which is used as the backend. Triton only supports devices of CUDA Capability >= 7.0, but your device is of CUDA capability 6.0\n",
      "\n",
      "Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
      "\n",
      "\n",
      "You can suppress this exception and fall back to eager by setting:\n",
      "    import torch._dynamo\n",
      "    torch._dynamo.config.suppress_errors = True\n",
      "\n",
      "Error generating response: backend='inductor' raised:\n",
      "RuntimeError: Found Tesla P100-PCIE-16GB which is too old to be supported by the triton GPU compiler, which is used as the backend. Triton only supports devices of CUDA Capability >= 7.0, but your device is of CUDA capability 6.0\n",
      "\n",
      "Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
      "\n",
      "\n",
      "You can suppress this exception and fall back to eager by setting:\n",
      "    import torch._dynamo\n",
      "    torch._dynamo.config.suppress_errors = True\n",
      "\n",
      "Error generating response: backend='inductor' raised:\n",
      "RuntimeError: Found Tesla P100-PCIE-16GB which is too old to be supported by the triton GPU compiler, which is used as the backend. Triton only supports devices of CUDA Capability >= 7.0, but your device is of CUDA capability 6.0\n",
      "\n",
      "Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
      "\n",
      "\n",
      "You can suppress this exception and fall back to eager by setting:\n",
      "    import torch._dynamo\n",
      "    torch._dynamo.config.suppress_errors = True\n",
      "\n",
      "Error generating response: backend='inductor' raised:\n",
      "RuntimeError: Found Tesla P100-PCIE-16GB which is too old to be supported by the triton GPU compiler, which is used as the backend. Triton only supports devices of CUDA Capability >= 7.0, but your device is of CUDA capability 6.0\n",
      "\n",
      "Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
      "\n",
      "\n",
      "You can suppress this exception and fall back to eager by setting:\n",
      "    import torch._dynamo\n",
      "    torch._dynamo.config.suppress_errors = True\n",
      "\n",
      "Error generating response: backend='inductor' raised:\n",
      "RuntimeError: Found Tesla P100-PCIE-16GB which is too old to be supported by the triton GPU compiler, which is used as the backend. Triton only supports devices of CUDA Capability >= 7.0, but your device is of CUDA capability 6.0\n",
      "\n",
      "Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
      "\n",
      "\n",
      "You can suppress this exception and fall back to eager by setting:\n",
      "    import torch._dynamo\n",
      "    torch._dynamo.config.suppress_errors = True\n",
      "\n",
      "Error generating response: backend='inductor' raised:\n",
      "RuntimeError: Found Tesla P100-PCIE-16GB which is too old to be supported by the triton GPU compiler, which is used as the backend. Triton only supports devices of CUDA Capability >= 7.0, but your device is of CUDA capability 6.0\n",
      "\n",
      "Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
      "\n",
      "\n",
      "You can suppress this exception and fall back to eager by setting:\n",
      "    import torch._dynamo\n",
      "    torch._dynamo.config.suppress_errors = True\n",
      "\n",
      "Error generating response: backend='inductor' raised:\n",
      "RuntimeError: Found Tesla P100-PCIE-16GB which is too old to be supported by the triton GPU compiler, which is used as the backend. Triton only supports devices of CUDA Capability >= 7.0, but your device is of CUDA capability 6.0\n",
      "\n",
      "Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
      "\n",
      "\n",
      "You can suppress this exception and fall back to eager by setting:\n",
      "    import torch._dynamo\n",
      "    torch._dynamo.config.suppress_errors = True\n",
      "\n",
      "Error generating response: backend='inductor' raised:\n",
      "RuntimeError: Found Tesla P100-PCIE-16GB which is too old to be supported by the triton GPU compiler, which is used as the backend. Triton only supports devices of CUDA Capability >= 7.0, but your device is of CUDA capability 6.0\n",
      "\n",
      "Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
      "\n",
      "\n",
      "You can suppress this exception and fall back to eager by setting:\n",
      "    import torch._dynamo\n",
      "    torch._dynamo.config.suppress_errors = True\n",
      "\n",
      "Error generating response: backend='inductor' raised:\n",
      "RuntimeError: Found Tesla P100-PCIE-16GB which is too old to be supported by the triton GPU compiler, which is used as the backend. Triton only supports devices of CUDA Capability >= 7.0, but your device is of CUDA capability 6.0\n",
      "\n",
      "Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
      "\n",
      "\n",
      "You can suppress this exception and fall back to eager by setting:\n",
      "    import torch._dynamo\n",
      "    torch._dynamo.config.suppress_errors = True\n",
      "\n",
      "Error generating response: backend='inductor' raised:\n",
      "RuntimeError: Found Tesla P100-PCIE-16GB which is too old to be supported by the triton GPU compiler, which is used as the backend. Triton only supports devices of CUDA Capability >= 7.0, but your device is of CUDA capability 6.0\n",
      "\n",
      "Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
      "\n",
      "\n",
      "You can suppress this exception and fall back to eager by setting:\n",
      "    import torch._dynamo\n",
      "    torch._dynamo.config.suppress_errors = True\n",
      "\n",
      "Error generating response: backend='inductor' raised:\n",
      "RuntimeError: Found Tesla P100-PCIE-16GB which is too old to be supported by the triton GPU compiler, which is used as the backend. Triton only supports devices of CUDA Capability >= 7.0, but your device is of CUDA capability 6.0\n",
      "\n",
      "Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
      "\n",
      "\n",
      "You can suppress this exception and fall back to eager by setting:\n",
      "    import torch._dynamo\n",
      "    torch._dynamo.config.suppress_errors = True\n",
      "\n",
      "Error generating response: backend='inductor' raised:\n",
      "RuntimeError: Found Tesla P100-PCIE-16GB which is too old to be supported by the triton GPU compiler, which is used as the backend. Triton only supports devices of CUDA Capability >= 7.0, but your device is of CUDA capability 6.0\n",
      "\n",
      "Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
      "\n",
      "\n",
      "You can suppress this exception and fall back to eager by setting:\n",
      "    import torch._dynamo\n",
      "    torch._dynamo.config.suppress_errors = True\n",
      "\n",
      "Error generating response: backend='inductor' raised:\n",
      "RuntimeError: Found Tesla P100-PCIE-16GB which is too old to be supported by the triton GPU compiler, which is used as the backend. Triton only supports devices of CUDA Capability >= 7.0, but your device is of CUDA capability 6.0\n",
      "\n",
      "Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
      "\n",
      "\n",
      "You can suppress this exception and fall back to eager by setting:\n",
      "    import torch._dynamo\n",
      "    torch._dynamo.config.suppress_errors = True\n",
      "\n",
      "Error generating response: backend='inductor' raised:\n",
      "RuntimeError: Found Tesla P100-PCIE-16GB which is too old to be supported by the triton GPU compiler, which is used as the backend. Triton only supports devices of CUDA Capability >= 7.0, but your device is of CUDA capability 6.0\n",
      "\n",
      "Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
      "\n",
      "\n",
      "You can suppress this exception and fall back to eager by setting:\n",
      "    import torch._dynamo\n",
      "    torch._dynamo.config.suppress_errors = True\n",
      "\n",
      "Error generating response: backend='inductor' raised:\n",
      "RuntimeError: Found Tesla P100-PCIE-16GB which is too old to be supported by the triton GPU compiler, which is used as the backend. Triton only supports devices of CUDA Capability >= 7.0, but your device is of CUDA capability 6.0\n",
      "\n",
      "Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
      "\n",
      "\n",
      "You can suppress this exception and fall back to eager by setting:\n",
      "    import torch._dynamo\n",
      "    torch._dynamo.config.suppress_errors = True\n",
      "\n",
      "Error generating response: backend='inductor' raised:\n",
      "RuntimeError: Found Tesla P100-PCIE-16GB which is too old to be supported by the triton GPU compiler, which is used as the backend. Triton only supports devices of CUDA Capability >= 7.0, but your device is of CUDA capability 6.0\n",
      "\n",
      "Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
      "\n",
      "\n",
      "You can suppress this exception and fall back to eager by setting:\n",
      "    import torch._dynamo\n",
      "    torch._dynamo.config.suppress_errors = True\n",
      "\n",
      "Error generating response: backend='inductor' raised:\n",
      "RuntimeError: Found Tesla P100-PCIE-16GB which is too old to be supported by the triton GPU compiler, which is used as the backend. Triton only supports devices of CUDA Capability >= 7.0, but your device is of CUDA capability 6.0\n",
      "\n",
      "Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
      "\n",
      "\n",
      "You can suppress this exception and fall back to eager by setting:\n",
      "    import torch._dynamo\n",
      "    torch._dynamo.config.suppress_errors = True\n",
      "\n",
      "Error generating response: backend='inductor' raised:\n",
      "RuntimeError: Found Tesla P100-PCIE-16GB which is too old to be supported by the triton GPU compiler, which is used as the backend. Triton only supports devices of CUDA Capability >= 7.0, but your device is of CUDA capability 6.0\n",
      "\n",
      "Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
      "\n",
      "\n",
      "You can suppress this exception and fall back to eager by setting:\n",
      "    import torch._dynamo\n",
      "    torch._dynamo.config.suppress_errors = True\n",
      "\n",
      "Error generating response: backend='inductor' raised:\n",
      "RuntimeError: Found Tesla P100-PCIE-16GB which is too old to be supported by the triton GPU compiler, which is used as the backend. Triton only supports devices of CUDA Capability >= 7.0, but your device is of CUDA capability 6.0\n",
      "\n",
      "Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
      "\n",
      "\n",
      "You can suppress this exception and fall back to eager by setting:\n",
      "    import torch._dynamo\n",
      "    torch._dynamo.config.suppress_errors = True\n",
      "\n",
      "Error generating response: backend='inductor' raised:\n",
      "RuntimeError: Found Tesla P100-PCIE-16GB which is too old to be supported by the triton GPU compiler, which is used as the backend. Triton only supports devices of CUDA Capability >= 7.0, but your device is of CUDA capability 6.0\n",
      "\n",
      "Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
      "\n",
      "\n",
      "You can suppress this exception and fall back to eager by setting:\n",
      "    import torch._dynamo\n",
      "    torch._dynamo.config.suppress_errors = True\n",
      "\n",
      "Error generating response: backend='inductor' raised:\n",
      "RuntimeError: Found Tesla P100-PCIE-16GB which is too old to be supported by the triton GPU compiler, which is used as the backend. Triton only supports devices of CUDA Capability >= 7.0, but your device is of CUDA capability 6.0\n",
      "\n",
      "Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
      "\n",
      "\n",
      "You can suppress this exception and fall back to eager by setting:\n",
      "    import torch._dynamo\n",
      "    torch._dynamo.config.suppress_errors = True\n",
      "\n",
      "Error generating response: backend='inductor' raised:\n",
      "RuntimeError: Found Tesla P100-PCIE-16GB which is too old to be supported by the triton GPU compiler, which is used as the backend. Triton only supports devices of CUDA Capability >= 7.0, but your device is of CUDA capability 6.0\n",
      "\n",
      "Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
      "\n",
      "\n",
      "You can suppress this exception and fall back to eager by setting:\n",
      "    import torch._dynamo\n",
      "    torch._dynamo.config.suppress_errors = True\n",
      "\n",
      "Error generating response: backend='inductor' raised:\n",
      "RuntimeError: Found Tesla P100-PCIE-16GB which is too old to be supported by the triton GPU compiler, which is used as the backend. Triton only supports devices of CUDA Capability >= 7.0, but your device is of CUDA capability 6.0\n",
      "\n",
      "Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
      "\n",
      "\n",
      "You can suppress this exception and fall back to eager by setting:\n",
      "    import torch._dynamo\n",
      "    torch._dynamo.config.suppress_errors = True\n",
      "\n",
      "Error generating response: backend='inductor' raised:\n",
      "RuntimeError: Found Tesla P100-PCIE-16GB which is too old to be supported by the triton GPU compiler, which is used as the backend. Triton only supports devices of CUDA Capability >= 7.0, but your device is of CUDA capability 6.0\n",
      "\n",
      "Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
      "\n",
      "\n",
      "You can suppress this exception and fall back to eager by setting:\n",
      "    import torch._dynamo\n",
      "    torch._dynamo.config.suppress_errors = True\n",
      "\n",
      "Error generating response: backend='inductor' raised:\n",
      "RuntimeError: Found Tesla P100-PCIE-16GB which is too old to be supported by the triton GPU compiler, which is used as the backend. Triton only supports devices of CUDA Capability >= 7.0, but your device is of CUDA capability 6.0\n",
      "\n",
      "Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
      "\n",
      "\n",
      "You can suppress this exception and fall back to eager by setting:\n",
      "    import torch._dynamo\n",
      "    torch._dynamo.config.suppress_errors = True\n",
      "\n",
      "Error generating response: backend='inductor' raised:\n",
      "RuntimeError: Found Tesla P100-PCIE-16GB which is too old to be supported by the triton GPU compiler, which is used as the backend. Triton only supports devices of CUDA Capability >= 7.0, but your device is of CUDA capability 6.0\n",
      "\n",
      "Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
      "\n",
      "\n",
      "You can suppress this exception and fall back to eager by setting:\n",
      "    import torch._dynamo\n",
      "    torch._dynamo.config.suppress_errors = True\n",
      "\n",
      "Error generating response: backend='inductor' raised:\n",
      "RuntimeError: Found Tesla P100-PCIE-16GB which is too old to be supported by the triton GPU compiler, which is used as the backend. Triton only supports devices of CUDA Capability >= 7.0, but your device is of CUDA capability 6.0\n",
      "\n",
      "Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
      "\n",
      "\n",
      "You can suppress this exception and fall back to eager by setting:\n",
      "    import torch._dynamo\n",
      "    torch._dynamo.config.suppress_errors = True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "run_mains(\n",
    "    test_embedding_models=[i for i in range(len(EMBEDDING_MODEL_OPTIONS))],\n",
    "    test_text_generation_models=[i for i in range(len(TEXT_GENERATION_MODEL_OPTIONS)) if \"gemma\" in TEXT_GENERATION_MODEL_OPTIONS[i]]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# run_mains(\n",
    "#     test_embedding_models=[i for i in range(len(EMBEDDING_MODEL_OPTIONS))],\n",
    "#     test_text_generation_models=[i for i in range(len(TEXT_GENERATION_MODEL_OPTIONS)) if \"Llama\" in TEXT_GENERATION_MODEL_OPTIONS[i]]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-08-20T04:56:04.689Z"
    },
    "scrolled": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# run_mains(\n",
    "#     test_embedding_models=[i for i in range(len(EMBEDDING_MODEL_OPTIONS))],\n",
    "    # test_text_generation_models=[i for i in range(len(TEXT_GENERATION_MODEL_OPTIONS)) if i not in [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]]\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-08-20T04:56:04.690Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# problem: recreating same db will cause error saving\n",
    "# run_mains(\n",
    "#     test_embedding_models=[1],\n",
    "#     test_text_generation_models=[]\n",
    "# )\n",
    "# run_mains(\n",
    "#     test_embedding_models=[1],\n",
    "#     test_text_generation_models=[]\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-08-20T04:56:04.690Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# problem: chat model 4,5,6 not previously used\n",
    "# run_mains(\n",
    "#     test_embedding_models=[i for i in range(len(EMBEDDING_MODEL_OPTIONS))],\n",
    "#     test_text_generation_models=[4,5,6]\n",
    "# )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-08-20T04:56:04.690Z"
    },
    "scrolled": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# problem: missing test cases\n",
    "\n",
    "# run_mains( # not enough GPU space\n",
    "#     test_embedding_models=[13],\n",
    "#     test_text_generation_models=[3]\n",
    "# )\n",
    "\n",
    "# skipped ([8], [4])\n",
    "\n",
    "# run_mains(\n",
    "#     test_embedding_models=[8],\n",
    "#     test_text_generation_models=[i for i in range(5, len(TEXT_GENERATION_MODEL_OPTIONS))]\n",
    "# )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-08-20T04:56:04.690Z"
    },
    "scrolled": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print_results_summary()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 8066456,
     "sourceId": 12760370,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8066490,
     "sourceId": 12760417,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8101637,
     "sourceId": 12812875,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
